{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: matplotlib in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (3.10.1)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (1.3.2)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (4.57.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (1.4.8)\n",
      "Requirement already satisfied: numpy>=1.23 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (2.2.5)\n",
      "Requirement already satisfied: packaging>=20.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (24.2)\n",
      "Requirement already satisfied: pillow>=8 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (11.2.1)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (3.2.3)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from matplotlib) (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from python-dateutil>=2.7->matplotlib) (1.17.0)\n",
      "Requirement already satisfied: lightning in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (2.5.1)\n",
      "Collecting timm\n",
      "  Downloading timm-1.0.15-py3-none-any.whl.metadata (52 kB)\n",
      "Requirement already satisfied: PyYAML<8.0,>=5.4 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (6.0.2)\n",
      "Requirement already satisfied: fsspec<2026.0,>=2022.5.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (2025.3.2)\n",
      "Requirement already satisfied: lightning-utilities<2.0,>=0.10.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (0.14.3)\n",
      "Requirement already satisfied: packaging<25.0,>=20.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (24.2)\n",
      "Requirement already satisfied: torch<4.0,>=2.1.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (2.7.0)\n",
      "Requirement already satisfied: torchmetrics<3.0,>=0.7.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (1.7.1)\n",
      "Requirement already satisfied: tqdm<6.0,>=4.57.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions<6.0,>=4.4.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (4.13.2)\n",
      "Requirement already satisfied: pytorch-lightning in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning) (2.5.1)\n",
      "Requirement already satisfied: torchvision in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from timm) (0.22.0)\n",
      "Requirement already satisfied: huggingface_hub in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from timm) (0.30.2)\n",
      "Requirement already satisfied: safetensors in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from timm) (0.5.3)\n",
      "Requirement already satisfied: aiohttp!=4.0.0a0,!=4.0.0a1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from fsspec[http]<2026.0,>=2022.5.0->lightning) (3.11.18)\n",
      "Requirement already satisfied: setuptools in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from lightning-utilities<2.0,>=0.10.0->lightning) (79.0.1)\n",
      "Requirement already satisfied: filelock in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.18.0)\n",
      "Requirement already satisfied: sympy>=1.13.3 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (1.13.3)\n",
      "Requirement already satisfied: networkx in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.4.2)\n",
      "Requirement already satisfied: jinja2 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torch<4.0,>=2.1.0->lightning) (3.1.4)\n",
      "Requirement already satisfied: numpy>1.20.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torchmetrics<3.0,>=0.7.0->lightning) (2.2.5)\n",
      "Requirement already satisfied: requests in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from huggingface_hub->timm) (2.32.3)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from torchvision->timm) (11.2.1)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (2.6.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.3.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.6.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (6.4.3)\n",
      "Requirement already satisfied: propcache>=0.2.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (0.3.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from aiohttp!=4.0.0a0,!=4.0.0a1->fsspec[http]<2026.0,>=2022.5.0->lightning) (1.20.0)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from sympy>=1.13.3->torch<4.0,>=2.1.0->lightning) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from jinja2->torch<4.0,>=2.1.0->lightning) (3.0.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2.4.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/beszabo/.pyenv/versions/3.12.10/lib/python3.12/site-packages (from requests->huggingface_hub->timm) (2025.1.31)\n",
      "Downloading timm-1.0.15-py3-none-any.whl (2.4 MB)\n",
      "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m MB/s\u001b[0m eta \u001b[36m0:00:01\u001b[0m:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: timm\n",
      "Successfully installed timm-1.0.15\n"
     ]
    }
   ],
   "source": [
    "!pip install matplotlib\n",
    "!pip install lightning timm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:15:14.660072Z",
     "iopub.status.busy": "2025-02-06T18:15:14.659781Z",
     "iopub.status.idle": "2025-02-06T18:15:40.834467Z",
     "shell.execute_reply": "2025-02-06T18:15:40.833767Z",
     "shell.execute_reply.started": "2025-02-06T18:15:14.660050Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import multiprocessing as mp\n",
    "\n",
    "# Then import other libraries and define your code\n",
    "import random\n",
    "from collections import defaultdict, Counter\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "#import seaborn as sns\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, average_precision_score\n",
    "from sklearn.metrics import multilabel_confusion_matrix as ConfusionMatrix\n",
    "from sklearn.model_selection import KFold\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "from transformers import get_cosine_schedule_with_warmup\n",
    "from torchvision import transforms, models\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision.transforms import AutoAugment, AutoAugmentPolicy\n",
    "\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.strategies import DDPStrategy\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "\n",
    "from torchmetrics.classification import Accuracy, Precision, Recall, F1Score, MulticlassF1Score\n",
    "from torchmetrics.classification import BinaryPrecisionRecallCurve\n",
    "\n",
    "#from bayes_opt import BayesianOptimization\n",
    "from tqdm import tqdm\n",
    "import timm\n",
    "\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:15:40.835814Z",
     "iopub.status.busy": "2025-02-06T18:15:40.835517Z",
     "iopub.status.idle": "2025-02-06T18:15:40.841403Z",
     "shell.execute_reply": "2025-02-06T18:15:40.840475Z",
     "shell.execute_reply.started": "2025-02-06T18:15:40.835786Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class SEBlock(nn.Module):\n",
    "    def __init__(self, in_channels, reduction=16):\n",
    "        super(SEBlock, self).__init__()\n",
    "        self.global_avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        self.fc1 = nn.Linear(in_channels, in_channels // reduction, bias=False)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc2 = nn.Linear(in_channels // reduction, in_channels, bias=False)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        b, c, _, _ = x.size()\n",
    "        y = self.global_avg_pool(x).view(b, c)\n",
    "        y = self.fc1(y)\n",
    "        y = self.relu(y)\n",
    "        y = self.fc2(y)\n",
    "        y = self.sigmoid(y).view(b, c, 1, 1)\n",
    "        return x * y\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:15:40.843351Z",
     "iopub.status.busy": "2025-02-06T18:15:40.843044Z",
     "iopub.status.idle": "2025-02-06T18:15:40.874499Z",
     "shell.execute_reply": "2025-02-06T18:15:40.873752Z",
     "shell.execute_reply.started": "2025-02-06T18:15:40.843328Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class ResNetWrapper(pl.LightningModule):\n",
    "    def __init__(self, lr, dropout_rate=0.5, ood_threshold=0.5):\n",
    "        super().__init__()\n",
    "        self.lr = lr\n",
    "        self.ood_threshold = ood_threshold\n",
    "        self.dropout_rate = dropout_rate\n",
    "        self.register_buffer('sigmoid_thresholds', torch.ones(80, dtype=torch.float32) * 0.5)  # Initial 0.5 for all classes\n",
    "        self.validation_step_y_hats = []\n",
    "\n",
    "        # Data collection for threshold tuning\n",
    "        self.validation_logits = []\n",
    "        self.validation_targets = []\n",
    "\n",
    "        # testing\n",
    "        self.validation_step_ys = []\n",
    "\n",
    "        backbone = timm.create_model('seresnet152d', pretrained=True, num_classes=80)\n",
    "        layers = list(backbone.children())[:-1]\n",
    "        layers.insert(-1, SEBlock(in_channels=2048))  # Insert FIRST\n",
    "        self.feature_extractor = nn.Sequential(*layers)  # Then create\n",
    "        # In __init__\n",
    "        assert isinstance(self.feature_extractor[-2], SEBlock), \"SEBlock not in model!\"\n",
    "\n",
    "        for param in self.feature_extractor.parameters():\n",
    "            param.requires_grad = False\n",
    "\n",
    "        layers.insert(-1, SEBlock(in_channels=2048))  # 2048 az utolsó réteg csatornáinak száma ResNet50 esetén\n",
    "\n",
    "        # Binary classifier head for OOD detection\n",
    "        self.ood_classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),\n",
    "            nn.Linear(backbone.fc.in_features, 1)  # Single output for binary classification\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Dropout(self.dropout_rate),  # hyperparameter\n",
    "            nn.Linear(backbone.fc.in_features, out_features=80)\n",
    "        )\n",
    "\n",
    "        # Separate loss functions for both tasks\n",
    "        self.loss_fn_class = nn.BCEWithLogitsLoss()  # Multi-label loss\n",
    "        self.loss_fn_ood = nn.BCEWithLogitsLoss()    # Binary OOD loss\n",
    "\n",
    "        self.train_acc = Accuracy(task=\"multilabel\", num_labels=80)\n",
    "        self.val_acc = Accuracy(task=\"multilabel\", num_labels=80)\n",
    "        self.train_precision = Precision(task=\"multilabel\", num_labels=80)\n",
    "        self.val_precision = Precision(task=\"multilabel\", num_labels=80)\n",
    "        self.train_recall = Recall(task=\"multilabel\", num_labels=80)\n",
    "        self.val_recall = Recall(task=\"multilabel\", num_labels=80)\n",
    "        self.train_f1 = F1Score(task=\"multilabel\", num_labels=80)\n",
    "        self.val_f1 = F1Score(task=\"multilabel\", num_labels=80)\n",
    "        self.ood_acc = Accuracy(task=\"binary\") \n",
    "        self.val_acc_id = Accuracy(task=\"multilabel\", num_labels=80)\n",
    "        self.val_f1_id = F1Score(task=\"multilabel\", num_labels=80)\n",
    "        self.val_precision_id = Precision(task=\"multilabel\", num_labels=80)\n",
    "        self.val_recall_id = Recall(task=\"multilabel\", num_labels=80)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Simple defensive check\n",
    "        if not isinstance(x, torch.Tensor):\n",
    "            print(f\"[ERROR] Non-tensor input in forward: {type(x)}\")\n",
    "            # Return zeros with appropriate shape\n",
    "            batch_size = 1  # Default value\n",
    "            return (\n",
    "                torch.zeros((batch_size, 80), device=self.device),\n",
    "                torch.zeros((batch_size, 1), device=self.device)\n",
    "            )\n",
    "            \n",
    "        # Original logic\n",
    "        x = self.feature_extractor(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        \n",
    "        class_logits = self.classifier(x)\n",
    "        ood_logits = self.ood_classifier(x)\n",
    "        \n",
    "        return class_logits, ood_logits\n",
    "\n",
    "    def _get_preds(self, logits, ood_logits):\n",
    "        ood_preds = torch.sigmoid(ood_logits) > self.ood_threshold\n",
    "        class_preds = torch.sigmoid(logits) > self.sigmoid_thresholds.to(logits.device)\n",
    "        class_preds[~ood_preds.expand(-1, class_preds.size(1))] = 0\n",
    "        return class_preds\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        if isinstance(batch, dict):  # CombinedLoader\n",
    "            try:\n",
    "                # Make sure we have both parts of the batch\n",
    "                if \"id\" not in batch or \"ood\" not in batch:\n",
    "                    print(\"Missing keys in batch dict:\", batch.keys())\n",
    "                    # Fall back to regular processing\n",
    "                    x, y = next(iter(batch.values()))\n",
    "                    class_logits, _ = self(x)\n",
    "                    loss = self.loss_fn_class(class_logits, y)\n",
    "                    return loss\n",
    "                    \n",
    "                id_batch = batch[\"id\"]\n",
    "                ood_batch = batch[\"ood\"]\n",
    "                \n",
    "                # Extra safety checks\n",
    "                if not isinstance(id_batch, (list, tuple)) or len(id_batch) < 2:\n",
    "                    print(f\"Invalid id_batch structure: {type(id_batch)}\")\n",
    "                    # Fall back\n",
    "                    x, y = next(iter(batch.values()))\n",
    "                    class_logits, _ = self(x)\n",
    "                    loss = self.loss_fn_class(class_logits, y)\n",
    "                    return loss\n",
    "                    \n",
    "                # Unpack ID batch\n",
    "                x_id, y_class = id_batch\n",
    "                \n",
    "                # Final check: ensure x_id is a tensor\n",
    "                if not isinstance(x_id, torch.Tensor):\n",
    "                    print(f\"x_id is not a tensor: {type(x_id)}\")\n",
    "                    # Create dummy tensor\n",
    "                    x_id = torch.zeros((batch_size, 3, 224, 224), device=self.device)\n",
    "                    \n",
    "                class_logits, ood_logits_id = self(x_id)\n",
    "                \n",
    "                # Rest of your combined loader code...\n",
    "            \n",
    "            except Exception as e:\n",
    "                print(f\"Error in training_step with CombinedLoader: {e}\")\n",
    "                # Fall back to simpler processing\n",
    "                x, y = next(iter(batch.values()))\n",
    "                class_logits, _ = self(x)\n",
    "                loss = self.loss_fn_class(class_logits, y)\n",
    "                return loss\n",
    "        else:\n",
    "            # Regular batch processing...\n",
    "            x, y = batch\n",
    "            class_logits, _ = self(x)\n",
    "            loss = self.loss_fn_class(class_logits, y)\n",
    "            return loss\n",
    "\n",
    "    def validation_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Validation step handling both classification and OOD detection evaluation.\n",
    "        Updates multiple metrics and collects data for threshold tuning.\n",
    "        \"\"\"\n",
    "        id_batch = batch[\"id\"]\n",
    "        ood_batch = batch[\"ood\"]\n",
    "\n",
    "        # === Process In-Distribution Samples ===\n",
    "        x_id, y_class = id_batch\n",
    "        class_logits, ood_logits_id = self(x_id)\n",
    "\n",
    "        # Convert to float for loss + metrics\n",
    "        y_class = y_class.float()\n",
    "\n",
    "        # Classification Loss\n",
    "        loss_class = self.loss_fn_class(class_logits, y_class)\n",
    "\n",
    "        # Raw predictions (no masking)\n",
    "        class_preds = torch.sigmoid(class_logits) > self.sigmoid_thresholds.to(class_logits.device)\n",
    "        ood_preds = torch.sigmoid(ood_logits_id).view(-1) > 0.5  # [batch_size]\n",
    "\n",
    "        # ==== MASK OOD SAMPLES ====\n",
    "        id_mask = ood_preds == 0\n",
    "        class_preds_id = class_preds[id_mask]\n",
    "        y_class_id = y_class[id_mask]\n",
    "\n",
    "        # === Update metrics: ONLY ON ID SAMPLES ===\n",
    "        if class_preds_id.numel() > 0:\n",
    "            self.val_acc.update(class_preds_id, y_class_id)\n",
    "            self.val_precision.update(class_preds_id, y_class_id)\n",
    "            self.val_recall.update(class_preds_id, y_class_id)\n",
    "            self.val_f1.update(class_preds_id, y_class_id)\n",
    "\n",
    "        # === Update ID-only metrics (without OOD masking) ===\n",
    "        self.val_acc_id.update(class_preds, y_class)\n",
    "        self.val_f1_id.update(class_preds, y_class)\n",
    "        self.val_precision_id.update(class_preds, y_class)\n",
    "        self.val_recall_id.update(class_preds, y_class)\n",
    "\n",
    "        # === Save for threshold tuning (before masking) ===\n",
    "        self.validation_logits.append(class_logits.detach().cpu())\n",
    "        self.validation_targets.append(y_class.detach().cpu())\n",
    "\n",
    "        # === Process OOD samples ===\n",
    "        x_ood, y_ood = ood_batch\n",
    "        _, ood_logits_ood = self(x_ood)\n",
    "\n",
    "        # OOD labels: 0 for ID, 1 for OOD\n",
    "        all_ood_logits = torch.cat([\n",
    "            ood_logits_id.view(-1),     # [batch_size]\n",
    "            ood_logits_ood.view(-1)\n",
    "        ], dim=0)\n",
    "\n",
    "        all_ood_labels = torch.cat([\n",
    "            torch.zeros_like(ood_logits_id.view(-1)),  # ID = 0\n",
    "            y_ood.float().view(-1)                     # OOD = 1\n",
    "        ], dim=0)\n",
    "\n",
    "        # OOD Loss\n",
    "        loss_ood = self.loss_fn_ood(all_ood_logits, all_ood_labels)\n",
    "\n",
    "        # === Final Combined Loss ===\n",
    "        loss = loss_class + 0.5 * loss_ood\n",
    "\n",
    "        # === Log Metrics ===\n",
    "        self.log('val_loss', loss, on_step=False, on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Track how many OOD samples were filtered (i.e., predicted as OOD)\n",
    "        ood_filtered_count = (~id_mask).sum()\n",
    "        self.log(\"val_ood_id_filtered\", ood_filtered_count, on_epoch=True)\n",
    "\n",
    "        return loss\n",
    "    \n",
    "    def test_step(self, batch, batch_idx):\n",
    "        \"\"\"\n",
    "        Test step for final model evaluation.\n",
    "        Similar to validation but with additional logging.\n",
    "        Only evaluates classification metrics on in-distribution samples.\n",
    "        \"\"\"\n",
    "        id_batch = batch[\"id\"]\n",
    "        x_id, y_class = id_batch\n",
    "        class_logits, ood_logits_id = self(x_id)\n",
    "\n",
    "        y_class = y_class.float()\n",
    "\n",
    "        # Predict class and OOD\n",
    "        class_preds = torch.sigmoid(class_logits) > self.sigmoid_thresholds.to(class_logits.device)\n",
    "        ood_preds = torch.sigmoid(ood_logits_id).view(-1) > 0.5  # [batch_size]\n",
    "\n",
    "        # Mask: only keep ID samples (OOD_pred == False)\n",
    "        id_mask = ood_preds == 0\n",
    "        class_preds_id = class_preds[id_mask]\n",
    "        y_class_id = y_class[id_mask]\n",
    "\n",
    "        # === Update classification metrics on ID samples only ===\n",
    "        if class_preds_id.numel() > 0:\n",
    "            self.val_acc.update(class_preds_id, y_class_id)\n",
    "            self.val_precision.update(class_preds_id, y_class_id)\n",
    "            self.val_recall.update(class_preds_id, y_class_id)\n",
    "            self.val_f1.update(class_preds_id, y_class_id)\n",
    "\n",
    "        # === Also update unmasked ID-specific metrics (for completeness/statistics) ===\n",
    "        self.val_acc_id.update(class_preds, y_class)\n",
    "        self.val_f1_id.update(class_preds, y_class)\n",
    "        self.val_precision_id.update(class_preds, y_class)\n",
    "        self.val_recall_id.update(class_preds, y_class)\n",
    "\n",
    "        # === Log raw predictions and targets ===\n",
    "        self.validation_step_y_hats.append(class_preds.cpu())\n",
    "        self.validation_step_ys.append(y_class.cpu())\n",
    "\n",
    "        return {'preds': class_preds, 'targets': y_class}\n",
    "\n",
    "    def on_validation_epoch_end(self):\n",
    "        if self.validation_logits:\n",
    "            all_logits = torch.cat(self.validation_logits, dim=0)  # Ensure batch stacking\n",
    "            all_targets = torch.cat(self.validation_targets, dim=0)\n",
    "            new_thresholds = self._calculate_optimal_thresholds(all_logits, all_targets)\n",
    "\n",
    "            # Use in-place assignment instead of .copy_() to avoid issues\n",
    "            self.sigmoid_thresholds = (0.7 * self.sigmoid_thresholds + 0.3 * new_thresholds).detach()\n",
    "\n",
    "            # Log the average threshold\n",
    "            self.log('avg_threshold', torch.mean(self.sigmoid_thresholds).item(), prog_bar=True)\n",
    "\n",
    "            # Clear buffers\n",
    "            self.validation_logits.clear()\n",
    "            self.validation_targets.clear()\n",
    "\n",
    "        # Log metrics\n",
    "        self.log('val_acc', self.val_acc.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('val_precision', self.val_precision.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('val_recall', self.val_recall.compute(), on_epoch=True, prog_bar=True)\n",
    "        self.log('val_f1', self.val_f1.compute(), on_epoch=True, prog_bar=True)\n",
    "\n",
    "        # Reset metrics\n",
    "        self.val_acc.reset()\n",
    "        self.val_precision.reset()\n",
    "        self.val_recall.reset()\n",
    "        self.val_f1.reset()\n",
    "        self.ood_acc.reset()\n",
    "\n",
    "    def _calculate_optimal_thresholds(self, logits, targets):\n",
    "        \"\"\"Core threshold optimization logic\"\"\"\n",
    "        probs = torch.sigmoid(logits).cpu().numpy().astype(np.float32)  # Ensure float32\n",
    "        targets = targets.cpu().numpy().astype(np.float32)  # Ensure float32\n",
    "        thresholds = np.full(probs.shape[1], 0.5, dtype=np.float32)  # Default to 0.5 and float32\n",
    "        print(f\"Min prob per class: {probs.min(axis=0)}\")\n",
    "        print(f\"Max prob per class: {probs.max(axis=0)}\")\n",
    "\n",
    "        for i in range(probs.shape[1]):\n",
    "\n",
    "            if np.sum(targets[:, i]) == 0:\n",
    "                # print(f\"Warning: No positive samples for class {i}, keeping threshold at 0.5\")\n",
    "                continue  # Skip threshold tuning for this class\n",
    "\n",
    "            try:\n",
    "                # Precision-recall curve with F1 optimization\n",
    "                precision, recall, threshs = precision_recall_curve(targets[:, i], probs[:, i])\n",
    "                f1_scores = (2 * precision * recall) / (precision + recall + 1e-8)\n",
    "                best_idx = np.nanargmax(f1_scores)\n",
    "\n",
    "                if best_idx < len(threshs):\n",
    "                    thresholds[i] = threshs[best_idx]\n",
    "            except Exception as e:\n",
    "                print(f\"Error computing threshold for class {i}: {e}\")\n",
    "\n",
    "        return torch.tensor(thresholds, device=self.device, dtype=torch.float32) #return tensor\n",
    "\n",
    "    def on_epoch_start(self):\n",
    "        optimizer = self.optimizers()\n",
    "        \n",
    "        if isinstance(optimizer, list):  # If multiple optimizers exist\n",
    "            optimizer = optimizer[0]  # Pick the first one (modify if needed)\n",
    "        \n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        self.log('lr', current_lr, on_epoch=True, prog_bar=True)\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = torch.optim.AdamW(self.parameters(), lr=self.lr)\n",
    "        \n",
    "        # Ensure num_training_steps is an int\n",
    "        total_training_steps = int(self.trainer.estimated_stepping_batches)  \n",
    "        warmup_steps = int(0.1 * total_training_steps)  # 10% of steps for warmup\n",
    "\n",
    "        scheduler = get_cosine_schedule_with_warmup(\n",
    "            optimizer,\n",
    "            num_warmup_steps=warmup_steps,\n",
    "            num_training_steps=total_training_steps  # Now explicitly an int\n",
    "        )\n",
    "        return [optimizer], [{\"scheduler\": scheduler, \"interval\": \"step\"}]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Best threshold: 0.3527\n",
    "Best validation loss: 0.0591"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:15:40.875932Z",
     "iopub.status.busy": "2025-02-06T18:15:40.875708Z",
     "iopub.status.idle": "2025-02-06T18:15:40.892728Z",
     "shell.execute_reply": "2025-02-06T18:15:40.891988Z",
     "shell.execute_reply.started": "2025-02-06T18:15:40.875894Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'custom_collate_fn' from 'dataset' (/Users/beszabo/Documents/meme/dataset.py)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m     idx_to_label \u001b[38;5;241m=\u001b[39m {v: k \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m dataset\u001b[38;5;241m.\u001b[39mclass_to_idx\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[1;32m     46\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m train_dataset, test_dataset, idx_to_label\n\u001b[0;32m---> 49\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdataset\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CustomDataset,TensorLabelDataset,custom_collate_fn\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mMyDataModule\u001b[39;00m(pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m     51\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\n\u001b[1;32m     52\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[1;32m     53\u001b[0m         train_data,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     62\u001b[0m         collate_fn\u001b[38;5;241m=\u001b[39mcustom_collate_fn\n\u001b[1;32m     63\u001b[0m     ):\n",
      "\u001b[0;31mImportError\u001b[0m: cannot import name 'custom_collate_fn' from 'dataset' (/Users/beszabo/Documents/meme/dataset.py)"
     ]
    }
   ],
   "source": [
    "from dataset import CustomDataset, CustomOODDataset,target_to_oh\n",
    "from torch.utils.data import ConcatDataset\n",
    "\n",
    "def create_train_test_split_proportional(dataset, test_ratio=0.1, seed=42, transform=None):\n",
    "    \"\"\"\n",
    "    Create a train-test split proportional to the dataset's labels.\n",
    "\n",
    "    Args:\n",
    "        dataset (ImageFolder): The dataset to split.\n",
    "        test_ratio (float): Proportion of the dataset to include in the test split.\n",
    "        seed (int): Random seed for reproducibility.\n",
    "        transform: Image transformation for preprocessing.\n",
    "\n",
    "    Returns:\n",
    "        train_dataset, test_dataset, idx_to_label: The train/test split datasets and label mapping.\n",
    "    \"\"\"\n",
    "    random.seed(seed)\n",
    "\n",
    "    # Group samples by label\n",
    "    label_to_samples = defaultdict(list)\n",
    "    for sample in dataset.samples:\n",
    "        label_to_samples[sample[1]].append(sample)\n",
    "\n",
    "    train_samples = []\n",
    "    test_samples = []\n",
    "\n",
    "    # Split the dataset into train and test samples\n",
    "    for label, samples in label_to_samples.items():\n",
    "        random.shuffle(samples)\n",
    "        num_test = int(len(samples) * test_ratio)\n",
    "        test_samples.extend(samples[:num_test])\n",
    "        train_samples.extend(samples[num_test:])\n",
    "\n",
    "    # Create ImageFolder datasets for train and test\n",
    "    train_dataset = ImageFolder(dataset.root, transform=transform)\n",
    "    train_dataset.samples = train_samples\n",
    "    train_dataset.targets = [sample[1] for sample in train_samples]  # Update targets\n",
    "\n",
    "    test_dataset = ImageFolder(dataset.root, transform=transform)\n",
    "    test_dataset.samples = test_samples\n",
    "    test_dataset.targets = [sample[1] for sample in test_samples]  # Update targets\n",
    "\n",
    "    # Use the class_to_idx from the original dataset for label mapping\n",
    "    idx_to_label = {v: k for k, v in dataset.class_to_idx.items()}\n",
    "\n",
    "    return train_dataset, test_dataset, idx_to_label\n",
    "\n",
    "\n",
    "from dataset import CustomDataset,TensorLabelDataset,custom_collate_fn\n",
    "class MyDataModule(pl.LightningDataModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        train_data,\n",
    "        val_data,\n",
    "        ood_data=None,\n",
    "        val_ood_data=None,\n",
    "        test_dataset=None,\n",
    "        batch_size=32,\n",
    "        num_workers=4,\n",
    "        persistent_workers=True,\n",
    "        idx_to_label=None,\n",
    "        collate_fn=custom_collate_fn\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.train_data = train_data\n",
    "        self.val_data = val_data\n",
    "        self.ood_data = ood_data\n",
    "        self.val_ood_data = val_ood_data\n",
    "        self.test_data = test_dataset\n",
    "        self.batch_size = batch_size\n",
    "        self.num_workers = num_workers\n",
    "        self.persistent_workers = persistent_workers\n",
    "        self.idx_to_label = idx_to_label\n",
    "        self.collate_fn = collate_fn\n",
    "        \n",
    "    def setup(self, stage=None):\n",
    "        # Create proper datasets\n",
    "        if isinstance(self.train_data, ConcatDataset):\n",
    "            self.train_dataset = self.train_data  # Don't wrap it again\n",
    "        else:\n",
    "            self.train_dataset = CustomDataset(self.train_data, self.idx_to_label)\n",
    "        self.val_dataset = CustomDataset(self.val_data, self.idx_to_label)\n",
    "\n",
    "        # Set up OOD datasets if available\n",
    "        if self.ood_data is not None:\n",
    "            if isinstance(self.ood_data, CustomOODDataset) or hasattr(self.ood_data, '__getitem__'):\n",
    "                self.train_ood_dataset = self.ood_data\n",
    "            else:\n",
    "                print(f\"WARNING: ood_data is not a proper dataset: {type(self.ood_data)}\")\n",
    "                self.train_ood_dataset = None\n",
    "        else:\n",
    "            self.train_ood_dataset = None\n",
    "\n",
    "        if self.val_ood_data is not None:\n",
    "            if isinstance(self.val_ood_data, CustomOODDataset) or hasattr(self.val_ood_data, '__getitem__'):\n",
    "                self.val_ood_dataset = self.val_ood_data\n",
    "            else:\n",
    "                print(f\"WARNING: val_ood_data is not a proper dataset: {type(self.val_ood_data)}\")\n",
    "                self.val_ood_dataset = None\n",
    "        else:\n",
    "            self.val_ood_dataset = None\n",
    "\n",
    "        if self.test_data:\n",
    "            print(\"Test dataset is VALID\")\n",
    "            self.test_dataset = CustomDataset(self.test_data, self.idx_to_label)\n",
    "        else:\n",
    "            print(\"Test dataset is INVALID\")\n",
    "            self.test_dataset = None\n",
    "\n",
    "    def train_dataloader(self):\n",
    "        id_loader = DataLoader(\n",
    "            self.train_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=True,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            collate_fn=self.collate_fn\n",
    "        )\n",
    "\n",
    "        if self.train_ood_dataset is not None:\n",
    "            ood_loader = DataLoader(\n",
    "                self.train_ood_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=True,\n",
    "                num_workers=self.num_workers,\n",
    "                persistent_workers=self.persistent_workers,\n",
    "                collate_fn=self.collate_fn\n",
    "            )\n",
    "\n",
    "            combined = CombinedLoader({\"id\": id_loader, \"ood\": ood_loader}, mode=\"min_size\")\n",
    "\n",
    "            # ✨ Force building the iterator manually\n",
    "            _ = iter(combined)\n",
    "\n",
    "            return combined\n",
    "        else:\n",
    "            print(\"No OOD data\")\n",
    "            return id_loader\n",
    "\n",
    "    def val_dataloader(self):\n",
    "        id_loader = DataLoader(\n",
    "            self.val_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers,\n",
    "            collate_fn=self.collate_fn  # Add this\n",
    "        )\n",
    "\n",
    "        if hasattr(self, \"val_ood_dataset\") and self.val_ood_dataset is not None:\n",
    "            ood_loader = DataLoader(\n",
    "                self.val_ood_dataset,\n",
    "                batch_size=self.batch_size,\n",
    "                shuffle=False,\n",
    "                num_workers=self.num_workers,\n",
    "                persistent_workers=self.persistent_workers,\n",
    "                collate_fn=self.collate_fn  # Add this\n",
    "            )\n",
    "            return CombinedLoader({\"id\": id_loader, \"ood\": ood_loader}, mode=\"min_size\")\n",
    "        \n",
    "        return id_loader\n",
    "\n",
    "\n",
    "    def test_dataloader(self):\n",
    "        if self.test_dataset is None:\n",
    "            print(\"No test dataset\")\n",
    "            return None\n",
    "        return DataLoader(\n",
    "            self.test_dataset,\n",
    "            batch_size=self.batch_size,\n",
    "            shuffle=False,\n",
    "            num_workers=self.num_workers,\n",
    "            persistent_workers=self.persistent_workers\n",
    "        )\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:15:40.893620Z",
     "iopub.status.busy": "2025-02-06T18:15:40.893422Z",
     "iopub.status.idle": "2025-02-06T18:16:06.935648Z",
     "shell.execute_reply": "2025-02-06T18:16:06.934748Z",
     "shell.execute_reply.started": "2025-02-06T18:15:40.893601Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataset size: 10000\n",
      "OOD dataset size: 10000\n",
      "\n",
      "Train dataset size: 9200\n",
      "Test dataset size: 800 \n",
      "\n",
      "Unique class indices in dataset: {0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47, 48, 49, 50, 51, 52, 53, 54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64, 65, 66, 67, 68, 69, 70, 71, 72, 73, 74, 75, 76, 77, 78, 79} \n",
      "\n",
      "Number of unique classes: 80 \n",
      "\n",
      "Train label distribution: Counter({0: 115, 1: 115, 2: 115, 3: 115, 4: 115, 5: 115, 6: 115, 7: 115, 8: 115, 9: 115, 10: 115, 11: 115, 12: 115, 13: 115, 14: 115, 15: 115, 16: 115, 17: 115, 18: 115, 19: 115, 20: 115, 21: 115, 22: 115, 23: 115, 24: 115, 25: 115, 26: 115, 27: 115, 28: 115, 29: 115, 30: 115, 31: 115, 32: 115, 33: 115, 34: 115, 35: 115, 36: 115, 37: 115, 38: 115, 39: 115, 40: 115, 41: 115, 42: 115, 43: 115, 44: 115, 45: 115, 46: 115, 47: 115, 48: 115, 49: 115, 50: 115, 51: 115, 52: 115, 53: 115, 54: 115, 55: 115, 56: 115, 57: 115, 58: 115, 59: 115, 60: 115, 61: 115, 62: 115, 63: 115, 64: 115, 65: 115, 66: 115, 67: 115, 68: 115, 69: 115, 70: 115, 71: 115, 72: 115, 73: 115, 74: 115, 75: 115, 76: 115, 77: 115, 78: 115, 79: 115}) \n",
      "\n",
      "Test label distribution: Counter({0: 10, 1: 10, 2: 10, 3: 10, 4: 10, 5: 10, 6: 10, 7: 10, 8: 10, 9: 10, 10: 10, 11: 10, 12: 10, 13: 10, 14: 10, 15: 10, 16: 10, 17: 10, 18: 10, 19: 10, 20: 10, 21: 10, 22: 10, 23: 10, 24: 10, 25: 10, 26: 10, 27: 10, 28: 10, 29: 10, 30: 10, 31: 10, 32: 10, 33: 10, 34: 10, 35: 10, 36: 10, 37: 10, 38: 10, 39: 10, 40: 10, 41: 10, 42: 10, 43: 10, 44: 10, 45: 10, 46: 10, 47: 10, 48: 10, 49: 10, 50: 10, 51: 10, 52: 10, 53: 10, 54: 10, 55: 10, 56: 10, 57: 10, 58: 10, 59: 10, 60: 10, 61: 10, 62: 10, 63: 10, 64: 10, 65: 10, 66: 10, 67: 10, 68: 10, 69: 10, 70: 10, 71: 10, 72: 10, 73: 10, 74: 10, 75: 10, 76: 10, 77: 10, 78: 10, 79: 10}) \n",
      "\n"
     ]
    }
   ],
   "source": [
    "transform_ = transforms.Compose([ \n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.TrivialAugmentWide(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "# Apply exclusions\n",
    "dataset = ImageFolder('125images', transform=None)\n",
    "ood_dataset = CustomOODDataset(data_path=r\"_ood\",transform=transform_)\n",
    "print(f\"Original dataset size: {len(dataset)}\")\n",
    "print(f\"OOD dataset size: {len(ood_dataset)}\")\n",
    "\n",
    "# Create the train-test split and update label mapping\n",
    "train_dataset, test_dataset, idx_to_label = create_train_test_split_proportional(dataset, test_ratio=0.08, seed=42, transform=transform_)\n",
    "print()\n",
    "print(f\"Train dataset size: {len(train_dataset)}\")\n",
    "print(f\"Test dataset size: {len(test_dataset)}\", '\\n')\n",
    "print(\"Unique class indices in dataset:\", set([sample[1] for sample in dataset.samples]), '\\n')\n",
    "print(\"Number of unique classes:\", len(set([sample[1] for sample in dataset.samples])), '\\n')\n",
    "print(\"Train label distribution:\", Counter([sample[1] for sample in train_dataset.samples]), '\\n')\n",
    "print(\"Test label distribution:\", Counter([sample[1] for sample in test_dataset.samples]), '\\n')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:16:06.936941Z",
     "iopub.status.busy": "2025-02-06T18:16:06.936593Z",
     "iopub.status.idle": "2025-02-06T18:16:06.940854Z",
     "shell.execute_reply": "2025-02-06T18:16:06.939994Z",
     "shell.execute_reply.started": "2025-02-06T18:16:06.936885Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "lr = 0.007323\n",
    "# threshold=0.4796\n",
    "\n",
    "batch_size= 23\n",
    "dropout_rate=0.3\n",
    "\n",
    "\n",
    "# szarnyprobalgatasok\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-02-06T18:16:06.942045Z",
     "iopub.status.busy": "2025-02-06T18:16:06.941764Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fold 1/5\n",
      "Setup ran successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name              | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | feature_extractor | Sequential          | 65.3 M | train\n",
      "1  | ood_classifier    | Sequential          | 2.0 K  | train\n",
      "2  | classifier        | Sequential          | 165 K  | train\n",
      "3  | loss_fn_class     | BCEWithLogitsLoss   | 0      | train\n",
      "4  | loss_fn_ood       | BCEWithLogitsLoss   | 0      | train\n",
      "5  | train_acc         | MultilabelAccuracy  | 0      | train\n",
      "6  | val_acc           | MultilabelAccuracy  | 0      | train\n",
      "7  | train_precision   | MultilabelPrecision | 0      | train\n",
      "8  | val_precision     | MultilabelPrecision | 0      | train\n",
      "9  | train_recall      | MultilabelRecall    | 0      | train\n",
      "10 | val_recall        | MultilabelRecall    | 0      | train\n",
      "11 | train_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1            | MultilabelF1Score   | 0      | train\n",
      "13 | ood_acc           | BinaryAccuracy      | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "168 K     Trainable params\n",
      "65.3 M    Non-trainable params\n",
      "65.5 M    Total params\n",
      "261.938   Total estimated model params size (MB)\n",
      "958       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  2.57it/s]Min prob per class: [0.44864318 0.47446796 0.47121352 0.4830181  0.45217678 0.46485037\n",
      " 0.46559355 0.48167637 0.47121856 0.45722637 0.4750608  0.4640526\n",
      " 0.44614488 0.45403677 0.4743428  0.43470535 0.454275   0.4510584\n",
      " 0.47363344 0.45081773 0.4806931  0.44685844 0.46435332 0.4397898\n",
      " 0.47071484 0.46782994 0.47209623 0.4542673  0.47141275 0.45619273\n",
      " 0.47381327 0.4743326  0.45529974 0.48668107 0.47047064 0.46757802\n",
      " 0.47793588 0.45183706 0.48534647 0.47370687 0.47350484 0.49971476\n",
      " 0.46086103 0.47578844 0.45000303 0.46229392 0.484217   0.46802756\n",
      " 0.48967963 0.46435407 0.45013908 0.47055012 0.46265486 0.47254327\n",
      " 0.46270803 0.46008402 0.46816078 0.44910964 0.46424243 0.47493726\n",
      " 0.4974303  0.48675212 0.45129684 0.47811618 0.46400458 0.47958142\n",
      " 0.48791116 0.44994363 0.48161522 0.46385694 0.4613328  0.48550096\n",
      " 0.44486597 0.48059264 0.4727877  0.43957534 0.47966468 0.47031322\n",
      " 0.49411163 0.47127107 0.47555137]\n",
      "Max prob per class: [0.54813087 0.5459234  0.53880835 0.5462621  0.5095202  0.52940804\n",
      " 0.52067906 0.57028884 0.53552955 0.5205941  0.5359735  0.5327236\n",
      " 0.5217997  0.53413534 0.5530007  0.5010485  0.5161586  0.52654755\n",
      " 0.5344083  0.5161898  0.5366696  0.51799643 0.53113115 0.4990086\n",
      " 0.5455668  0.53016585 0.5244092  0.5203448  0.55981934 0.53574353\n",
      " 0.52225375 0.5467244  0.5184756  0.55292004 0.53522784 0.5387384\n",
      " 0.5501454  0.5296865  0.54498476 0.5388284  0.5499897  0.5502569\n",
      " 0.52553546 0.52792835 0.51656383 0.5219018  0.56280565 0.5240502\n",
      " 0.54042244 0.5076824  0.5196183  0.54084724 0.5461487  0.5330537\n",
      " 0.52616465 0.53227586 0.5240562  0.5048164  0.5194652  0.53893495\n",
      " 0.55767226 0.5592878  0.53536737 0.54109085 0.5142072  0.5308816\n",
      " 0.5378668  0.5096097  0.55509347 0.52728325 0.53338903 0.5464797\n",
      " 0.5114164  0.5377369  0.5244827  0.5039911  0.53517634 0.5225593\n",
      " 0.5926965  0.52395165 0.53027606]\n",
      "Warning: No positive samples for class 2, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 3, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 4, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 5, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 6, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 7, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 8, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 9, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 10, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 11, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 12, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 13, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 14, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 15, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 16, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 17, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 18, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 19, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 20, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 21, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 22, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 23, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 24, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 25, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 26, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 27, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 28, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 29, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 30, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 31, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 32, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 33, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 34, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 35, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 36, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 37, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 38, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 39, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 40, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 41, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 42, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 43, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 44, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 45, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 46, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 47, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 48, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 49, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 50, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 51, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 52, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 53, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 54, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 55, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 56, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 57, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 58, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 59, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 60, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 61, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 62, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 63, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 64, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 65, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 66, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 67, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 68, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 69, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 70, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 71, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 72, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 73, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 74, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 75, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 76, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 77, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 78, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 79, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 80, keeping threshold at 0.5\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 640/640 [03:46<00:00,  2.82it/s, v_num=0, train_loss_step=0.137] Min prob per class: [1.96056899e-05 2.39030996e-05 1.64066096e-05 1.89530183e-05\n",
      " 1.10648507e-05 6.16285388e-06 2.87540788e-05 1.71205011e-05\n",
      " 3.31809206e-05 1.26423784e-05 1.62216420e-05 7.88225316e-06\n",
      " 2.50267076e-05 8.42795816e-06 3.90643581e-05 4.94761662e-05\n",
      " 1.24518874e-05 1.73298431e-05 2.51691908e-05 1.32212353e-05\n",
      " 1.02595304e-05 9.06447713e-06 1.62655269e-05 1.66851987e-05\n",
      " 1.44173582e-05 3.77044526e-05 1.31229654e-05 3.87646214e-05\n",
      " 2.56916519e-05 1.83863449e-05 5.05755179e-06 5.97227045e-05\n",
      " 1.32807554e-05 1.43822162e-05 2.66666248e-05 3.99914716e-05\n",
      " 1.70875355e-05 2.78583520e-05 6.49192443e-05 9.26603570e-06\n",
      " 9.93675258e-06 1.77027741e-05 1.79503822e-05 1.01849537e-05\n",
      " 1.54274658e-05 2.47347671e-05 6.76045602e-05 1.29109676e-05\n",
      " 4.73812215e-05 4.58650029e-05 2.26305547e-05 4.98739200e-06\n",
      " 2.00346058e-05 8.37868083e-06 1.52781249e-05 7.57422913e-06\n",
      " 1.30593935e-05 3.55762968e-05 1.46043112e-05 1.47493165e-05\n",
      " 2.68858166e-05 1.82303247e-05 6.45142291e-06 7.36458896e-05\n",
      " 1.33304839e-05 5.73427178e-06 1.19675969e-05 3.23828353e-05\n",
      " 3.15972575e-05 4.33513997e-05 1.30986209e-05 7.06266728e-05\n",
      " 7.52646065e-06 2.16802509e-05 2.22090021e-05 9.35976004e-06\n",
      " 2.00720133e-05 4.08358901e-05 1.27141480e-03 9.99661370e-06\n",
      " 2.54590941e-05]\n",
      "Max prob per class: [0.39390036 0.38623562 0.37630853 0.3981672  0.3968349  0.3873769\n",
      " 0.39076322 0.4061709  0.5583088  0.41276166 0.39668357 0.37770241\n",
      " 0.4015955  0.38262326 0.39502674 0.39125502 0.38701308 0.37786353\n",
      " 0.39316192 0.38556302 0.3763542  0.4000858  0.40552378 0.3846259\n",
      " 0.39515835 0.40147012 0.38409734 0.39110428 0.40137985 0.38790497\n",
      " 0.39607346 0.38742983 0.3955436  0.38526163 0.3848004  0.39161143\n",
      " 0.39373872 0.38468257 0.38929012 0.39161134 0.39519498 0.38875404\n",
      " 0.38747546 0.38859412 0.3996454  0.43846673 0.4069561  0.38852134\n",
      " 0.39304963 0.38770732 0.39285553 0.3867761  0.37989944 0.40023395\n",
      " 0.3773055  0.3926523  0.3808873  0.397826   0.39280733 0.39102682\n",
      " 0.3918777  0.40919667 0.38907963 0.3899674  0.38074175 0.37930343\n",
      " 0.38257688 0.3840032  0.38439575 0.3939432  0.39146492 0.41709164\n",
      " 0.3992208  0.39377582 0.39888304 0.3766087  0.3803131  0.38653162\n",
      " 0.9993926  0.3870545  0.38517994]\n",
      "Epoch 1: 100%|██████████| 640/640 [03:40<00:00,  2.90it/s, v_num=0, train_loss_step=0.137, val_loss=0.177, val_ood_acc=0.901, val_acc_all=0.993, val_precision_all=0.876, val_recall_all=0.458, val_f1_all=0.602, avg_threshold=0.372, val_acc=0.993, val_precision=0.876, val_recall=0.458, val_f1=0.602, train_loss_epoch=0.308, train_acc=0.989, train_precision=0.562, train_recall=0.430, train_f1=0.487] Min prob per class: [6.0890743e-06 2.3496054e-06 6.2456525e-06 1.5169163e-06 2.3985215e-06\n",
      " 5.4464422e-06 1.6151170e-06 1.4923217e-05 2.0108673e-06 4.2748402e-06\n",
      " 2.3595558e-05 9.6902913e-06 3.8191820e-06 1.1575131e-05 2.8009420e-06\n",
      " 1.2333600e-06 2.9541989e-05 4.1882936e-06 1.8178329e-05 4.0049340e-06\n",
      " 3.2389726e-06 1.6993354e-06 6.3957141e-06 1.0603568e-05 5.8458434e-05\n",
      " 2.6067921e-06 6.4779675e-07 9.9411782e-06 1.3519775e-06 8.0944551e-07\n",
      " 4.2538472e-06 4.0278046e-06 1.0067497e-05 1.3354405e-06 2.9332175e-05\n",
      " 8.2158167e-06 1.8648831e-05 1.5015947e-06 3.7761956e-06 6.2189833e-06\n",
      " 1.1515448e-06 1.3390573e-06 4.1448907e-06 2.1783668e-05 2.0518250e-06\n",
      " 1.2518020e-06 1.3438353e-05 1.1271424e-05 4.9907844e-06 7.9774105e-07\n",
      " 7.5070665e-07 9.9215822e-06 5.0425351e-05 1.6268148e-05 4.9110304e-06\n",
      " 2.2045717e-05 7.5818144e-07 1.8559605e-05 2.8533757e-06 5.6770186e-06\n",
      " 1.9407348e-06 8.3658106e-06 2.2935817e-05 2.0047801e-06 9.0401836e-06\n",
      " 1.3756747e-06 3.6465945e-06 6.0930706e-05 6.7939436e-06 4.6770190e-05\n",
      " 1.6655839e-06 1.3611332e-06 3.7115690e-06 1.6835048e-06 7.1119217e-07\n",
      " 3.8318449e-06 7.7535342e-06 1.4458611e-06 1.8548688e-05 4.9603168e-06\n",
      " 8.3899812e-07]\n",
      "Max prob per class: [0.97169477 0.6964624  0.5939411  0.9143378  0.6937799  0.9178699\n",
      " 0.6772119  0.92157096 0.9682243  0.79215944 0.9618536  0.98464334\n",
      " 0.94877183 0.9141733  0.65765345 0.92103946 0.83654505 0.85211396\n",
      " 0.25905702 0.98662335 0.89434934 0.80713695 0.98163795 0.66166013\n",
      " 0.90215075 0.6400199  0.50403017 0.8145779  0.67910695 0.939412\n",
      " 0.7999606  0.97763616 0.8440801  0.9807248  0.8762167  0.9772363\n",
      " 0.63311493 0.9833454  0.70886916 0.9458752  0.38748634 0.8481072\n",
      " 0.4471445  0.8850945  0.8651036  0.9614836  0.98800045 0.7880713\n",
      " 0.9803324  0.46647963 0.3209433  0.75630075 0.8464846  0.9736777\n",
      " 0.86659914 0.5645823  0.75606364 0.83353573 0.5395623  0.8361088\n",
      " 0.8761184  0.8934914  0.87864697 0.78388596 0.9692022  0.9849999\n",
      " 0.72084063 0.9207866  0.23852539 0.994568   0.8187506  0.6450745\n",
      " 0.46193832 0.8487457  0.8492094  0.49805045 0.760824   0.7267124\n",
      " 0.99997807 0.6925554  0.63564044]\n",
      "Epoch 2: 100%|██████████| 640/640 [03:43<00:00,  2.86it/s, v_num=0, train_loss_step=0.0884, val_loss=0.143, val_ood_acc=0.909, val_acc_all=0.994, val_precision_all=0.894, val_recall_all=0.563, val_f1_all=0.691, avg_threshold=0.321, val_acc=0.994, val_precision=0.894, val_recall=0.563, val_f1=0.691, train_loss_epoch=0.168, train_acc=0.993, train_precision=0.901, train_recall=0.441, train_f1=0.592]Min prob per class: [5.1006177e-06 1.6141793e-06 1.2486353e-06 1.0696826e-05 3.1939203e-07\n",
      " 3.1097977e-07 6.1543983e-07 2.5539038e-07 3.0076719e-06 1.0071328e-06\n",
      " 4.6522652e-07 1.6326647e-06 5.0565689e-07 6.3352729e-07 2.1866144e-07\n",
      " 3.2546856e-07 3.2850394e-07 1.2034658e-07 6.1197420e-06 1.9361723e-06\n",
      " 2.5657633e-07 5.4579311e-07 6.2419866e-07 9.5041383e-07 6.9101714e-07\n",
      " 3.9505253e-06 1.4564993e-07 1.6973902e-06 6.2747868e-06 4.2952379e-07\n",
      " 5.1041417e-07 1.1371955e-05 2.0395089e-06 3.4348909e-06 4.1875905e-06\n",
      " 1.2740032e-06 6.9182488e-07 1.7233260e-06 1.9796257e-06 3.2916219e-07\n",
      " 8.4444729e-08 2.8021361e-06 1.6455200e-06 4.0192399e-06 1.6840396e-06\n",
      " 1.5139985e-06 1.4486587e-06 1.2555921e-06 2.7940800e-06 3.3297601e-06\n",
      " 3.6874860e-07 1.7506982e-06 3.5154046e-06 4.7189394e-07 3.3055749e-06\n",
      " 4.8495218e-07 7.5357402e-06 1.2927246e-07 1.0355916e-06 1.7233640e-07\n",
      " 5.8020913e-07 4.9649032e-07 2.6876609e-07 2.0468101e-06 2.2004995e-06\n",
      " 3.0814056e-06 1.4317377e-06 1.4906100e-06 3.4648629e-06 9.5773282e-07\n",
      " 1.5414848e-07 5.4913824e-07 2.2877709e-06 6.2480018e-07 3.0905967e-06\n",
      " 8.5019765e-06 2.0492416e-06 2.5617825e-07 1.1428882e-05 1.1637640e-06\n",
      " 3.1764176e-07]\n",
      "Max prob per class: [0.9893994  0.9731564  0.9339662  0.99831045 0.96979886 0.9615924\n",
      " 0.85959846 0.85379225 0.9552059  0.87740135 0.9645033  0.9766045\n",
      " 0.9820326  0.84199506 0.897635   0.96335584 0.76388264 0.9310197\n",
      " 0.9123656  0.9951716  0.91947806 0.98799175 0.994296   0.92153424\n",
      " 0.85606843 0.9774214  0.97658145 0.8778969  0.9987638  0.98505646\n",
      " 0.9764949  0.9849675  0.9715731  0.9987857  0.977764   0.99807453\n",
      " 0.88672364 0.9939991  0.9539321  0.97447973 0.84497964 0.97779286\n",
      " 0.8428531  0.9507357  0.9975504  0.99769825 0.9985201  0.8618635\n",
      " 0.99695754 0.8990466  0.95054907 0.9171058  0.9384281  0.997951\n",
      " 0.98060143 0.27708152 0.9930275  0.76067394 0.734633   0.9657611\n",
      " 0.971521   0.8970124  0.8989835  0.98742837 0.96505827 0.9972402\n",
      " 0.9062535  0.98578215 0.48001227 0.9976769  0.967878   0.88448447\n",
      " 0.56928337 0.9947354  0.99801934 0.9724414  0.98783433 0.9838202\n",
      " 0.99997187 0.9404252  0.9265345 ]\n",
      "Epoch 3: 100%|██████████| 640/640 [25:58<00:00,  0.41it/s, v_num=0, train_loss_step=0.145, val_loss=0.129, val_ood_acc=0.918, val_acc_all=0.995, val_precision_all=0.894, val_recall_all=0.643, val_f1_all=0.748, avg_threshold=0.301, val_acc=0.995, val_precision=0.894, val_recall=0.643, val_f1=0.748, train_loss_epoch=0.152, train_acc=0.993, train_precision=0.908, train_recall=0.443, train_f1=0.595] Min prob per class: [6.17388332e-06 2.49997925e-08 5.51318522e-07 1.17557352e-06\n",
      " 6.57502000e-08 3.56574681e-08 7.18163307e-08 2.71997180e-08\n",
      " 2.82912254e-07 7.91245441e-07 1.04165099e-09 2.34585968e-06\n",
      " 2.79065766e-08 2.07415582e-07 1.05114166e-08 7.97450639e-08\n",
      " 5.86389355e-08 3.03009756e-07 3.68919530e-07 2.52919813e-06\n",
      " 9.31548101e-08 1.49632562e-09 1.87240573e-07 8.79167715e-07\n",
      " 1.35187745e-08 4.19579919e-07 6.14523099e-10 4.90515113e-08\n",
      " 1.06627873e-07 4.34109246e-08 4.81594611e-07 1.72008981e-06\n",
      " 8.63656950e-08 5.79619552e-08 4.79516700e-07 2.23351265e-07\n",
      " 1.21734416e-07 8.37391667e-08 3.20331495e-08 9.46489195e-07\n",
      " 5.85137716e-09 2.38768251e-07 1.88130098e-08 8.60076597e-08\n",
      " 3.82672649e-09 1.50277586e-08 1.49234936e-07 1.72874024e-07\n",
      " 7.61012416e-07 3.48358867e-07 3.63287967e-07 2.59566832e-07\n",
      " 1.49634545e-07 5.43658530e-07 3.16318187e-06 2.78346004e-08\n",
      " 1.75521859e-06 3.09495043e-08 1.28996307e-08 6.10014794e-08\n",
      " 1.84251832e-08 1.16989218e-08 1.23836983e-07 2.26113777e-08\n",
      " 2.85557292e-07 2.79797003e-08 2.66700499e-06 2.51661191e-07\n",
      " 4.36617455e-07 7.60981152e-07 4.24065291e-07 4.63782044e-06\n",
      " 8.99991974e-08 2.53084352e-07 1.37067229e-06 3.21592825e-06\n",
      " 1.68329244e-08 2.00871739e-07 9.77901095e-07 9.61921955e-08\n",
      " 1.71979249e-08]\n",
      "Max prob per class: [0.9998171  0.94924164 0.9806903  0.99962354 0.9964316  0.98445743\n",
      " 0.9476731  0.9887059  0.976723   0.99004996 0.9492775  0.99623054\n",
      " 0.9650192  0.9475437  0.95239407 0.9987715  0.8135216  0.9872262\n",
      " 0.95575696 0.9997913  0.9954999  0.9810497  0.9996996  0.99835724\n",
      " 0.95262533 0.97033644 0.88591737 0.994273   0.9965758  0.99506927\n",
      " 0.99785954 0.9985599  0.97047    0.9998828  0.9705955  0.99318004\n",
      " 0.6593015  0.98785454 0.9640427  0.9931288  0.76205486 0.9969687\n",
      " 0.81354773 0.97139186 0.9982338  0.9971355  0.99941814 0.99284333\n",
      " 0.99794    0.9670525  0.99171865 0.9623751  0.9629787  0.9997552\n",
      " 0.99227375 0.60321754 0.9989311  0.9713551  0.7484154  0.98890626\n",
      " 0.9370204  0.86582553 0.9613957  0.9437168  0.99884474 0.99872714\n",
      " 0.9984725  0.9916618  0.42575055 0.99963284 0.9951886  0.9992605\n",
      " 0.83988035 0.9987676  0.9996408  0.9960872  0.9831146  0.9967098\n",
      " 0.9999933  0.8592251  0.92129797]\n",
      "Epoch 4: 100%|██████████| 640/640 [03:34<00:00,  2.99it/s, v_num=0, train_loss_step=0.156, val_loss=0.125, val_ood_acc=0.920, val_acc_all=0.995, val_precision_all=0.901, val_recall_all=0.657, val_f1_all=0.760, avg_threshold=0.289, val_acc=0.995, val_precision=0.901, val_recall=0.657, val_f1=0.760, train_loss_epoch=0.148, train_acc=0.993, train_precision=0.903, train_recall=0.444, train_f1=0.595] Min prob per class: [1.62211893e-06 1.04121849e-08 1.17286191e-07 1.81845678e-07\n",
      " 9.25603860e-09 1.43266297e-08 7.74696751e-09 1.27096005e-08\n",
      " 2.92806153e-08 9.03295643e-08 1.73181380e-08 2.86081445e-07\n",
      " 6.94937174e-09 1.69580630e-07 5.86164317e-09 1.60244209e-08\n",
      " 5.15637417e-08 3.43702652e-07 2.21913439e-07 6.74678915e-07\n",
      " 1.20861126e-08 3.09900869e-08 3.01380823e-08 3.94932220e-09\n",
      " 5.49262381e-07 1.26863668e-07 3.81625931e-09 4.15116602e-07\n",
      " 1.76225932e-08 3.24691314e-07 1.40352043e-07 2.03268641e-07\n",
      " 4.40436914e-08 8.86428268e-07 4.85869691e-07 1.62572888e-07\n",
      " 4.34984457e-08 1.29917561e-08 1.08313223e-07 2.79220025e-09\n",
      " 7.47228679e-09 1.63464904e-08 4.42363159e-08 7.00977338e-08\n",
      " 4.57424072e-08 6.61606805e-08 1.29578950e-07 1.22057131e-07\n",
      " 7.07968129e-09 7.49263851e-09 9.30709874e-08 4.97327157e-08\n",
      " 1.28622344e-06 1.14113625e-08 3.69949653e-08 3.53166087e-07\n",
      " 1.05220681e-06 2.81701187e-08 1.40519478e-06 2.56274735e-08\n",
      " 6.33817976e-08 8.52394422e-08 1.22783783e-09 1.18994542e-07\n",
      " 5.40583869e-07 9.59305524e-08 3.73776849e-08 5.59210944e-09\n",
      " 8.19809998e-08 3.02177227e-07 3.62259755e-07 1.64385270e-07\n",
      " 1.43990945e-07 7.00690928e-09 2.49232642e-07 8.50522497e-07\n",
      " 3.56071126e-07 1.27581572e-07 3.56352439e-06 1.31716831e-07\n",
      " 1.95325320e-08]\n",
      "Max prob per class: [0.99902046 0.95735    0.97079897 0.9999398  0.9891544  0.9995297\n",
      " 0.9715118  0.9854533  0.98781186 0.9810998  0.9861618  0.9966401\n",
      " 0.99582314 0.9982376  0.98304325 0.9998419  0.9980805  0.99155396\n",
      " 0.99205387 0.99994826 0.9949845  0.99948716 0.9994319  0.8037392\n",
      " 0.9914316  0.9757312  0.994557   0.99250925 0.9877676  0.9995436\n",
      " 0.9998807  0.999801   0.9927643  0.9999249  0.99789655 0.9977685\n",
      " 0.7181907  0.99907553 0.99189794 0.98847187 0.81663996 0.9973846\n",
      " 0.94136846 0.9811828  0.9993272  0.99964166 0.9998759  0.99432003\n",
      " 0.96686846 0.9860417  0.996185   0.9828844  0.98713636 0.9989016\n",
      " 0.9955564  0.76074404 0.98938483 0.9595342  0.9220775  0.97711253\n",
      " 0.9757414  0.9924505  0.98672676 0.99911195 0.99980885 0.99976796\n",
      " 0.9891885  0.9811648  0.4503391  0.99962103 0.99886775 0.99757403\n",
      " 0.9702153  0.9998491  0.9998209  0.98857415 0.997288   0.99796885\n",
      " 0.99999297 0.99659973 0.99508756]\n",
      "Epoch 5: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.157, val_loss=0.123, val_ood_acc=0.923, val_acc_all=0.995, val_precision_all=0.880, val_recall_all=0.692, val_f1_all=0.775, avg_threshold=0.291, val_acc=0.995, val_precision=0.880, val_recall=0.692, val_f1=0.775, train_loss_epoch=0.149, train_acc=0.993, train_precision=0.902, train_recall=0.450, train_f1=0.600] Min prob per class: [7.44868146e-07 9.19168297e-09 7.26287563e-09 1.87624103e-08\n",
      " 3.14317923e-08 2.10895704e-10 9.09036457e-10 2.19126362e-09\n",
      " 1.22356070e-07 4.53582905e-09 1.04246989e-09 4.84890883e-09\n",
      " 2.79821277e-10 9.44376577e-09 1.68333472e-07 1.62475100e-09\n",
      " 7.13575474e-08 3.85504828e-09 1.48339678e-08 5.26038718e-07\n",
      " 4.60139787e-10 4.32688019e-09 1.12960370e-10 1.44112615e-08\n",
      " 1.17625156e-07 1.68913550e-09 1.08640297e-08 1.53376578e-09\n",
      " 6.37966924e-09 2.94231501e-08 2.17429523e-08 4.97829774e-07\n",
      " 2.50171066e-08 6.16101969e-09 2.49807619e-09 2.35672921e-08\n",
      " 3.00645198e-09 8.96397356e-09 5.16716518e-08 7.39099537e-09\n",
      " 2.38764408e-09 2.41450371e-09 8.80815421e-08 1.35141081e-08\n",
      " 4.60167016e-10 5.04233846e-08 5.51129233e-07 1.25714044e-08\n",
      " 1.58809872e-08 4.05332390e-09 6.11106898e-09 8.16330470e-08\n",
      " 4.70163535e-08 5.04984605e-08 5.98397492e-08 1.81936173e-08\n",
      " 1.17640360e-08 1.17430741e-08 5.47880186e-10 4.18207975e-08\n",
      " 4.63795613e-10 1.89005767e-09 6.37520203e-10 1.09422466e-07\n",
      " 3.14380877e-08 5.13094234e-09 6.01652346e-08 4.41507408e-09\n",
      " 1.47092578e-07 1.36397986e-07 1.29789761e-08 1.57921278e-07\n",
      " 3.35988126e-09 1.66291303e-09 2.92177962e-08 1.26420417e-07\n",
      " 1.36061232e-07 6.12017033e-08 1.45957245e-06 6.59872423e-09\n",
      " 4.69622874e-09]\n",
      "Max prob per class: [0.9985127  0.9703546  0.98655033 0.999998   0.9990804  0.99903107\n",
      " 0.9875378  0.99143815 0.9946404  0.9933443  0.9643628  0.99831665\n",
      " 0.9927066  0.9872782  0.99551624 0.9997403  0.9968194  0.9839156\n",
      " 0.98866606 0.9999547  0.9963427  0.99949384 0.9970957  0.9913799\n",
      " 0.996479   0.98017144 0.99895895 0.985516   0.9965927  0.9996636\n",
      " 0.99155277 0.99998724 0.9958611  0.99985635 0.9935272  0.99919504\n",
      " 0.9635566  0.99611026 0.998816   0.9983411  0.80914634 0.99607486\n",
      " 0.9732887  0.9980829  0.9880194  0.99921215 0.99992466 0.987323\n",
      " 0.9992718  0.9854441  0.98301536 0.9910324  0.98750556 0.9996158\n",
      " 0.99674726 0.612265   0.997535   0.9891102  0.76913446 0.9987118\n",
      " 0.9918926  0.9844072  0.9878419  0.99964285 0.9993698  0.99968886\n",
      " 0.9762222  0.98607177 0.87480134 0.9998117  0.998993   0.9976407\n",
      " 0.9441226  0.9998118  0.99993443 0.99945265 0.999933   0.99933404\n",
      " 0.99999034 0.9899894  0.9906159 ]\n",
      "Epoch 6: 100%|██████████| 640/640 [29:06<00:00,  0.37it/s, v_num=0, train_loss_step=0.105, val_loss=0.124, val_ood_acc=0.917, val_acc_all=0.995, val_precision_all=0.895, val_recall_all=0.695, val_f1_all=0.783, avg_threshold=0.298, val_acc=0.995, val_precision=0.895, val_recall=0.695, val_f1=0.783, train_loss_epoch=0.143, train_acc=0.993, train_precision=0.896, train_recall=0.449, train_f1=0.599]   Min prob per class: [7.64302797e-07 5.84186521e-09 9.39183487e-09 1.02280207e-09\n",
      " 9.82577131e-09 2.20963403e-09 9.46825396e-10 3.07473780e-09\n",
      " 6.54264110e-09 3.33264727e-09 8.96634600e-10 2.90651645e-08\n",
      " 3.41219941e-09 7.54895524e-09 2.56240384e-09 1.82240223e-10\n",
      " 6.58246735e-10 6.45940617e-08 1.53900970e-07 1.42142909e-07\n",
      " 8.16721091e-09 9.75290959e-10 1.95848143e-10 2.30020314e-09\n",
      " 1.35809310e-08 5.00228801e-08 3.99646011e-10 2.86217574e-08\n",
      " 7.27030525e-09 5.47911760e-09 3.05843684e-09 9.42602583e-08\n",
      " 3.36441519e-09 5.88981983e-08 1.10105773e-08 1.01248865e-07\n",
      " 4.36043557e-08 2.48185272e-09 1.63745673e-09 1.26265243e-09\n",
      " 5.78041703e-09 1.41748724e-09 8.33431324e-10 3.27168920e-07\n",
      " 9.05229502e-11 2.39219200e-10 1.22243353e-08 1.24748567e-08\n",
      " 8.45381631e-09 1.09581932e-09 3.59349217e-09 1.77356583e-08\n",
      " 1.00228199e-07 1.51978288e-08 1.01502934e-07 2.88381532e-08\n",
      " 9.95718841e-10 5.04063458e-09 1.05411520e-08 4.08498213e-09\n",
      " 1.66028968e-08 1.15838166e-08 1.38503153e-09 5.04600184e-09\n",
      " 7.13885617e-09 8.83665197e-10 1.25173063e-08 7.63503039e-08\n",
      " 8.50953512e-08 3.89069150e-08 1.05338904e-09 1.15502816e-08\n",
      " 1.90843927e-08 1.34735656e-09 8.75737616e-09 3.39303625e-08\n",
      " 3.00856442e-08 2.50191956e-09 4.93264096e-08 9.76831060e-09\n",
      " 7.26785077e-09]\n",
      "Max prob per class: [0.9998554  0.9996195  0.9984492  0.99998605 0.9994642  0.9998832\n",
      " 0.9939528  0.9992499  0.9817347  0.9876435  0.9791215  0.9997135\n",
      " 0.99612695 0.9976318  0.9967751  0.9998659  0.970532   0.9996815\n",
      " 0.9935709  0.9999814  0.9899879  0.99994314 0.9997569  0.9896091\n",
      " 0.99865735 0.99580246 0.9926172  0.9940268  0.99990404 0.995565\n",
      " 0.99965847 0.99999    0.9947127  0.9999728  0.9987134  0.9999056\n",
      " 0.96216595 0.99944633 0.9668337  0.9864681  0.9460087  0.99794585\n",
      " 0.9902132  0.99088025 0.9997868  0.9995703  0.99993813 0.9990126\n",
      " 0.999582   0.9930727  0.9936366  0.9882741  0.9956801  0.99995804\n",
      " 0.9952277  0.8414817  0.99934465 0.9915177  0.9258952  0.9758573\n",
      " 0.98540187 0.9986883  0.98116356 0.99795204 0.9996942  0.99995303\n",
      " 0.9954423  0.999067   0.9265173  0.9993686  0.9995969  0.9998702\n",
      " 0.88439035 0.9956617  0.9997662  0.9986078  0.99968004 0.9927164\n",
      " 0.99998164 0.99911815 0.99895155]\n",
      "Epoch 7: 100%|██████████| 640/640 [03:33<00:00,  3.00it/s, v_num=0, train_loss_step=0.0866, val_loss=0.123, val_ood_acc=0.920, val_acc_all=0.995, val_precision_all=0.914, val_recall_all=0.683, val_f1_all=0.782, avg_threshold=0.314, val_acc=0.995, val_precision=0.914, val_recall=0.683, val_f1=0.782, train_loss_epoch=0.140, train_acc=0.993, train_precision=0.897, train_recall=0.453, train_f1=0.602]Min prob per class: [3.59230667e-09 8.11023582e-10 6.25586916e-09 1.20854837e-09\n",
      " 1.06202758e-09 4.77366702e-10 7.03314962e-10 4.14411988e-14\n",
      " 1.92873317e-09 6.99319436e-10 2.83984658e-09 5.41045573e-08\n",
      " 4.87451801e-10 1.00191277e-08 4.04304479e-10 5.32028366e-10\n",
      " 1.39784815e-08 5.91085958e-10 1.55518247e-08 5.71084229e-08\n",
      " 9.10053088e-10 3.05847714e-13 5.67420333e-10 7.88047613e-11\n",
      " 2.38233664e-08 5.73782633e-10 8.04409817e-10 3.68709685e-09\n",
      " 4.26383018e-09 3.28227334e-10 5.77694975e-11 4.39412196e-09\n",
      " 6.34988673e-10 2.11975095e-08 3.05014822e-08 4.33200853e-09\n",
      " 2.99756131e-08 1.63993530e-09 8.99860741e-10 3.48140267e-10\n",
      " 2.96613689e-09 1.38727874e-09 4.27699876e-10 3.77476610e-08\n",
      " 4.89077834e-10 2.64480748e-09 1.12043741e-09 5.99525651e-10\n",
      " 5.80217741e-09 1.25309818e-10 1.19264314e-08 2.02405905e-11\n",
      " 1.00956861e-08 9.61494084e-09 3.77785589e-08 3.12909941e-11\n",
      " 2.97603142e-09 1.06579329e-10 6.93101757e-11 7.93772936e-10\n",
      " 3.05996539e-09 9.80522885e-10 3.67594019e-12 1.02578195e-08\n",
      " 4.65550709e-09 3.36821925e-11 2.14280282e-09 2.82055868e-09\n",
      " 2.09885478e-08 2.26303484e-07 1.52897550e-09 3.23882199e-10\n",
      " 6.64654776e-09 6.35698827e-10 2.17655406e-06 2.80546237e-06\n",
      " 1.10095266e-08 1.65765221e-10 1.88156844e-06 4.41998864e-08\n",
      " 2.94988925e-12]\n",
      "Max prob per class: [0.99993193 0.9993512  0.99363685 0.99997604 0.99732566 0.9984687\n",
      " 0.9741178  0.9929518  0.9906839  0.98960143 0.998319   0.9999695\n",
      " 0.9896952  0.9895755  0.99071574 0.9999769  0.998147   0.9938944\n",
      " 0.9564021  0.99997354 0.99524325 0.99726236 0.9998869  0.99970514\n",
      " 0.9925649  0.93537074 0.9946373  0.9927125  0.999912   0.9973502\n",
      " 0.9958189  0.99992085 0.998708   0.99997246 0.99722964 0.9997652\n",
      " 0.9972198  0.9997404  0.9958781  0.9973105  0.87921226 0.99777573\n",
      " 0.9982784  0.98712033 0.9996105  0.99991846 0.99910897 0.9974955\n",
      " 0.99934334 0.99163026 0.99652904 0.9980762  0.991222   0.99993324\n",
      " 0.99655366 0.8253376  0.9995449  0.9766852  0.96136373 0.9976037\n",
      " 0.9958508  0.99819213 0.99345773 0.9971655  0.9941565  0.9994937\n",
      " 0.96786904 0.9989999  0.79409456 0.99998283 0.99622005 0.998831\n",
      " 0.9536461  0.99961615 0.9999964  0.9998801  0.9999405  0.99200684\n",
      " 0.99999344 0.99751604 0.99738604]\n",
      "Epoch 8: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.0783, val_loss=0.128, val_ood_acc=0.917, val_acc_all=0.995, val_precision_all=0.880, val_recall_all=0.701, val_f1_all=0.780, avg_threshold=0.322, val_acc=0.995, val_precision=0.880, val_recall=0.701, val_f1=0.780, train_loss_epoch=0.141, train_acc=0.993, train_precision=0.893, train_recall=0.459, train_f1=0.606]Min prob per class: [6.71961198e-09 2.00054015e-10 8.82393547e-10 8.08915246e-09\n",
      " 2.32844943e-09 3.23680700e-11 8.31374761e-11 1.17073540e-09\n",
      " 1.24274075e-08 2.83997912e-11 2.04110751e-09 4.91108210e-10\n",
      " 3.28896821e-09 3.41874662e-09 1.48597579e-09 7.33059391e-10\n",
      " 8.04471600e-09 2.42437004e-09 2.14813998e-08 5.39143912e-08\n",
      " 1.70370468e-10 1.04116932e-11 4.45869675e-10 2.59963517e-09\n",
      " 2.61785638e-09 1.82729778e-08 6.97772562e-10 6.46719969e-11\n",
      " 4.44285372e-11 4.12523106e-11 6.48731913e-09 5.05795299e-08\n",
      " 8.36487379e-10 5.09121217e-11 1.56237967e-09 1.09711662e-08\n",
      " 5.15883114e-09 6.91329589e-08 1.84987876e-11 8.34795078e-09\n",
      " 8.83795437e-09 7.85532972e-10 3.18041660e-09 3.17251603e-09\n",
      " 2.57534636e-11 3.34026945e-10 2.87015290e-11 3.99760954e-11\n",
      " 7.52562279e-09 2.29661734e-09 4.92553065e-09 3.39691164e-10\n",
      " 1.63178626e-08 3.98313521e-10 6.22504388e-07 4.97433250e-09\n",
      " 1.79138304e-09 5.15979065e-11 9.03453368e-10 1.17359997e-08\n",
      " 1.15219299e-10 1.06555294e-11 4.64350869e-09 3.42353451e-10\n",
      " 6.04576513e-08 1.69110101e-10 1.19176322e-08 1.69928446e-10\n",
      " 1.09153886e-09 2.54545807e-08 1.35397915e-10 7.80679343e-08\n",
      " 1.64091762e-09 1.39457712e-09 7.51457829e-09 4.62851180e-09\n",
      " 2.24867378e-08 9.80960535e-10 2.28146337e-06 1.25151347e-08\n",
      " 1.27257120e-11]\n",
      "Max prob per class: [0.99938846 0.99958533 0.99401444 0.99999964 0.9996074  0.997733\n",
      " 0.97904897 0.99962497 0.9751556  0.99133533 0.988529   0.9978484\n",
      " 0.99618053 0.9993801  0.9990011  0.9999933  0.9945364  0.9996482\n",
      " 0.982235   0.9999927  0.9925679  0.99795413 0.99989927 0.99995494\n",
      " 0.99847585 0.9989791  0.9915386  0.99577314 0.9997105  0.99823475\n",
      " 0.999526   0.99996483 0.9991911  0.999925   0.99886596 0.9999974\n",
      " 0.9809239  0.9999862  0.9972905  0.9995521  0.98577094 0.996289\n",
      " 0.99957806 0.9922122  0.9998481  0.9996433  0.9999888  0.99857044\n",
      " 0.999065   0.99963903 0.9933924  0.9814811  0.9958288  0.99992406\n",
      " 0.99973685 0.97873425 0.9992982  0.97253674 0.9654875  0.99939084\n",
      " 0.97853124 0.9964239  0.99973875 0.9988456  0.9995522  0.9998809\n",
      " 0.98506594 0.9928237  0.7624322  0.99998164 0.9998267  0.9995042\n",
      " 0.9933935  0.9999778  0.9976283  0.9987607  0.9999455  0.998548\n",
      " 0.9999976  0.9997743  0.98885846]\n",
      "Epoch 9: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.0404, val_loss=0.126, val_ood_acc=0.919, val_acc_all=0.995, val_precision_all=0.886, val_recall_all=0.719, val_f1_all=0.794, avg_threshold=0.340, val_acc=0.995, val_precision=0.886, val_recall=0.719, val_f1=0.794, train_loss_epoch=0.137, train_acc=0.993, train_precision=0.895, train_recall=0.459, train_f1=0.606]Min prob per class: [3.96159834e-08 6.56156726e-11 1.59612767e-08 3.03742675e-09\n",
      " 8.66332960e-12 3.39856723e-12 4.36009273e-09 1.66949926e-11\n",
      " 3.37047212e-09 1.34435754e-10 5.68845304e-10 2.20363519e-10\n",
      " 1.01308018e-10 2.47843723e-09 2.51538412e-09 3.22015461e-12\n",
      " 5.08288966e-10 4.74271733e-11 7.33803573e-10 6.41624176e-09\n",
      " 4.99280617e-11 3.03647975e-11 1.13851914e-11 8.79925111e-09\n",
      " 1.21833388e-09 2.31828823e-09 4.22622870e-09 3.07948155e-12\n",
      " 5.03102456e-11 2.60302252e-10 5.78518955e-10 1.23801802e-09\n",
      " 6.16709461e-10 4.91255259e-10 4.61257560e-10 9.62850333e-09\n",
      " 1.28130576e-10 5.59428281e-10 3.16472265e-11 1.03535465e-08\n",
      " 2.65950123e-10 2.77587805e-12 1.24099134e-10 8.65820193e-10\n",
      " 2.02160733e-09 1.48713181e-10 1.45505419e-09 1.02207076e-09\n",
      " 5.87218407e-09 9.47907752e-10 4.59358729e-09 7.70282416e-09\n",
      " 4.28114699e-09 3.53086865e-10 3.64531019e-08 3.02994518e-09\n",
      " 8.89682439e-10 5.91343272e-13 2.22623683e-10 6.43169571e-08\n",
      " 3.29028081e-12 6.42678064e-11 1.41473621e-11 3.81798315e-10\n",
      " 3.12406367e-09 5.04543629e-12 1.82810993e-08 1.79607260e-10\n",
      " 2.52256077e-10 1.34033149e-08 1.46247042e-11 1.18853682e-09\n",
      " 8.21085067e-09 9.52361731e-11 1.48591339e-09 1.71502879e-09\n",
      " 3.29370131e-09 1.63173242e-09 3.55096432e-08 1.78930593e-10\n",
      " 6.21727984e-13]\n",
      "Max prob per class: [0.9999975  0.999559   0.99230886 0.999998   0.9986737  0.99139774\n",
      " 0.99907506 0.99736965 0.97862726 0.99758613 0.99813074 0.9998741\n",
      " 0.98811674 0.99809533 0.9990958  0.9999566  0.9970619  0.94949156\n",
      " 0.9866659  0.99988544 0.9976413  0.9999224  0.9991371  0.9998313\n",
      " 0.9969381  0.9897962  0.9999273  0.997437   0.9998061  0.9997271\n",
      " 0.9993988  0.9999287  0.9981324  0.99997604 0.9996076  0.9997186\n",
      " 0.9875723  0.99907386 0.9956572  0.99973136 0.9691984  0.998543\n",
      " 0.99840087 0.9923522  0.999997   0.9998053  0.99998033 0.99863017\n",
      " 0.99985445 0.9967452  0.9915394  0.9958847  0.97830373 0.9999285\n",
      " 0.999962   0.9900359  0.99990034 0.99881834 0.9363932  0.999729\n",
      " 0.99827194 0.99982053 0.9982578  0.9976617  0.99997485 0.99997675\n",
      " 0.9969342  0.998256   0.56177276 0.9999471  0.996662   0.9999553\n",
      " 0.9957165  0.99994147 0.99975175 0.9999262  0.99973863 0.99996674\n",
      " 0.99997556 0.9990075  0.99810755]\n",
      "Epoch 10: 100%|██████████| 640/640 [03:36<00:00,  2.96it/s, v_num=0, train_loss_step=0.168, val_loss=0.126, val_ood_acc=0.924, val_acc_all=0.995, val_precision_all=0.910, val_recall_all=0.701, val_f1_all=0.792, avg_threshold=0.333, val_acc=0.995, val_precision=0.910, val_recall=0.701, val_f1=0.792, train_loss_epoch=0.134, train_acc=0.993, train_precision=0.895, train_recall=0.457, train_f1=0.606] Min prob per class: [1.40371992e-09 7.44821094e-09 3.63873515e-10 1.01536099e-10\n",
      " 3.19010063e-09 1.37159901e-11 1.47908318e-11 1.68190028e-11\n",
      " 1.06067533e-09 4.94040121e-11 4.11592760e-10 1.61316838e-09\n",
      " 4.06615269e-10 1.38890699e-09 2.12305763e-11 1.06109774e-10\n",
      " 7.37309602e-10 7.44522083e-11 4.15941628e-08 3.71087250e-09\n",
      " 3.21253718e-11 4.86479780e-12 2.21777648e-11 3.09672912e-11\n",
      " 1.55821922e-09 1.19958721e-09 1.27490657e-10 2.53583654e-10\n",
      " 1.74517512e-10 6.89373614e-10 1.69794412e-09 1.38025626e-08\n",
      " 5.98217448e-11 1.23839106e-09 2.48674747e-09 3.93478281e-08\n",
      " 4.73578865e-09 1.15644061e-09 6.03414402e-11 2.06887927e-11\n",
      " 3.71297326e-10 4.88462548e-10 2.21532959e-10 2.49008436e-09\n",
      " 1.46940543e-10 2.68129208e-09 5.72108472e-10 1.24136717e-11\n",
      " 2.01061052e-08 2.28331201e-10 4.81071294e-10 1.67193248e-09\n",
      " 5.85506976e-10 2.43556175e-10 1.82744664e-09 1.40371725e-09\n",
      " 9.35844041e-11 1.78294116e-11 1.32653100e-09 5.04790543e-10\n",
      " 6.13549878e-10 1.65876462e-11 2.33665226e-10 1.50073287e-09\n",
      " 2.97363112e-09 1.71029683e-11 5.55231772e-09 1.29281730e-09\n",
      " 2.73101186e-09 2.25636860e-08 6.46890250e-11 3.17718178e-11\n",
      " 1.60305397e-11 1.43543233e-10 2.54577248e-09 3.22219318e-09\n",
      " 8.62668870e-09 4.75935069e-09 6.67416089e-07 2.14871676e-09\n",
      " 3.57861518e-09]\n",
      "Max prob per class: [0.9997832  0.99975127 0.9975183  0.9999993  0.9997954  0.99791735\n",
      " 0.98924726 0.99956447 0.97248375 0.9993411  0.9987023  0.99996996\n",
      " 0.99609    0.9982615  0.9974738  0.9999939  0.9982419  0.99921083\n",
      " 0.9968309  0.99996865 0.9959967  0.99915826 0.99987304 0.9992631\n",
      " 0.99912053 0.9837931  0.99964213 0.99736917 0.99989617 0.99971503\n",
      " 0.999811   0.9999609  0.9991067  0.9999924  0.9963594  0.9999945\n",
      " 0.9980307  0.99975914 0.9988932  0.9981394  0.9438627  0.99939525\n",
      " 0.9991229  0.99906594 0.9999925  0.99995327 0.99998045 0.996487\n",
      " 0.99942756 0.9976999  0.9860801  0.9974172  0.99491924 0.9997423\n",
      " 0.9941948  0.9617483  0.99929595 0.9990583  0.9494827  0.99985373\n",
      " 0.9999796  0.99706656 0.99977857 0.9996972  0.99996793 0.99986875\n",
      " 0.99955696 0.99971336 0.77410406 0.99998736 0.99989915 0.9972779\n",
      " 0.9900096  0.9998981  0.9999417  0.9996855  0.9998877  0.99987185\n",
      " 0.99997675 0.9997161  0.9994566 ]\n",
      "Epoch 11: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.0644, val_loss=0.120, val_ood_acc=0.925, val_acc_all=0.996, val_precision_all=0.903, val_recall_all=0.720, val_f1_all=0.801, avg_threshold=0.341, val_acc=0.996, val_precision=0.903, val_recall=0.720, val_f1=0.801, train_loss_epoch=0.137, train_acc=0.993, train_precision=0.899, train_recall=0.460, train_f1=0.609]Min prob per class: [3.6174856e-09 2.4846045e-09 1.3755433e-09 1.8150663e-09 1.2917666e-10\n",
      " 4.1801437e-11 5.3442525e-11 4.1545972e-11 4.4154594e-10 2.7345759e-10\n",
      " 8.9601571e-10 2.9254235e-10 1.0041908e-09 6.7665268e-10 2.4241988e-09\n",
      " 2.8707987e-11 1.1789615e-09 1.3401008e-09 1.3602499e-08 7.2521928e-08\n",
      " 6.0368521e-10 2.8164651e-11 2.5061736e-10 3.0268937e-10 9.0934510e-10\n",
      " 7.0284562e-10 1.2134954e-10 2.5261009e-09 8.4780294e-11 1.4338651e-09\n",
      " 3.0060621e-09 4.1713233e-09 3.7910469e-10 1.9890233e-10 9.3297514e-10\n",
      " 2.5020099e-09 1.2771116e-10 8.9797469e-10 1.4904848e-11 5.8767863e-10\n",
      " 7.1886254e-11 7.9296717e-12 2.1139025e-10 2.2561264e-09 6.7733191e-10\n",
      " 9.0485880e-10 1.9269757e-09 1.6205690e-10 1.0514905e-08 3.6548815e-09\n",
      " 3.8062747e-11 4.4633169e-10 4.8471702e-09 4.6665016e-10 7.0833714e-08\n",
      " 1.1137938e-09 5.0162230e-10 2.2866186e-11 4.8707701e-11 4.8449240e-09\n",
      " 4.7890681e-11 1.5819540e-10 4.8164580e-11 4.1755799e-10 2.5448985e-09\n",
      " 1.4643357e-10 5.3472892e-08 2.5421418e-10 2.6085645e-10 4.2119370e-09\n",
      " 5.1397359e-10 9.9028685e-10 1.5171717e-10 1.0076400e-10 3.0029961e-09\n",
      " 4.3622783e-10 3.2211627e-08 3.3613926e-10 1.5648012e-06 2.9747147e-09\n",
      " 4.0663892e-12]\n",
      "Max prob per class: [0.99990296 0.9973834  0.9994281  0.9999995  0.99826044 0.9998783\n",
      " 0.99415106 0.9987425  0.9832736  0.9994516  0.9918528  0.9998914\n",
      " 0.9987582  0.997546   0.9999138  0.9983114  0.9895605  0.9995185\n",
      " 0.99672097 0.9999795  0.99771947 0.9998241  0.9997273  0.9651235\n",
      " 0.9965546  0.9990631  0.9966313  0.99837506 0.99999785 0.9995821\n",
      " 0.9997397  0.999943   0.9988084  0.9999645  0.9971955  0.9998934\n",
      " 0.9555698  0.99988294 0.9981823  0.99291974 0.98147285 0.999071\n",
      " 0.9956436  0.9936073  0.9998209  0.99989355 0.9999875  0.9950304\n",
      " 0.99858546 0.9998528  0.98045886 0.99746275 0.9903386  0.99978536\n",
      " 0.999866   0.96884984 0.9988481  0.9908907  0.65573853 0.999742\n",
      " 0.9904192  0.99984455 0.9994106  0.9999256  0.9987503  0.99993753\n",
      " 0.99978787 0.99565256 0.69376504 0.999967   0.99835974 0.99709845\n",
      " 0.9818764  0.99990904 0.99996996 0.9739865  0.99983776 0.9998833\n",
      " 0.99999094 0.99997234 0.9907362 ]\n",
      "Epoch 12: 100%|██████████| 640/640 [03:36<00:00,  2.96it/s, v_num=0, train_loss_step=0.0854, val_loss=0.127, val_ood_acc=0.916, val_acc_all=0.996, val_precision_all=0.897, val_recall_all=0.724, val_f1_all=0.801, avg_threshold=0.341, val_acc=0.996, val_precision=0.897, val_recall=0.724, val_f1=0.801, train_loss_epoch=0.133, train_acc=0.993, train_precision=0.900, train_recall=0.463, train_f1=0.611]Min prob per class: [5.0508744e-09 2.4020680e-10 2.4357580e-09 9.9596387e-10 8.3100853e-11\n",
      " 8.2221736e-11 8.3903079e-11 1.0728655e-12 1.8116550e-10 1.7727489e-10\n",
      " 6.1923154e-11 1.4974783e-10 5.6893207e-10 5.4096626e-11 1.6293494e-10\n",
      " 3.0180778e-11 5.9999783e-11 5.4264171e-10 7.8624982e-09 3.3333958e-10\n",
      " 2.5381433e-11 1.7888127e-12 7.7135451e-12 1.8361324e-09 1.7417531e-10\n",
      " 1.6241563e-09 9.1524264e-09 1.1598617e-10 2.3545410e-12 6.4620737e-10\n",
      " 7.1325101e-10 5.2367337e-09 2.7419925e-10 2.1158630e-10 5.7937064e-09\n",
      " 3.2627118e-10 7.8006192e-11 4.2084258e-10 1.2262995e-10 3.8311847e-11\n",
      " 4.2718603e-11 6.1988498e-11 2.6764556e-11 1.5906232e-09 1.2898246e-09\n",
      " 5.7267946e-10 6.0744569e-11 9.6457356e-11 1.6168810e-08 4.1802155e-11\n",
      " 1.5775015e-09 1.0637142e-09 1.0957008e-08 2.0018494e-10 2.4182183e-09\n",
      " 7.7165846e-10 2.3648601e-09 3.0492074e-11 5.9537056e-11 1.3474285e-09\n",
      " 2.0903788e-10 1.0758476e-11 1.2552290e-11 2.3096410e-10 3.4201666e-10\n",
      " 3.1604459e-11 2.3593964e-08 5.5229075e-11 1.2959255e-09 1.5886471e-08\n",
      " 7.7305461e-11 4.6558241e-10 5.6934759e-11 7.0895650e-11 7.9930157e-10\n",
      " 1.2002074e-09 1.9443732e-08 5.1455429e-10 2.7683842e-07 3.6186620e-11\n",
      " 2.1522614e-12]\n",
      "Max prob per class: [0.99998736 0.9997578  0.99965537 0.9999727  0.9993857  0.99976736\n",
      " 0.9926479  0.9991611  0.98136824 0.99424416 0.99805945 0.9998242\n",
      " 0.9989672  0.99782795 0.9994037  0.9999981  0.9998354  0.99955434\n",
      " 0.9984989  0.9999738  0.9974826  0.99976224 0.9994103  0.999223\n",
      " 0.9982481  0.99933785 0.9996499  0.99727577 0.99983656 0.9996613\n",
      " 0.9998267  0.99995756 0.99967575 0.99970716 0.99938    0.99996424\n",
      " 0.9784728  0.99995005 0.9996903  0.9997147  0.6913798  0.99682176\n",
      " 0.9992762  0.9964296  0.99999905 0.99979    0.99999166 0.99843675\n",
      " 0.9999455  0.9939029  0.99975914 0.9961862  0.995031   0.9999378\n",
      " 0.99836785 0.97876716 0.9997204  0.9995486  0.8894338  0.99878496\n",
      " 0.9996846  0.99934906 0.99198115 0.9999914  0.9982816  0.9999485\n",
      " 0.9999341  0.9982672  0.80109257 0.99999905 0.99995565 0.99989676\n",
      " 0.99018365 0.99994504 0.99976987 0.996002   0.9999082  0.999097\n",
      " 0.9999931  0.999819   0.99534225]\n",
      "Epoch 13: 100%|██████████| 640/640 [03:34<00:00,  2.99it/s, v_num=0, train_loss_step=0.334, val_loss=0.119, val_ood_acc=0.924, val_acc_all=0.996, val_precision_all=0.899, val_recall_all=0.719, val_f1_all=0.799, avg_threshold=0.348, val_acc=0.996, val_precision=0.899, val_recall=0.719, val_f1=0.799, train_loss_epoch=0.130, train_acc=0.993, train_precision=0.903, train_recall=0.459, train_f1=0.608] Min prob per class: [1.3974928e-07 4.1480194e-10 2.8642604e-09 5.0683799e-09 5.4490168e-10\n",
      " 7.6028746e-11 1.2289314e-10 1.6872727e-11 1.9372157e-10 1.3810238e-10\n",
      " 2.5250482e-11 4.9879539e-10 3.2912570e-10 1.3425978e-09 1.7068857e-10\n",
      " 1.6386267e-11 4.0952311e-10 8.8659424e-10 3.0359095e-09 1.9756595e-08\n",
      " 8.0092794e-11 3.6715346e-11 5.0485122e-10 1.3105288e-09 1.6080404e-10\n",
      " 3.5062204e-09 2.4448157e-09 2.8984849e-11 5.3218943e-11 8.9393444e-11\n",
      " 4.2252193e-10 2.8463862e-09 6.6607164e-11 2.5857021e-11 4.6947846e-10\n",
      " 2.7749560e-09 3.6634064e-11 1.5514043e-09 4.6592688e-11 6.3235923e-12\n",
      " 6.5826268e-11 3.9267020e-11 2.5812932e-10 2.0900572e-09 8.5778017e-11\n",
      " 1.6912680e-11 1.4631325e-10 7.1386036e-11 3.0538376e-09 3.9715586e-10\n",
      " 2.8258185e-10 1.8917481e-10 3.4536203e-09 8.0625301e-10 3.7338190e-09\n",
      " 2.7750285e-10 3.3421560e-10 1.9540895e-10 1.3112952e-11 8.6572771e-10\n",
      " 1.0159358e-11 2.9413381e-12 8.9725222e-10 2.4755731e-10 4.2949528e-09\n",
      " 2.6453056e-10 2.5719962e-09 2.0018724e-10 5.4697596e-10 2.8178501e-08\n",
      " 6.2824834e-10 3.6874254e-10 5.2861427e-11 5.1491235e-11 2.1767712e-09\n",
      " 4.1987187e-09 1.7734813e-08 5.6966609e-10 3.8949989e-07 1.4340019e-09\n",
      " 3.7385524e-12]\n",
      "Max prob per class: [0.9999751  0.996525   0.9966859  0.99999523 0.99931324 0.99936694\n",
      " 0.9946785  0.9995357  0.9833076  0.9942661  0.99662906 0.9997218\n",
      " 0.99896014 0.99589205 0.9975853  0.9999906  0.9991641  0.9997445\n",
      " 0.9931938  0.99999523 0.99906236 0.99996483 0.9999453  0.999102\n",
      " 0.998485   0.99904555 0.99871576 0.99814904 0.9999336  0.99889994\n",
      " 0.9998704  0.9999845  0.9977884  0.99998856 0.9962961  0.9999237\n",
      " 0.99752    0.9999062  0.9977137  0.99907076 0.9697213  0.99921906\n",
      " 0.99165076 0.9967964  0.99999297 0.9996482  0.9998814  0.99886334\n",
      " 0.9994253  0.99980646 0.9980447  0.99853766 0.9839368  0.99989974\n",
      " 0.99993706 0.93408746 0.99944764 0.9997303  0.9426343  0.99876297\n",
      " 0.9994522  0.99833137 0.99758184 0.9998381  0.99998486 0.999974\n",
      " 0.9990741  0.99709404 0.9069658  0.9999918  0.9999614  0.99985147\n",
      " 0.98637784 0.999944   0.9999534  0.9997826  0.9999565  0.99977237\n",
      " 0.99999666 0.9995005  0.9993056 ]\n",
      "Epoch 14: 100%|██████████| 640/640 [03:33<00:00,  3.00it/s, v_num=0, train_loss_step=0.175, val_loss=0.125, val_ood_acc=0.923, val_acc_all=0.996, val_precision_all=0.894, val_recall_all=0.729, val_f1_all=0.803, avg_threshold=0.349, val_acc=0.996, val_precision=0.894, val_recall=0.729, val_f1=0.803, train_loss_epoch=0.129, train_acc=0.993, train_precision=0.913, train_recall=0.463, train_f1=0.614] Min prob per class: [4.40750797e-10 1.28293998e-09 6.80700440e-10 1.53543700e-09\n",
      " 1.41166055e-11 3.72073448e-11 3.13452354e-11 7.19798249e-11\n",
      " 4.43418462e-11 1.55159074e-09 6.68829020e-11 1.45195675e-10\n",
      " 2.92702529e-10 8.72108330e-10 3.40954986e-10 1.46199632e-11\n",
      " 5.31028721e-10 4.04858536e-10 1.67070908e-08 2.64972169e-08\n",
      " 1.84176799e-10 1.40781544e-11 1.08214354e-10 1.86052340e-10\n",
      " 8.63411981e-11 3.81518039e-10 9.16989054e-11 1.71882560e-11\n",
      " 3.76712168e-11 9.90931115e-10 2.60319127e-10 1.61968678e-08\n",
      " 3.20405119e-10 8.07305195e-11 3.16222182e-09 6.33593789e-10\n",
      " 2.18138785e-10 7.09041714e-11 7.43318428e-12 3.97769734e-11\n",
      " 3.57352509e-10 1.90639800e-12 3.28896022e-10 7.06663528e-09\n",
      " 3.28060704e-11 2.64367139e-10 3.40114520e-10 3.16258061e-11\n",
      " 2.19207452e-09 1.00721341e-10 1.78227814e-11 6.98694103e-10\n",
      " 3.77972764e-09 1.17394690e-08 2.65182876e-09 1.29792410e-09\n",
      " 5.26781674e-10 6.99744637e-11 5.13510123e-10 1.96567429e-10\n",
      " 7.45489781e-11 6.69865042e-13 2.57471770e-11 3.61646789e-11\n",
      " 3.21550719e-09 3.25613252e-11 1.12251441e-09 3.10137083e-10\n",
      " 1.01200692e-09 5.08298914e-09 3.64000392e-11 7.44875758e-11\n",
      " 2.23504409e-10 1.07131658e-10 2.08465356e-10 2.89078028e-09\n",
      " 1.73295369e-08 5.69011283e-10 2.39063297e-07 7.95091271e-10\n",
      " 1.47245428e-11]\n",
      "Max prob per class: [0.9997371  0.9969723  0.9990429  0.99999964 0.9995921  0.99920565\n",
      " 0.9927736  0.9997843  0.9820354  0.99969053 0.99662423 0.9999238\n",
      " 0.99123806 0.99778086 0.9998543  0.999846   0.9996321  0.99885416\n",
      " 0.9977718  0.999982   0.999127   0.99984396 0.99984026 0.9975171\n",
      " 0.986459   0.99540424 0.99543935 0.99952435 0.9999678  0.99973935\n",
      " 0.99992406 0.999995   0.99815243 0.99999464 0.99974376 0.99993014\n",
      " 0.99541914 0.99939895 0.9991279  0.9998983  0.9821146  0.99925953\n",
      " 0.99863905 0.99816364 0.99998355 0.9998683  0.9999962  0.99942905\n",
      " 0.99895144 0.99971575 0.9933549  0.99880385 0.9960311  0.9998677\n",
      " 0.99948835 0.9436622  0.994042   0.984689   0.9885437  0.99859864\n",
      " 0.98968047 0.9979607  0.9976623  0.9997491  0.99992204 0.9999721\n",
      " 0.9971324  0.9988261  0.8938007  0.9999697  0.9999882  0.9994647\n",
      " 0.9866919  0.9995577  0.99998224 0.99893326 0.9997297  0.999744\n",
      " 0.99997616 0.99869746 0.99855524]\n",
      "Epoch 15: 100%|██████████| 640/640 [03:34<00:00,  2.99it/s, v_num=0, train_loss_step=0.0883, val_loss=0.119, val_ood_acc=0.920, val_acc_all=0.996, val_precision_all=0.907, val_recall_all=0.727, val_f1_all=0.807, avg_threshold=0.355, val_acc=0.996, val_precision=0.907, val_recall=0.727, val_f1=0.807, train_loss_epoch=0.129, train_acc=0.993, train_precision=0.911, train_recall=0.459, train_f1=0.611]Min prob per class: [5.4724021e-09 4.6596088e-10 4.9963805e-10 9.8287833e-10 1.6581961e-10\n",
      " 5.4830435e-11 7.7401516e-11 2.6575782e-11 2.8445127e-10 7.0342322e-11\n",
      " 1.1269761e-10 1.2389173e-10 1.8887773e-10 7.0008405e-10 9.9174370e-11\n",
      " 4.2188232e-11 2.9963751e-10 1.9548885e-09 1.0580152e-07 2.8800061e-08\n",
      " 8.7212453e-11 1.0647870e-11 1.6479584e-10 6.1833366e-10 5.7500310e-10\n",
      " 1.5051713e-08 4.3014867e-09 2.0413820e-11 2.7587412e-11 9.6261984e-11\n",
      " 8.6054186e-10 5.2767897e-09 1.1667868e-10 2.2938700e-11 1.8497288e-09\n",
      " 1.7615125e-09 2.9909120e-09 4.4416187e-10 1.1600105e-09 7.8287055e-10\n",
      " 9.1832632e-11 6.6123024e-10 6.8066254e-11 8.8040695e-09 1.0647592e-10\n",
      " 9.0356705e-10 3.0109536e-11 4.6443873e-12 7.9972090e-10 1.5668164e-10\n",
      " 3.3283518e-10 1.8922355e-10 2.3346221e-09 8.0075430e-10 2.0890738e-08\n",
      " 8.6643803e-10 3.9361692e-09 2.9579138e-11 5.7267042e-11 3.4727830e-09\n",
      " 3.7110461e-11 1.3329870e-11 2.5921857e-11 3.8304929e-10 1.6392034e-09\n",
      " 4.3286280e-10 4.2403512e-09 4.5221438e-10 3.6799934e-09 1.1418330e-08\n",
      " 2.2392203e-11 5.5960975e-10 4.3403659e-11 7.1207616e-11 4.2052823e-09\n",
      " 3.3816636e-09 5.4785017e-09 1.1972762e-09 9.1549845e-07 7.5621231e-10\n",
      " 3.0756305e-12]\n",
      "Max prob per class: [0.9999393  0.99860966 0.9980945  0.9998266  0.99966705 0.9996555\n",
      " 0.9988747  0.9994276  0.98576456 0.9968748  0.99891603 0.999742\n",
      " 0.99904126 0.99777645 0.99978703 0.99999475 0.9950079  0.9993186\n",
      " 0.9965578  0.99998724 0.9989477  0.99998844 0.99991786 0.99868053\n",
      " 0.9960271  0.9996068  0.9985     0.99924314 0.9999392  0.9996252\n",
      " 0.99970657 0.9999938  0.9986902  0.99999046 0.98993886 0.99983335\n",
      " 0.9951196  0.99975437 0.9991103  0.99887687 0.97759944 0.9995011\n",
      " 0.98946244 0.99441093 0.99998367 0.9998847  0.9999988  0.9992329\n",
      " 0.9989631  0.99974746 0.99876034 0.99169415 0.9977215  0.9998697\n",
      " 0.9996581  0.9214583  0.999899   0.9987413  0.7774035  0.99937785\n",
      " 0.9911261  0.9979824  0.99306643 0.9998018  0.99995375 0.9999714\n",
      " 0.9998281  0.99753714 0.8398098  0.99999416 0.9999039  0.9989324\n",
      " 0.99280965 0.9998729  0.9999591  0.99468344 0.9998356  0.99994457\n",
      " 0.9999845  0.9995253  0.99949276]\n",
      "Epoch 16: 100%|██████████| 640/640 [03:38<00:00,  2.93it/s, v_num=0, train_loss_step=0.0822, val_loss=0.121, val_ood_acc=0.923, val_acc_all=0.996, val_precision_all=0.902, val_recall_all=0.724, val_f1_all=0.804, avg_threshold=0.361, val_acc=0.996, val_precision=0.902, val_recall=0.724, val_f1=0.804, train_loss_epoch=0.126, train_acc=0.993, train_precision=0.912, train_recall=0.464, train_f1=0.615]Min prob per class: [1.40796539e-08 2.16950946e-09 3.10606074e-09 1.02702258e-09\n",
      " 6.11490192e-10 3.16414915e-11 7.53328372e-11 2.71660667e-12\n",
      " 6.92310376e-11 1.29505254e-10 1.36581454e-10 1.37420519e-10\n",
      " 1.87332372e-11 6.12046747e-10 2.53292165e-10 7.37072643e-12\n",
      " 6.43883280e-10 1.74206971e-09 3.01626955e-08 5.41825216e-08\n",
      " 1.17069066e-09 1.64602880e-11 1.45083279e-10 2.19949059e-10\n",
      " 1.19704324e-09 7.96052202e-09 6.89644508e-10 2.15015894e-10\n",
      " 6.94696592e-11 1.21015323e-10 1.91789651e-09 3.01290437e-09\n",
      " 3.48702456e-10 1.33161104e-09 1.41712571e-08 1.01078002e-09\n",
      " 2.93373797e-10 4.01888772e-10 2.76637213e-10 1.15969412e-09\n",
      " 6.61485033e-10 1.05066103e-11 1.84803423e-11 4.94922503e-09\n",
      " 8.20855606e-11 2.38579934e-10 1.76321568e-10 8.26550772e-11\n",
      " 5.39936007e-09 5.20001375e-10 5.61619973e-10 1.16283050e-09\n",
      " 2.36778952e-09 7.06427361e-10 1.06013019e-07 7.51589724e-09\n",
      " 5.93770755e-10 6.28597174e-10 2.28634514e-10 7.76735176e-10\n",
      " 2.61993656e-11 7.61317814e-11 8.69563241e-11 7.26391114e-10\n",
      " 1.90259031e-09 1.67565406e-10 5.77475223e-09 5.35578637e-10\n",
      " 1.04919617e-09 1.36285978e-08 1.50159968e-10 1.42678303e-09\n",
      " 8.86714369e-10 6.75870679e-11 6.85185686e-09 1.44033745e-08\n",
      " 6.95031277e-09 2.90271862e-10 1.02654428e-06 7.78549225e-10\n",
      " 8.97146940e-12]\n",
      "Max prob per class: [0.99999475 0.9995252  0.99895775 0.9999988  0.9995895  0.99948347\n",
      " 0.9908522  0.99922884 0.9859963  0.99550784 0.9935821  0.9998209\n",
      " 0.99901426 0.9984681  0.99976915 0.99998546 0.99986506 0.9994443\n",
      " 0.996808   0.99994826 0.9952676  0.99977547 0.9999207  0.9752423\n",
      " 0.99832267 0.9961408  0.99558    0.9937815  0.99981874 0.9992249\n",
      " 0.9999248  0.9999957  0.99782985 0.9999771  0.9996433  0.9996443\n",
      " 0.99465585 0.9998361  0.99310714 0.99784124 0.95305955 0.99866104\n",
      " 0.99890864 0.9986065  0.99999523 0.99971086 0.9999975  0.9985031\n",
      " 0.9993381  0.9982748  0.9986576  0.99181104 0.9820582  0.9998609\n",
      " 0.99816436 0.98733157 0.99469537 0.99947506 0.956509   0.99818003\n",
      " 0.9986761  0.9961565  0.9997516  0.9995999  0.999938   0.999956\n",
      " 0.99993336 0.9990244  0.83207875 0.9999865  0.9999701  0.9998399\n",
      " 0.981751   0.99990904 0.9999815  0.99982774 0.9992175  0.9996592\n",
      " 0.99998045 0.9996086  0.9994338 ]\n",
      "Epoch 17: 100%|██████████| 640/640 [03:36<00:00,  2.95it/s, v_num=0, train_loss_step=0.167, val_loss=0.119, val_ood_acc=0.923, val_acc_all=0.996, val_precision_all=0.913, val_recall_all=0.719, val_f1_all=0.804, avg_threshold=0.355, val_acc=0.996, val_precision=0.913, val_recall=0.719, val_f1=0.804, train_loss_epoch=0.124, train_acc=0.993, train_precision=0.920, train_recall=0.463, train_f1=0.616] Min prob per class: [3.7076888e-09 4.6567217e-10 4.8456639e-09 4.0984212e-09 2.5044784e-09\n",
      " 4.2041315e-11 5.6275418e-11 1.2981093e-10 5.3689597e-10 4.5284509e-11\n",
      " 3.7108872e-09 1.6258941e-10 2.6167868e-10 1.0046046e-09 9.4935303e-11\n",
      " 1.9540886e-11 4.7083139e-11 5.7623056e-10 1.2329973e-07 3.3586895e-09\n",
      " 4.2997786e-10 1.6861658e-11 7.6656771e-11 1.0658121e-09 6.4008371e-10\n",
      " 6.0392851e-09 2.8925548e-10 1.5525513e-11 2.2076338e-12 8.6239205e-10\n",
      " 2.9569014e-09 2.0134916e-09 6.5993500e-10 2.2123992e-10 6.4716610e-09\n",
      " 2.4974092e-09 3.8953937e-11 8.6556090e-10 2.8063621e-11 3.8427699e-11\n",
      " 5.8303967e-10 5.4105501e-11 6.5598783e-11 7.8016988e-09 4.3458270e-10\n",
      " 2.5887481e-10 1.2473941e-10 1.3688242e-10 5.1904445e-09 2.7283450e-10\n",
      " 1.2146191e-09 8.7226761e-11 2.1727600e-09 1.7110459e-09 1.3602785e-08\n",
      " 3.3134555e-09 5.1506582e-10 3.5324816e-12 2.5003152e-10 1.5051878e-09\n",
      " 2.3558494e-11 3.6643147e-11 2.4876048e-10 1.4545131e-09 3.9013908e-09\n",
      " 1.6044406e-10 3.4573109e-09 1.5347958e-09 3.6823880e-09 4.9996270e-08\n",
      " 5.7651528e-10 9.9107100e-10 3.4818542e-10 9.6363931e-11 5.2069145e-09\n",
      " 9.3260013e-09 9.1506980e-09 2.7367153e-10 7.2108742e-07 6.9367574e-11\n",
      " 1.8588731e-11]\n",
      "Max prob per class: [0.9999833  0.9995005  0.99826485 0.99999964 0.99335444 0.9997353\n",
      " 0.98606724 0.9973031  0.9792514  0.99367553 0.97844154 0.9999224\n",
      " 0.9983088  0.99025035 0.99664307 0.99999774 0.99856865 0.9994438\n",
      " 0.9984511  0.99997807 0.99904066 0.9999658  0.99953306 0.99987316\n",
      " 0.99799937 0.99764353 0.9969752  0.99554235 0.9998443  0.99949634\n",
      " 0.99977785 0.9999552  0.99859875 0.9999558  0.999141   0.9998659\n",
      " 0.9910377  0.99986494 0.9976708  0.998992   0.9248574  0.99967563\n",
      " 0.99906415 0.9980287  0.99997354 0.99994564 0.9999962  0.99953866\n",
      " 0.99941254 0.99937963 0.9928681  0.9989957  0.9989613  0.9995858\n",
      " 0.999876   0.9752587  0.99869496 0.99901474 0.9746413  0.99932396\n",
      " 0.99428695 0.9925869  0.99952877 0.9998117  0.9999424  0.99998295\n",
      " 0.99987864 0.9991943  0.7704685  0.99999356 0.99964345 0.9994553\n",
      " 0.99627376 0.9994874  0.99993753 0.99984527 0.9998429  0.9998678\n",
      " 0.9999908  0.9997242  0.99990714]\n",
      "Epoch 18: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.0451, val_loss=0.117, val_ood_acc=0.925, val_acc_all=0.996, val_precision_all=0.911, val_recall_all=0.730, val_f1_all=0.810, avg_threshold=0.348, val_acc=0.996, val_precision=0.911, val_recall=0.730, val_f1=0.810, train_loss_epoch=0.126, train_acc=0.993, train_precision=0.912, train_recall=0.463, train_f1=0.614]Min prob per class: [4.88855179e-09 1.36579703e-09 1.36242773e-09 4.83842300e-09\n",
      " 4.91453933e-10 3.62811621e-11 1.89325621e-11 3.07120059e-11\n",
      " 1.33839634e-10 7.15213722e-11 2.92438601e-10 3.09123343e-12\n",
      " 2.18397730e-10 8.61622496e-10 4.12040485e-10 5.03252162e-11\n",
      " 2.87429580e-10 2.96784597e-09 5.82072452e-08 2.64191975e-08\n",
      " 2.54769483e-10 5.48042688e-12 3.57425270e-11 4.60085164e-11\n",
      " 1.42949053e-09 5.40974066e-09 2.05005013e-09 5.72565302e-12\n",
      " 1.30023534e-10 1.09348930e-09 6.50447918e-09 6.13621554e-09\n",
      " 4.65636384e-11 3.66226043e-11 4.29904029e-10 2.83638735e-09\n",
      " 6.58231636e-10 1.11910903e-09 1.63984368e-10 5.97990407e-11\n",
      " 7.59344046e-11 2.18266544e-10 4.43551529e-10 8.28734414e-10\n",
      " 2.41799931e-11 2.22249805e-11 1.40025908e-10 1.16701163e-10\n",
      " 7.65328156e-09 1.87128923e-10 6.48115311e-11 4.43133780e-10\n",
      " 4.85283147e-09 5.43755485e-10 3.11112025e-08 5.78951953e-09\n",
      " 2.20717800e-09 1.08309132e-10 1.24636912e-10 1.09060860e-09\n",
      " 6.64129446e-11 2.17931033e-12 5.68817840e-11 1.02166289e-10\n",
      " 6.20208507e-09 2.45104770e-10 1.45194985e-08 2.61884875e-10\n",
      " 4.34326394e-11 2.05972022e-08 1.00096625e-10 6.85577539e-10\n",
      " 4.18503210e-12 1.21496146e-10 9.89518867e-09 9.42364942e-09\n",
      " 1.97591188e-08 4.17459234e-10 1.00270992e-08 2.26097169e-10\n",
      " 4.23752526e-11]\n",
      "Max prob per class: [0.9999535  0.99950945 0.99444205 0.9999869  0.9996425  0.9992436\n",
      " 0.9678059  0.9993475  0.98808354 0.9896457  0.9969453  0.99954337\n",
      " 0.9976714  0.99893314 0.99972004 0.9998282  0.99540234 0.99832946\n",
      " 0.99704    0.9999788  0.99712616 0.99996424 0.9999095  0.9965437\n",
      " 0.99849    0.9989818  0.99775773 0.9993018  0.9999392  0.9983041\n",
      " 0.99992096 0.9999764  0.9993741  0.9999651  0.9990036  0.9999609\n",
      " 0.99484104 0.9996916  0.9985019  0.9992654  0.9442146  0.99988127\n",
      " 0.99701715 0.9827172  0.9999902  0.999969   0.9999976  0.9930073\n",
      " 0.9985537  0.9994411  0.9979693  0.9935954  0.9931676  0.99981207\n",
      " 0.99772376 0.9786936  0.9996861  0.9959572  0.9225808  0.9988477\n",
      " 0.9952554  0.9990907  0.9771153  0.99978584 0.999071   0.99995756\n",
      " 0.99954176 0.998519   0.876598   0.9999565  0.99996734 0.99978465\n",
      " 0.99776447 0.9998324  0.9999465  0.99945384 0.9998971  0.9999325\n",
      " 0.9999809  0.99895895 0.99965465]\n",
      "Epoch 19: 100%|██████████| 640/640 [03:34<00:00,  2.99it/s, v_num=0, train_loss_step=0.0581, val_loss=0.116, val_ood_acc=0.929, val_acc_all=0.996, val_precision_all=0.916, val_recall_all=0.725, val_f1_all=0.809, avg_threshold=0.345, val_acc=0.996, val_precision=0.916, val_recall=0.725, val_f1=0.809, train_loss_epoch=0.123, train_acc=0.993, train_precision=0.915, train_recall=0.469, train_f1=0.620]Min prob per class: [3.4513088e-09 1.5905281e-10 1.6267512e-09 1.3571575e-09 2.7019942e-11\n",
      " 5.0042557e-11 8.2141750e-12 2.6621834e-12 7.5581541e-11 1.8504328e-11\n",
      " 5.5059024e-10 2.2606190e-09 1.8072925e-10 5.5142052e-10 3.7824807e-10\n",
      " 4.0837000e-11 2.1279520e-10 2.4946241e-09 3.9052342e-08 5.0696396e-08\n",
      " 1.3076293e-10 5.0888086e-13 1.8844198e-10 4.9690453e-11 1.6705521e-09\n",
      " 9.5751727e-09 1.7704891e-09 2.2610049e-11 6.6875380e-11 5.5456895e-10\n",
      " 3.2801958e-10 2.0166662e-09 3.8182746e-10 1.9836699e-10 1.5876981e-09\n",
      " 4.7262687e-09 1.4427418e-10 5.1800175e-10 9.9183065e-11 1.0774766e-10\n",
      " 2.3709415e-11 2.3547184e-10 2.1111585e-10 3.3726584e-09 8.3893802e-11\n",
      " 1.3592993e-10 4.4435255e-10 2.9948991e-11 3.7571657e-09 1.8100040e-10\n",
      " 6.5693943e-11 1.9594726e-09 4.0592889e-09 6.4603484e-10 8.0368174e-09\n",
      " 5.7964117e-10 8.6188551e-10 1.2817213e-10 1.4173215e-10 4.0864161e-09\n",
      " 3.6145299e-11 1.5371211e-10 7.5323650e-11 1.8362577e-10 5.6831637e-09\n",
      " 7.2284019e-11 3.2708636e-09 2.5881064e-10 1.3397122e-09 3.5724366e-09\n",
      " 1.8945069e-10 2.0424033e-10 3.0617386e-10 1.3427176e-10 6.4244712e-09\n",
      " 2.4240279e-09 9.5290602e-09 9.9754938e-10 1.4910876e-07 1.9523396e-09\n",
      " 1.7362067e-11]\n",
      "Max prob per class: [0.9999788  0.99885833 0.9968388  0.99999774 0.9960186  0.99936503\n",
      " 0.995424   0.98782384 0.9826299  0.99727744 0.99660057 0.99997413\n",
      " 0.99545187 0.97594315 0.99839205 0.9999993  0.99931014 0.999428\n",
      " 0.9968996  0.9999778  0.9959871  0.9998776  0.9995296  0.99962056\n",
      " 0.9982942  0.9975108  0.99962485 0.9949923  0.9998281  0.99911743\n",
      " 0.99976414 0.99999607 0.99970406 0.9999876  0.9988262  0.9999895\n",
      " 0.9910424  0.99992955 0.99897826 0.9991642  0.87239486 0.9993117\n",
      " 0.99819416 0.9970509  0.99999166 0.99980205 0.99999356 0.9985831\n",
      " 0.9995433  0.99943024 0.9951651  0.9990489  0.9974968  0.9998166\n",
      " 0.9990688  0.92369986 0.99963975 0.99924326 0.94734687 0.9991666\n",
      " 0.9916762  0.9992009  0.99975353 0.9998172  0.9999678  0.9998307\n",
      " 0.9946955  0.9984226  0.6175771  0.99999726 0.9999174  0.99683064\n",
      " 0.99309677 0.99851805 0.99960846 0.99977523 0.9999633  0.9995472\n",
      " 0.9999862  0.999728   0.9993247 ]\n",
      "Epoch 19: 100%|██████████| 640/640 [04:23<00:00,  2.43it/s, v_num=0, train_loss_step=0.0581, val_loss=0.122, val_ood_acc=0.923, val_acc_all=0.996, val_precision_all=0.909, val_recall_all=0.726, val_f1_all=0.808, avg_threshold=0.343, val_acc=0.996, val_precision=0.909, val_recall=0.726, val_f1=0.808, train_loss_epoch=0.123, train_acc=0.993, train_precision=0.914, train_recall=0.466, train_f1=0.618]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 640/640 [04:23<00:00,  2.43it/s, v_num=0, train_loss_step=0.0581, val_loss=0.122, val_ood_acc=0.923, val_acc_all=0.996, val_precision_all=0.909, val_recall_all=0.726, val_f1_all=0.808, avg_threshold=0.343, val_acc=0.996, val_precision=0.909, val_recall=0.726, val_f1=0.808, train_loss_epoch=0.123, train_acc=0.993, train_precision=0.914, train_recall=0.466, train_f1=0.618]\n",
      "Fold 2/5\n",
      "Setup ran successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name              | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | feature_extractor | Sequential          | 65.3 M | train\n",
      "1  | ood_classifier    | Sequential          | 2.0 K  | train\n",
      "2  | classifier        | Sequential          | 165 K  | train\n",
      "3  | loss_fn_class     | BCEWithLogitsLoss   | 0      | train\n",
      "4  | loss_fn_ood       | BCEWithLogitsLoss   | 0      | train\n",
      "5  | train_acc         | MultilabelAccuracy  | 0      | train\n",
      "6  | val_acc           | MultilabelAccuracy  | 0      | train\n",
      "7  | train_precision   | MultilabelPrecision | 0      | train\n",
      "8  | val_precision     | MultilabelPrecision | 0      | train\n",
      "9  | train_recall      | MultilabelRecall    | 0      | train\n",
      "10 | val_recall        | MultilabelRecall    | 0      | train\n",
      "11 | train_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1            | MultilabelF1Score   | 0      | train\n",
      "13 | ood_acc           | BinaryAccuracy      | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "168 K     Trainable params\n",
      "65.3 M    Non-trainable params\n",
      "65.5 M    Total params\n",
      "261.938   Total estimated model params size (MB)\n",
      "958       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]Min prob per class: [0.4582875  0.45924583 0.4630952  0.46893653 0.45876983 0.4554715\n",
      " 0.48066428 0.471525   0.4789913  0.48958248 0.46715963 0.46499014\n",
      " 0.4583211  0.44632667 0.4776607  0.4390234  0.46833873 0.45139736\n",
      " 0.4504294  0.47951096 0.47889793 0.47996855 0.464392   0.48047537\n",
      " 0.49045414 0.45403612 0.45474303 0.45218807 0.47688025 0.4896872\n",
      " 0.46100208 0.45485023 0.45926252 0.44516617 0.4416159  0.4687895\n",
      " 0.47791147 0.4679831  0.44490734 0.453998   0.45722806 0.46830264\n",
      " 0.47734538 0.46802166 0.4536572  0.46522895 0.45824203 0.46949968\n",
      " 0.47778952 0.43734568 0.45109046 0.45366117 0.46568742 0.4380944\n",
      " 0.4619445  0.459053   0.4774769  0.47134775 0.46426898 0.46283263\n",
      " 0.43645316 0.4492729  0.46897173 0.46145168 0.44415024 0.46031234\n",
      " 0.4839479  0.45143506 0.46452475 0.4725798  0.48335233 0.47351483\n",
      " 0.4748572  0.4536743  0.44537166 0.46684507 0.4509167  0.47515753\n",
      " 0.4780965  0.4733011  0.47638813]\n",
      "Max prob per class: [0.51366967 0.52550656 0.52350986 0.53395474 0.5113109  0.51429826\n",
      " 0.52907705 0.5410742  0.55620676 0.5626271  0.5317239  0.52427495\n",
      " 0.5260046  0.5241882  0.54673034 0.5147672  0.52954346 0.5360931\n",
      " 0.54130065 0.5293322  0.5300887  0.5535627  0.54325765 0.5425589\n",
      " 0.55224013 0.53592485 0.51052886 0.5219852  0.5471655  0.5432259\n",
      " 0.5149     0.52236646 0.5076135  0.5274658  0.5257062  0.53650534\n",
      " 0.5190134  0.54008937 0.53112423 0.5442542  0.53318787 0.53800565\n",
      " 0.53124523 0.54292715 0.5131599  0.52717614 0.5460755  0.5220331\n",
      " 0.55451316 0.52669895 0.5557998  0.5091333  0.52446365 0.5438036\n",
      " 0.5338006  0.533775   0.54101694 0.5394747  0.5316361  0.51353884\n",
      " 0.5278973  0.5255033  0.54791224 0.517431   0.5099792  0.527921\n",
      " 0.539264   0.5088166  0.5243673  0.56171453 0.53322434 0.53533953\n",
      " 0.52255094 0.51737005 0.53350466 0.53990656 0.5431134  0.5524236\n",
      " 0.54238904 0.5472167  0.5553283 ]\n",
      "Warning: No positive samples for class 3, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 4, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 5, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 6, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 7, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 8, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 9, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 10, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 11, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 12, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 13, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 14, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 15, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 16, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 17, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 18, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 19, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 20, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 21, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 22, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 23, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 24, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 25, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 26, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 27, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 28, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 29, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 30, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 31, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 32, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 33, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 34, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 35, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 36, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 37, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 38, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 39, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 40, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 41, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 42, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 43, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 44, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 45, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 46, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 47, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 48, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 49, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 50, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 51, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 52, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 53, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 54, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 55, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 56, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 57, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 58, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 59, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 60, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 61, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 62, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 63, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 64, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 65, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 66, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 67, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 68, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 69, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 70, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 71, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 72, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 73, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 74, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 75, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 76, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 77, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 78, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 79, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 80, keeping threshold at 0.5\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.222] Min prob per class: [3.65011556e-06 4.56934089e-07 9.36801371e-07 2.43259888e-06\n",
      " 8.04381943e-06 1.92411804e-07 1.82664007e-05 2.07765297e-05\n",
      " 1.04966975e-05 4.94397636e-06 1.31273566e-06 9.13205076e-06\n",
      " 2.00152499e-06 8.00770067e-06 6.09916924e-06 2.77628374e-06\n",
      " 1.68594704e-06 2.33586297e-06 2.08570714e-06 9.84302801e-07\n",
      " 4.75516572e-06 5.01091063e-06 2.85675219e-06 2.05404308e-06\n",
      " 3.84727912e-07 1.60195066e-06 2.81273037e-06 2.22008293e-05\n",
      " 9.02245029e-06 6.00950352e-06 3.58438706e-06 1.13263593e-06\n",
      " 3.34813944e-06 1.74665747e-05 1.30110459e-06 1.10720271e-06\n",
      " 2.13746239e-06 2.98217219e-06 5.99485020e-06 4.79149639e-06\n",
      " 3.30347621e-06 5.29176214e-06 7.36465927e-06 1.23498512e-05\n",
      " 2.76782714e-07 2.11149063e-06 1.69131624e-07 4.99146097e-07\n",
      " 1.20900195e-05 3.68818951e-06 2.70177225e-06 9.43667203e-07\n",
      " 3.24430857e-06 3.68056135e-06 1.05122388e-06 1.46487014e-06\n",
      " 7.20573951e-07 1.38115388e-06 1.90457667e-05 4.43617728e-06\n",
      " 1.69519899e-05 2.31710757e-07 4.68052031e-06 9.17440559e-07\n",
      " 5.51929452e-06 5.74415026e-07 2.67784662e-06 1.33163792e-06\n",
      " 2.39623523e-06 2.49883146e-06 2.25919075e-06 1.42044144e-06\n",
      " 8.05581863e-07 1.04177866e-06 6.64989238e-06 6.34257560e-07\n",
      " 1.11733914e-06 4.42478131e-06 1.43282034e-03 4.02667183e-06\n",
      " 2.81517782e-06]\n",
      "Max prob per class: [0.4574371  0.45351496 0.45816964 0.4561168  0.46026382 0.45783827\n",
      " 0.4583199  0.45330802 0.46474415 0.46172222 0.45316097 0.4533711\n",
      " 0.4575556  0.45144698 0.45098403 0.45786327 0.45107648 0.4599711\n",
      " 0.45182878 0.452163   0.447576   0.46416783 0.46256474 0.4476928\n",
      " 0.4548156  0.4508946  0.45807508 0.45513886 0.45983368 0.45129544\n",
      " 0.46129242 0.451651   0.462277   0.46224368 0.45026094 0.45283112\n",
      " 0.45453873 0.45072487 0.45668253 0.45766962 0.46351668 0.45140767\n",
      " 0.4543164  0.46067682 0.44968203 0.46291998 0.45527187 0.45105407\n",
      " 0.4590434  0.4487738  0.4575298  0.45714644 0.45445532 0.4602538\n",
      " 0.45885414 0.45898584 0.45669872 0.45766672 0.4574479  0.4507341\n",
      " 0.45763326 0.4588589  0.45770016 0.45007426 0.45146865 0.45547485\n",
      " 0.45922887 0.4477121  0.45458692 0.4561664  0.4607336  0.45609716\n",
      " 0.46216637 0.44849244 0.44734204 0.4467548  0.4572818  0.46430132\n",
      " 0.99962413 0.45313522 0.45373893]\n",
      "Epoch 1: 100%|██████████| 640/640 [03:36<00:00,  2.96it/s, v_num=0, train_loss_step=0.228, val_loss=0.174, val_ood_acc=0.904, val_acc_all=0.993, val_precision_all=0.890, val_recall_all=0.461, val_f1_all=0.608, avg_threshold=0.372, val_acc=0.993, val_precision=0.890, val_recall=0.461, val_f1=0.608, train_loss_epoch=0.310, train_acc=0.986, train_precision=0.421, train_recall=0.443, train_f1=0.432] Min prob per class: [4.03287049e-06 2.45464980e-05 4.49313711e-06 4.84746852e-06\n",
      " 8.32804653e-06 3.23318744e-07 3.39472126e-06 1.50286121e-06\n",
      " 1.99535384e-06 1.16782644e-06 7.03286787e-05 6.61433319e-07\n",
      " 5.23135850e-06 7.21432571e-06 1.57030972e-05 2.24187801e-07\n",
      " 2.17560755e-05 5.43968872e-06 4.74682820e-05 8.97934171e-07\n",
      " 8.21989215e-06 1.14083139e-06 6.02511307e-07 9.27216115e-06\n",
      " 8.75043952e-07 1.27913645e-05 1.56306214e-06 5.16098908e-07\n",
      " 2.33354285e-06 4.45965998e-06 1.22146844e-06 5.41032841e-06\n",
      " 2.19505736e-07 1.17188858e-06 7.11234088e-06 5.52499478e-05\n",
      " 2.15471755e-06 4.41344355e-07 1.10267920e-05 3.55755475e-07\n",
      " 2.10266606e-07 1.26853786e-06 1.54693817e-06 4.05959599e-06\n",
      " 2.92004756e-07 7.30281135e-06 2.31099762e-06 2.42300416e-06\n",
      " 8.80975858e-06 2.37158201e-05 1.30972094e-05 5.80899041e-06\n",
      " 6.95280846e-07 1.24477469e-06 3.71717283e-06 4.05785067e-06\n",
      " 2.89184641e-06 3.12511929e-06 2.10566304e-06 1.86016723e-06\n",
      " 3.73809712e-06 7.68220389e-07 1.26349710e-06 6.00291605e-06\n",
      " 4.97775090e-06 9.77331820e-06 9.47566662e-07 4.33476953e-06\n",
      " 1.37929983e-05 9.31851973e-05 3.44984096e-06 6.72146143e-06\n",
      " 2.20292259e-06 3.48680351e-06 1.39688143e-06 5.68536234e-06\n",
      " 1.81590622e-05 1.99841925e-05 1.60263618e-04 8.88323143e-07\n",
      " 1.27432406e-06]\n",
      "Max prob per class: [0.94118726 0.9102095  0.43314767 0.9235833  0.73705906 0.8540253\n",
      " 0.88328177 0.48729986 0.8332181  0.6447579  0.9044499  0.87571746\n",
      " 0.845984   0.81086785 0.5908033  0.7320896  0.761031   0.724538\n",
      " 0.43438768 0.973989   0.82072586 0.8350902  0.96216524 0.4347659\n",
      " 0.42580408 0.7402526  0.8532547  0.6917933  0.530238   0.93284625\n",
      " 0.49322274 0.9664298  0.76431125 0.9623985  0.6061424  0.99467707\n",
      " 0.45100364 0.92510104 0.8344499  0.68566716 0.43239784 0.77740115\n",
      " 0.52391183 0.6935928  0.47617266 0.986613   0.93136585 0.89362264\n",
      " 0.97992593 0.5587796  0.9039343  0.79559207 0.9339105  0.76667714\n",
      " 0.8237725  0.43038648 0.6816272  0.91671324 0.43888044 0.7864141\n",
      " 0.783546   0.66475064 0.42577037 0.69992375 0.8270274  0.99201185\n",
      " 0.69278574 0.8351198  0.42623883 0.99077153 0.83491004 0.602309\n",
      " 0.45777032 0.9815425  0.95960325 0.63034296 0.7189528  0.6638573\n",
      " 0.9999821  0.8415872  0.88256127]\n",
      "Epoch 2: 100%|██████████| 640/640 [03:39<00:00,  2.91it/s, v_num=0, train_loss_step=0.152, val_loss=0.138, val_ood_acc=0.918, val_acc_all=0.994, val_precision_all=0.872, val_recall_all=0.565, val_f1_all=0.686, avg_threshold=0.320, val_acc=0.994, val_precision=0.872, val_recall=0.565, val_f1=0.686, train_loss_epoch=0.167, train_acc=0.993, train_precision=0.907, train_recall=0.437, train_f1=0.590] Min prob per class: [3.50465598e-06 1.01361707e-07 5.16667171e-08 7.21414835e-07\n",
      " 2.65214680e-07 1.32377579e-07 2.00708982e-07 5.38194513e-07\n",
      " 1.22874681e-07 5.93513079e-08 6.56935924e-07 3.14583076e-07\n",
      " 1.01721025e-06 6.45317414e-07 4.83959275e-06 9.37156415e-08\n",
      " 2.59940657e-06 4.96669281e-06 1.91951358e-06 6.53080133e-06\n",
      " 5.62252865e-07 5.25330336e-07 7.97922297e-08 4.51779414e-08\n",
      " 3.28539795e-06 5.60865828e-07 1.69527573e-06 8.30955685e-07\n",
      " 3.85532957e-08 3.24831539e-08 7.67057315e-08 1.88183753e-06\n",
      " 8.78220874e-07 4.15050465e-07 1.69967578e-07 4.52460836e-06\n",
      " 1.64294846e-07 1.09922205e-07 7.01276903e-08 3.53942866e-08\n",
      " 5.17581043e-07 1.40070568e-07 9.04573824e-07 8.56117524e-08\n",
      " 4.14223280e-07 4.87395596e-07 4.99539874e-06 3.27324045e-07\n",
      " 3.52383586e-06 5.24365305e-07 1.51968356e-06 1.54217432e-07\n",
      " 1.07909254e-05 2.02296292e-07 1.32531977e-06 3.30291209e-06\n",
      " 7.35164519e-07 1.79648637e-07 1.32143145e-06 1.12016119e-06\n",
      " 2.99197177e-06 2.12937525e-08 2.02323093e-07 4.06347363e-06\n",
      " 2.10512485e-06 1.40554832e-07 2.09072959e-06 3.23651619e-07\n",
      " 2.68597751e-06 2.11360912e-05 1.56612703e-06 1.65463757e-06\n",
      " 4.04749301e-08 3.46399645e-07 3.53236054e-08 3.23771565e-05\n",
      " 1.26261784e-06 6.57499442e-08 6.26955261e-06 2.72750026e-06\n",
      " 3.78918287e-07]\n",
      "Max prob per class: [0.9957183  0.94916564 0.93608135 0.997572   0.76282257 0.9913094\n",
      " 0.98248845 0.9445835  0.7522328  0.74122006 0.9853725  0.9886391\n",
      " 0.9231051  0.92812747 0.93293756 0.9982638  0.98762566 0.9571178\n",
      " 0.70961136 0.99962246 0.91009474 0.9867617  0.9922862  0.44358933\n",
      " 0.94212073 0.954201   0.9880044  0.95639354 0.8610435  0.9287406\n",
      " 0.9423455  0.99930596 0.9856553  0.9951534  0.8371233  0.99942553\n",
      " 0.8175442  0.9906978  0.925159   0.866925   0.6250867  0.9879556\n",
      " 0.9811069  0.75298804 0.8977735  0.99778736 0.99810684 0.98943466\n",
      " 0.99831045 0.82842183 0.9927061  0.946997   0.99201196 0.9750461\n",
      " 0.9220054  0.8798519  0.99011594 0.88634896 0.6316603  0.9825051\n",
      " 0.97456497 0.8300987  0.840006   0.9933258  0.9973418  0.99922514\n",
      " 0.9458751  0.9237361  0.58595926 0.97068024 0.9930976  0.98486984\n",
      " 0.39372647 0.99582136 0.9867296  0.9579686  0.88875866 0.87317145\n",
      " 0.99995923 0.99132544 0.9793108 ]\n",
      "Epoch 3: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.256, val_loss=0.128, val_ood_acc=0.922, val_acc_all=0.995, val_precision_all=0.907, val_recall_all=0.630, val_f1_all=0.744, avg_threshold=0.304, val_acc=0.995, val_precision=0.907, val_recall=0.630, val_f1=0.744, train_loss_epoch=0.154, train_acc=0.992, train_precision=0.910, train_recall=0.433, train_f1=0.587] Min prob per class: [4.6501546e-06 2.4464478e-07 5.9244542e-08 5.6473453e-07 1.3088328e-09\n",
      " 2.5196241e-09 5.4271055e-08 4.0089893e-07 1.2099690e-07 4.8099466e-08\n",
      " 3.5476884e-07 7.8428423e-07 1.8684170e-07 1.6870023e-07 1.2280778e-07\n",
      " 8.6804297e-09 7.0219905e-08 4.9672195e-07 2.9237620e-07 3.2478144e-07\n",
      " 1.1806762e-07 6.2336554e-09 2.0508997e-08 8.7731287e-08 4.2245386e-08\n",
      " 6.8620615e-08 1.7715022e-06 9.8557315e-08 7.4265301e-07 2.2752038e-06\n",
      " 7.4533162e-08 1.6227905e-06 7.6308957e-08 1.4738530e-08 1.6786873e-07\n",
      " 2.4031854e-06 9.1073808e-08 5.0917439e-08 3.2275647e-08 1.3174793e-08\n",
      " 2.2064541e-09 1.1418439e-08 2.6532751e-07 1.5625508e-07 3.4410302e-08\n",
      " 6.0345067e-09 1.0200108e-08 1.0255339e-07 5.3746874e-07 5.3933103e-08\n",
      " 6.7873444e-08 1.1799073e-07 5.2315551e-07 1.8598833e-08 5.7692944e-07\n",
      " 5.4024841e-08 5.3984149e-08 1.3713578e-08 6.0301723e-08 4.5491824e-08\n",
      " 1.4782816e-08 1.5304940e-07 2.8473764e-06 9.5752128e-08 1.6693417e-07\n",
      " 1.0113157e-06 2.9876017e-07 4.0032850e-07 3.9473771e-06 2.4694087e-05\n",
      " 2.8455412e-08 7.2441878e-08 1.2788020e-08 3.0816558e-08 8.6950712e-07\n",
      " 3.8741899e-07 1.0956497e-05 1.8127183e-06 6.1046335e-06 5.5686110e-07\n",
      " 8.0064964e-08]\n",
      "Max prob per class: [0.99960834 0.9936811  0.99379843 0.99605775 0.6008408  0.9869179\n",
      " 0.9847303  0.96676767 0.82154983 0.94837135 0.9604408  0.9990349\n",
      " 0.89123    0.9159491  0.9351381  0.9990939  0.9823559  0.94318753\n",
      " 0.94644606 0.99962413 0.9820348  0.991537   0.9975     0.76491785\n",
      " 0.8678698  0.9581766  0.9896081  0.97707    0.9982882  0.997673\n",
      " 0.99404514 0.9995695  0.9831835  0.99959654 0.97176546 0.999648\n",
      " 0.9724768  0.99172556 0.9748003  0.9465862  0.84717524 0.97330624\n",
      " 0.9438665  0.98771954 0.9550715  0.9941292  0.9986847  0.9871416\n",
      " 0.99985707 0.98935366 0.9702976  0.9903487  0.98665154 0.898434\n",
      " 0.9971499  0.9182269  0.96562564 0.9593744  0.71917474 0.9904032\n",
      " 0.9817346  0.99683905 0.96336967 0.9704972  0.998727   0.99986136\n",
      " 0.93721586 0.9797212  0.8775796  0.99954283 0.9704832  0.9617295\n",
      " 0.37013075 0.9983773  0.9999006  0.9668018  0.9820763  0.9664952\n",
      " 0.999995   0.99404585 0.9956672 ]\n",
      "Epoch 4: 100%|██████████| 640/640 [03:32<00:00,  3.00it/s, v_num=0, train_loss_step=0.122, val_loss=0.122, val_ood_acc=0.921, val_acc_all=0.995, val_precision_all=0.912, val_recall_all=0.648, val_f1_all=0.757, avg_threshold=0.296, val_acc=0.995, val_precision=0.912, val_recall=0.648, val_f1=0.757, train_loss_epoch=0.150, train_acc=0.993, train_precision=0.912, train_recall=0.442, train_f1=0.595] Min prob per class: [1.87321930e-05 1.16004365e-07 1.48298369e-08 1.94075923e-07\n",
      " 2.56672426e-08 8.12961087e-10 1.47683221e-09 5.91632379e-08\n",
      " 5.53931500e-07 2.50638656e-08 1.40331107e-08 3.63846837e-08\n",
      " 5.64627634e-09 3.61157277e-07 1.37889899e-07 1.18191146e-08\n",
      " 1.43891015e-07 2.30360708e-08 2.20645955e-08 9.43999021e-06\n",
      " 4.11582768e-09 1.91211366e-10 5.38017464e-10 3.91916544e-08\n",
      " 2.94367752e-09 7.83243124e-08 1.68471274e-07 6.09526074e-09\n",
      " 4.11772945e-08 1.35939572e-07 2.71706817e-08 3.51158405e-07\n",
      " 4.72702837e-08 3.17315916e-08 1.65043460e-07 7.37592563e-06\n",
      " 9.94813742e-09 1.37533931e-08 1.76806614e-09 1.65183764e-07\n",
      " 2.53258831e-10 1.06815961e-07 2.03264694e-08 5.28076384e-07\n",
      " 1.33675904e-09 1.05210445e-08 5.32678479e-09 1.43038781e-09\n",
      " 3.70667301e-08 5.24285326e-08 3.73756173e-08 6.19181373e-09\n",
      " 2.43897284e-06 7.16138061e-07 1.54168916e-07 5.44561431e-07\n",
      " 1.09523901e-08 9.26304988e-09 2.37800570e-08 2.69901079e-08\n",
      " 1.84969107e-08 1.47412003e-08 8.81556250e-09 9.73550414e-08\n",
      " 5.76987844e-08 3.57649483e-08 8.76690308e-07 1.03179275e-07\n",
      " 4.74631490e-09 1.56907822e-06 4.65794301e-08 2.89251716e-07\n",
      " 2.95648829e-06 5.67205261e-09 1.20590371e-07 3.66895779e-06\n",
      " 2.60893330e-07 3.41121869e-08 9.87811563e-06 2.56745949e-07\n",
      " 1.80215523e-08]\n",
      "Max prob per class: [0.99986815 0.9951617  0.98881197 0.9997459  0.9523995  0.98574555\n",
      " 0.9470734  0.99626523 0.9329609  0.9669611  0.974324   0.99973065\n",
      " 0.94293976 0.98316425 0.9237597  0.9997366  0.99434537 0.97091615\n",
      " 0.9414051  0.9999498  0.95742464 0.993008   0.99991333 0.7125305\n",
      " 0.75146616 0.9357778  0.9974195  0.8729479  0.99924767 0.99381965\n",
      " 0.9972052  0.9997278  0.99596184 0.9997814  0.99286157 0.99998915\n",
      " 0.8770714  0.9929577  0.98075175 0.99609023 0.84343374 0.9991228\n",
      " 0.9665903  0.97655857 0.97782874 0.9949686  0.999313   0.87292695\n",
      " 0.99909747 0.99922264 0.9884347  0.96034944 0.9992532  0.9919017\n",
      " 0.99903214 0.99297124 0.9860363  0.89702576 0.8628567  0.9540026\n",
      " 0.9826032  0.98099786 0.95920277 0.9960609  0.99825054 0.99995935\n",
      " 0.9434938  0.99064386 0.585412   0.9997813  0.9969693  0.9992661\n",
      " 0.9860097  0.9996642  0.9997738  0.99766445 0.9955155  0.9850841\n",
      " 0.9999933  0.99741936 0.9938367 ]\n",
      "Epoch 5: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.0792, val_loss=0.123, val_ood_acc=0.922, val_acc_all=0.995, val_precision_all=0.917, val_recall_all=0.685, val_f1_all=0.784, avg_threshold=0.317, val_acc=0.995, val_precision=0.917, val_recall=0.685, val_f1=0.784, train_loss_epoch=0.149, train_acc=0.993, train_precision=0.895, train_recall=0.447, train_f1=0.597]Min prob per class: [1.14149593e-07 3.63239430e-08 5.26910560e-09 8.68081074e-09\n",
      " 1.71827030e-09 2.38343989e-09 4.17797769e-10 9.85914905e-09\n",
      " 1.10753655e-08 1.33754463e-09 1.44896202e-08 2.15025953e-08\n",
      " 8.01362365e-10 1.95722869e-08 3.15109645e-08 3.46542323e-10\n",
      " 2.84158974e-10 1.30930280e-08 2.13545633e-07 6.11149105e-08\n",
      " 3.35282841e-08 9.70633018e-10 2.17076232e-10 1.54205237e-09\n",
      " 2.06939941e-08 8.52397690e-08 3.11269872e-08 5.41759135e-08\n",
      " 1.90223126e-09 2.47908645e-08 1.85808542e-08 1.93612493e-07\n",
      " 6.58774590e-09 7.65681074e-10 2.63057096e-08 8.03668243e-08\n",
      " 1.99315338e-08 1.66967618e-09 7.14020443e-09 2.57285970e-09\n",
      " 9.20020338e-10 6.14463929e-08 1.30228106e-09 1.26097845e-07\n",
      " 1.88610705e-09 5.52597923e-09 1.45117107e-09 3.39410083e-10\n",
      " 2.54053816e-07 3.49720863e-10 3.02697778e-08 6.26352517e-07\n",
      " 7.67925940e-07 1.26147830e-08 1.80862205e-08 6.13469453e-09\n",
      " 1.55287641e-08 1.28599498e-09 1.52448948e-07 9.36287758e-09\n",
      " 5.32748290e-10 1.22279897e-09 2.28537855e-09 1.85770990e-08\n",
      " 1.24013104e-06 1.61189266e-08 2.84312147e-08 8.10790457e-09\n",
      " 2.04585593e-08 4.83852673e-08 1.82745719e-09 1.18331451e-08\n",
      " 2.69325131e-08 5.66069369e-09 4.42853718e-08 1.02386871e-07\n",
      " 5.55911335e-08 4.07365555e-08 3.46412179e-07 1.96823002e-09\n",
      " 9.15448217e-10]\n",
      "Max prob per class: [0.99972945 0.9769488  0.98235613 0.9996018  0.78293806 0.99925286\n",
      " 0.996815   0.99671304 0.962236   0.9682543  0.91131705 0.9999281\n",
      " 0.9865465  0.98814    0.98593396 0.98346967 0.9825068  0.96847445\n",
      " 0.9734547  0.9999579  0.9906426  0.99923015 0.99948573 0.98424464\n",
      " 0.9753262  0.9845003  0.99837947 0.9852182  0.99957615 0.99866474\n",
      " 0.9980142  0.9999428  0.9933136  0.9998858  0.9912612  0.9998851\n",
      " 0.9804083  0.9994916  0.9823856  0.9974504  0.9346255  0.9992766\n",
      " 0.98911744 0.9873461  0.9990577  0.9995796  0.9990723  0.9986363\n",
      " 0.99983907 0.9448581  0.9927804  0.9983014  0.9964725  0.9776848\n",
      " 0.99933857 0.98507047 0.99624    0.99399865 0.98207575 0.998722\n",
      " 0.99718577 0.9941374  0.9749638  0.99850595 0.99948585 0.9997718\n",
      " 0.9799731  0.9980044  0.92501676 0.9966826  0.9997094  0.99952686\n",
      " 0.7760573  0.9998635  0.99977773 0.99843496 0.9609046  0.997191\n",
      " 0.9999722  0.9967083  0.99836487]\n",
      "Epoch 6: 100%|██████████| 640/640 [03:37<00:00,  2.94it/s, v_num=0, train_loss_step=0.119, val_loss=0.118, val_ood_acc=0.920, val_acc_all=0.995, val_precision_all=0.919, val_recall_all=0.676, val_f1_all=0.779, avg_threshold=0.317, val_acc=0.995, val_precision=0.919, val_recall=0.676, val_f1=0.779, train_loss_epoch=0.147, train_acc=0.993, train_precision=0.892, train_recall=0.449, train_f1=0.597] Min prob per class: [2.69556892e-08 1.53661563e-08 1.90804150e-09 7.90043642e-10\n",
      " 4.29030811e-10 7.99463523e-13 1.44111789e-08 2.77066391e-11\n",
      " 3.32740271e-08 8.99889399e-11 2.26112480e-08 2.21468865e-09\n",
      " 7.28509475e-11 7.60875807e-09 6.54679422e-10 5.71987110e-11\n",
      " 1.94448610e-10 4.99647834e-09 2.52174548e-08 8.18675545e-08\n",
      " 2.91899447e-08 8.53605242e-11 5.17677790e-10 1.54575397e-09\n",
      " 3.44293882e-09 1.43703369e-08 6.39734932e-09 6.76921497e-09\n",
      " 1.15717314e-09 2.77642904e-08 1.29895450e-09 3.43545651e-07\n",
      " 4.16024493e-10 1.78860302e-10 9.71151959e-09 3.46440849e-08\n",
      " 1.79288029e-09 1.06201794e-08 7.05629721e-11 6.13517126e-10\n",
      " 2.22670526e-09 3.48859652e-09 1.30745113e-08 2.98662677e-08\n",
      " 8.10737422e-10 4.43754589e-10 1.52762674e-08 3.11174730e-09\n",
      " 7.86453835e-09 9.39304856e-10 4.29358504e-09 6.26382102e-09\n",
      " 6.24636654e-09 1.82883098e-09 1.06291066e-06 2.21232432e-09\n",
      " 1.98184047e-09 2.12726198e-10 2.18881580e-09 6.05178769e-08\n",
      " 5.50329893e-10 1.20925159e-09 2.85080493e-08 2.15052795e-10\n",
      " 2.80380988e-07 1.35231595e-08 8.92056207e-10 4.78786744e-09\n",
      " 2.96026084e-08 1.22649260e-08 4.35852110e-09 7.17338855e-09\n",
      " 1.63751679e-08 7.16615045e-10 1.07235365e-09 7.25790770e-08\n",
      " 4.78299533e-08 2.84741675e-10 9.13063275e-07 3.28895577e-09\n",
      " 3.37449868e-09]\n",
      "Max prob per class: [0.99688125 0.9990351  0.9912744  0.99976534 0.92660207 0.9816202\n",
      " 0.9987702  0.9486172  0.9375381  0.9914418  0.9924233  0.99995255\n",
      " 0.9719828  0.9924946  0.9923619  0.99989545 0.9987739  0.99094254\n",
      " 0.8097404  0.9999063  0.99992776 0.99445724 0.9998603  0.9593992\n",
      " 0.9924833  0.9378382  0.99851197 0.99255204 0.9997633  0.998425\n",
      " 0.9989371  0.99990714 0.995144   0.9997881  0.99488103 0.99998105\n",
      " 0.9908977  0.99952936 0.9982963  0.9977252  0.9913743  0.9998554\n",
      " 0.9966185  0.98748815 0.9896689  0.9991799  0.99975437 0.9955363\n",
      " 0.99949443 0.99629587 0.9758473  0.9979748  0.9979697  0.9997818\n",
      " 0.99997985 0.9980849  0.99308276 0.9799121  0.98554254 0.9992662\n",
      " 0.99877995 0.99335176 0.9919898  0.9975153  0.99996436 0.9999447\n",
      " 0.9816913  0.9978714  0.8864604  0.999311   0.99798584 0.9992938\n",
      " 0.812921   0.9997085  0.9999813  0.9982924  0.9878588  0.9812367\n",
      " 0.9999893  0.99296254 0.99995446]\n",
      "Epoch 7: 100%|██████████| 640/640 [03:33<00:00,  3.00it/s, v_num=0, train_loss_step=0.252, val_loss=0.122, val_ood_acc=0.926, val_acc_all=0.996, val_precision_all=0.911, val_recall_all=0.706, val_f1_all=0.796, avg_threshold=0.328, val_acc=0.996, val_precision=0.911, val_recall=0.706, val_f1=0.796, train_loss_epoch=0.142, train_acc=0.993, train_precision=0.895, train_recall=0.453, train_f1=0.602] Min prob per class: [9.3215644e-11 1.2994879e-08 6.2175001e-11 3.9376353e-08 1.4066889e-10\n",
      " 1.1726277e-10 4.3748186e-10 8.4161927e-11 6.8182486e-09 2.8317912e-10\n",
      " 8.5556371e-09 4.6972790e-08 2.4790792e-10 3.4606639e-08 2.2284514e-09\n",
      " 1.6905560e-10 3.9864473e-11 2.1938398e-09 3.0161875e-09 9.4845261e-09\n",
      " 5.9716782e-10 1.9995368e-10 1.1866182e-09 5.2327923e-08 2.1312270e-09\n",
      " 7.0727841e-09 4.4058126e-09 5.5060574e-11 4.8170735e-11 3.5107848e-09\n",
      " 2.1205668e-09 2.6454267e-10 3.4220199e-10 1.2947142e-10 1.2074101e-09\n",
      " 1.5076860e-07 8.5706313e-09 2.9123962e-10 7.8285893e-09 9.5834818e-10\n",
      " 3.0000561e-11 1.2188818e-10 1.1952996e-10 3.0031134e-11 3.8995269e-11\n",
      " 4.8131659e-09 1.5658881e-09 2.9700627e-11 2.9740247e-07 7.4655414e-11\n",
      " 3.8887737e-10 1.6218995e-09 1.3905274e-08 1.9073950e-08 4.5818158e-10\n",
      " 3.0137204e-09 1.0117778e-08 3.0650094e-11 1.3303987e-11 8.1193835e-10\n",
      " 1.2512877e-10 1.7311581e-10 4.3500914e-09 2.7205685e-08 9.7811750e-09\n",
      " 2.2939959e-08 5.7806893e-10 8.6090135e-10 4.6660951e-09 9.9872588e-10\n",
      " 1.3773502e-08 2.6765056e-08 3.0189284e-08 1.3239603e-09 6.6667982e-09\n",
      " 6.4678012e-08 2.6951115e-08 1.4516314e-08 1.8126703e-07 2.3689400e-10\n",
      " 9.1421626e-10]\n",
      "Max prob per class: [0.9996044  0.9971431  0.9867208  0.9999558  0.9852469  0.99972314\n",
      " 0.99972194 0.992566   0.9716277  0.97027844 0.97270536 0.99993527\n",
      " 0.99518675 0.9997205  0.9982212  0.9999745  0.99894005 0.9982798\n",
      " 0.9843216  0.99900216 0.9979012  0.9980715  0.9999827  0.99995005\n",
      " 0.9906866  0.9916543  0.99965453 0.92763716 0.9981421  0.994379\n",
      " 0.99991643 0.999308   0.9879269  0.9999244  0.9950671  0.9999926\n",
      " 0.99928975 0.9996549  0.9995741  0.9981073  0.955919   0.9995716\n",
      " 0.9433066  0.99627376 0.99675435 0.9999565  0.9995826  0.99902046\n",
      " 0.99991477 0.99619836 0.9967146  0.999691   0.9993494  0.9976514\n",
      " 0.99494135 0.9942984  0.9970421  0.9933984  0.87037086 0.99899\n",
      " 0.9976749  0.9988242  0.99825865 0.99991333 0.9984577  0.99997354\n",
      " 0.93481684 0.9923383  0.77680624 0.99934417 0.99994576 0.99993074\n",
      " 0.9432519  0.9997632  0.999998   0.99759537 0.99971884 0.99250823\n",
      " 0.99999034 0.9988231  0.99978226]\n",
      "Epoch 8: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.110, val_loss=0.123, val_ood_acc=0.917, val_acc_all=0.995, val_precision_all=0.913, val_recall_all=0.685, val_f1_all=0.782, avg_threshold=0.340, val_acc=0.995, val_precision=0.913, val_recall=0.685, val_f1=0.782, train_loss_epoch=0.140, train_acc=0.993, train_precision=0.894, train_recall=0.455, train_f1=0.603] Min prob per class: [1.34703599e-08 9.23943588e-10 6.20164764e-09 2.44966092e-09\n",
      " 1.79966742e-09 6.27381957e-12 2.46732967e-09 1.23941593e-10\n",
      " 3.19864718e-10 4.22260921e-10 6.51960796e-09 8.75938078e-09\n",
      " 6.47213949e-10 2.39547298e-08 1.18824672e-09 1.56286685e-11\n",
      " 8.81239803e-10 2.85123996e-08 1.23172555e-07 2.23912959e-08\n",
      " 2.86472721e-11 5.31814259e-10 3.37418815e-10 8.06591044e-11\n",
      " 2.48975063e-10 1.36396503e-08 4.08621137e-10 1.55512236e-09\n",
      " 1.01094536e-10 2.29398296e-08 1.77419424e-09 2.29029293e-08\n",
      " 2.55233124e-09 1.10617591e-10 1.06819185e-08 1.18199317e-07\n",
      " 6.50022569e-09 1.48249091e-09 6.03503303e-10 7.94057653e-10\n",
      " 3.97407440e-10 3.68912256e-09 1.55172686e-09 1.10579890e-09\n",
      " 7.87506345e-12 3.61474545e-10 4.10277146e-08 3.06998560e-09\n",
      " 1.13933707e-09 4.55234427e-11 1.58758229e-10 6.99010988e-11\n",
      " 1.24617303e-08 1.29590083e-09 4.45592535e-10 3.66872577e-08\n",
      " 3.52984708e-09 8.88010360e-11 1.87529903e-09 1.18351251e-09\n",
      " 2.20259741e-10 1.84608703e-10 5.08636422e-09 1.62647817e-09\n",
      " 4.89235485e-10 1.43317136e-08 2.05605710e-09 1.09449294e-09\n",
      " 2.32585862e-08 2.75259584e-08 3.66495251e-10 7.24160110e-09\n",
      " 3.04705328e-09 2.39908260e-10 1.17736154e-09 1.66278596e-07\n",
      " 6.66597177e-09 1.89189091e-08 1.25617685e-08 1.79343773e-09\n",
      " 6.26164720e-09]\n",
      "Max prob per class: [0.9999248  0.9995747  0.9944042  0.9996972  0.9945141  0.9986349\n",
      " 0.9996107  0.9992124  0.94104207 0.9935348  0.98471695 0.9998821\n",
      " 0.9847104  0.99983025 0.9884454  0.99915695 0.99929166 0.9996929\n",
      " 0.99342406 0.9997366  0.99579006 0.9994622  0.9999621  0.86826056\n",
      " 0.94998735 0.9644118  0.98427594 0.9805177  0.99653506 0.99961424\n",
      " 0.99925107 0.999845   0.9936052  0.99989474 0.99864966 0.9999974\n",
      " 0.9823406  0.9989674  0.99835736 0.9996172  0.98773754 0.99955636\n",
      " 0.9991554  0.9031323  0.99115    0.9997657  0.99998856 0.99796945\n",
      " 0.99994576 0.9989496  0.9928115  0.99809045 0.9981629  0.9389133\n",
      " 0.9997336  0.99944514 0.9965565  0.99079764 0.97056186 0.99808973\n",
      " 0.9965487  0.99728274 0.99981934 0.9994679  0.99557173 0.99999607\n",
      " 0.99566084 0.99801207 0.9306994  0.9999236  0.99982053 0.9999325\n",
      " 0.9219827  0.99991107 0.9999962  0.9985765  0.9999379  0.9950187\n",
      " 0.9999759  0.99914896 0.9999666 ]\n",
      "Epoch 9: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.184, val_loss=0.117, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.907, val_recall_all=0.716, val_f1_all=0.800, avg_threshold=0.358, val_acc=0.996, val_precision=0.907, val_recall=0.716, val_f1=0.800, train_loss_epoch=0.139, train_acc=0.993, train_precision=0.897, train_recall=0.460, train_f1=0.608] Min prob per class: [1.4702345e-11 2.2482170e-10 7.8325249e-11 1.3305074e-08 5.3685195e-10\n",
      " 3.7199042e-11 2.9231072e-11 1.3336565e-10 1.5628991e-08 6.5054281e-11\n",
      " 4.7401871e-10 1.2472734e-09 1.0697184e-09 7.1306228e-09 1.3437943e-09\n",
      " 5.0104542e-11 1.4267604e-10 2.9630836e-09 3.0300910e-08 1.3611898e-07\n",
      " 1.0659722e-10 6.0983246e-11 2.5595986e-10 1.2975628e-09 3.2908192e-09\n",
      " 5.0688709e-10 1.3221811e-09 7.6014826e-11 4.2748782e-10 2.1431052e-09\n",
      " 2.3277746e-09 6.5025249e-11 1.2642459e-08 1.4944994e-10 2.9910774e-09\n",
      " 2.2859295e-07 1.0771792e-09 7.6080431e-10 5.0689068e-11 1.4069741e-09\n",
      " 7.9582625e-12 2.9952707e-11 1.4616235e-10 2.3641387e-09 1.0064271e-08\n",
      " 1.6116614e-09 8.7132690e-10 2.0422943e-10 6.0550842e-10 2.2621676e-09\n",
      " 4.8286114e-10 1.3615099e-10 1.3921302e-08 2.1533366e-09 6.9158390e-09\n",
      " 7.0886352e-10 6.4443899e-09 3.5634704e-11 9.0394194e-12 1.4673448e-09\n",
      " 3.4209479e-11 1.4620185e-11 3.6383584e-11 5.9167109e-11 9.8925979e-10\n",
      " 2.9850022e-09 1.6561930e-09 1.7351854e-09 3.1388595e-08 1.6623168e-09\n",
      " 4.7450449e-10 1.1880467e-10 7.9741030e-10 2.9974082e-11 8.0655491e-09\n",
      " 2.5978174e-08 3.0195763e-10 3.8062914e-10 3.7570712e-07 6.2963512e-10\n",
      " 6.3293065e-10]\n",
      "Max prob per class: [0.99966097 0.999278   0.9950389  0.99991703 0.9906131  0.99975723\n",
      " 0.99838495 0.9990416  0.9939067  0.9837307  0.9038546  0.99978334\n",
      " 0.99914706 0.9996984  0.99860424 0.99996626 0.9999751  0.99925905\n",
      " 0.9943094  0.9999832  0.99671197 0.999749   0.99994385 0.9959176\n",
      " 0.99772495 0.97797525 0.9972452  0.9937215  0.99981827 0.9986883\n",
      " 0.99998975 0.9999224  0.99708515 0.9999969  0.99922276 0.99999404\n",
      " 0.99758697 0.9991398  0.99756205 0.9945444  0.9503612  0.99943477\n",
      " 0.9947649  0.99396026 0.9993636  0.99989843 0.99997747 0.99943215\n",
      " 0.9900378  0.9996464  0.9922356  0.9989458  0.99534506 0.9996711\n",
      " 0.9998802  0.99479747 0.9997009  0.9981212  0.9952048  0.9991812\n",
      " 0.9932326  0.9988133  0.9850203  0.9995521  0.99964845 0.99999523\n",
      " 0.96872944 0.99345195 0.93962824 0.99998796 0.9999149  0.9999503\n",
      " 0.94013053 0.9998437  0.99999785 0.99864656 0.9998847  0.999273\n",
      " 0.9999174  0.99993396 0.99988735]\n",
      "Epoch 10: 100%|██████████| 640/640 [03:40<00:00,  2.91it/s, v_num=0, train_loss_step=0.289, val_loss=0.126, val_ood_acc=0.914, val_acc_all=0.996, val_precision_all=0.920, val_recall_all=0.696, val_f1_all=0.793, avg_threshold=0.367, val_acc=0.996, val_precision=0.920, val_recall=0.696, val_f1=0.793, train_loss_epoch=0.139, train_acc=0.993, train_precision=0.892, train_recall=0.461, train_f1=0.608] Min prob per class: [1.59632174e-09 2.25672903e-09 2.25420266e-10 7.31142524e-09\n",
      " 5.89638408e-12 1.08347515e-11 5.94038915e-11 3.55578956e-12\n",
      " 4.13750201e-10 2.70607373e-11 6.50481446e-09 7.89539045e-10\n",
      " 2.77803269e-09 2.58218447e-10 2.53293480e-11 4.63549477e-11\n",
      " 1.44443249e-10 1.70722525e-09 9.02727280e-08 2.90088966e-08\n",
      " 7.58594576e-10 4.72278570e-11 1.03826800e-11 3.29345336e-11\n",
      " 9.82930848e-10 1.90215133e-09 1.71628212e-09 8.03710487e-10\n",
      " 2.09237752e-10 1.10039949e-10 1.07867857e-08 1.17305605e-08\n",
      " 5.64524781e-11 3.94008293e-09 5.24024824e-10 4.60212890e-09\n",
      " 1.57881086e-09 6.75581369e-09 1.58646526e-10 1.33685785e-10\n",
      " 3.29156754e-12 1.89451427e-10 1.01770509e-10 6.94521418e-08\n",
      " 1.22233369e-11 3.50450571e-11 3.49248297e-10 2.27705055e-09\n",
      " 4.90514873e-09 3.28118262e-11 1.91632696e-10 2.03280104e-09\n",
      " 2.61482374e-08 1.67795122e-09 3.77476610e-08 1.19284504e-09\n",
      " 7.21433269e-09 3.75964919e-11 7.49732013e-11 1.97503303e-09\n",
      " 7.83410489e-12 1.75792353e-10 5.03599051e-10 2.02136710e-10\n",
      " 1.48737644e-09 1.91350005e-10 6.57101795e-09 6.83356871e-10\n",
      " 1.27871114e-09 7.78610421e-09 2.11152607e-09 6.92987490e-09\n",
      " 1.29878353e-09 2.86936058e-10 2.65637318e-10 8.68211814e-09\n",
      " 7.64046160e-09 4.26308017e-10 3.30843733e-07 7.64936670e-10\n",
      " 4.23218821e-10]\n",
      "Max prob per class: [0.999762   0.9994362  0.98501736 0.9999263  0.987815   0.9983205\n",
      " 0.99968314 0.999326   0.9570908  0.99548894 0.98020864 0.99994266\n",
      " 0.9957027  0.9676292  0.99932575 0.99998546 0.9959364  0.99958426\n",
      " 0.9986823  0.99997306 0.9985843  0.99971825 0.9999517  0.99839514\n",
      " 0.97063744 0.9850366  0.9964575  0.9928277  0.9999584  0.9982262\n",
      " 0.9999832  0.99996495 0.9933981  0.99999785 0.99972755 0.99999344\n",
      " 0.9795816  0.99966097 0.99834156 0.99853873 0.9693303  0.9999596\n",
      " 0.97581756 0.9977034  0.995795   0.99994075 0.99994993 0.999276\n",
      " 0.9998883  0.99954396 0.9970284  0.9997758  0.9889259  0.99295866\n",
      " 0.999998   0.9973552  0.99865365 0.99948525 0.95974654 0.9989355\n",
      " 0.9905455  0.9996464  0.9973793  0.9994677  0.9993468  0.99999213\n",
      " 0.9805767  0.9968659  0.83671707 0.99997294 0.9994313  0.99992526\n",
      " 0.99208367 0.9999558  0.999966   0.9992982  0.9795065  0.9980075\n",
      " 0.9999802  0.99993503 0.9992582 ]\n",
      "Epoch 11: 100%|██████████| 640/640 [03:31<00:00,  3.02it/s, v_num=0, train_loss_step=0.104, val_loss=0.115, val_ood_acc=0.921, val_acc_all=0.996, val_precision_all=0.912, val_recall_all=0.716, val_f1_all=0.802, avg_threshold=0.364, val_acc=0.996, val_precision=0.912, val_recall=0.716, val_f1=0.802, train_loss_epoch=0.138, train_acc=0.993, train_precision=0.895, train_recall=0.463, train_f1=0.610] Min prob per class: [3.4632075e-11 2.8934088e-11 4.4641088e-10 3.4975374e-09 3.8592896e-11\n",
      " 9.0901077e-13 1.4972919e-11 4.6889805e-12 1.3880622e-09 1.2475779e-09\n",
      " 2.0520599e-10 7.5747691e-10 2.3719016e-10 1.2906952e-10 1.3607578e-09\n",
      " 5.2526213e-12 1.4726348e-10 1.2265692e-09 4.4627296e-10 4.4690882e-08\n",
      " 2.8221975e-11 2.4855784e-11 4.7750740e-12 4.3680375e-11 1.6062510e-09\n",
      " 1.7966038e-10 2.8713822e-09 2.4156610e-12 1.9543719e-11 4.7530707e-10\n",
      " 2.9291014e-11 4.2229914e-08 7.5926147e-11 9.9899984e-11 5.3120397e-10\n",
      " 1.1409992e-08 1.0170977e-10 2.1511720e-10 4.1760259e-10 4.1627075e-09\n",
      " 2.1147807e-11 1.7001729e-10 1.5661472e-10 4.8964910e-10 5.9801981e-12\n",
      " 3.0899516e-10 2.5426982e-11 2.5936843e-11 6.6734357e-10 3.8555811e-11\n",
      " 1.4934658e-09 4.1636370e-09 2.3912502e-08 2.6325561e-10 9.3028172e-09\n",
      " 1.1236406e-10 1.9552464e-09 1.6652948e-11 2.5955563e-10 3.2069306e-10\n",
      " 4.1454645e-10 1.5160659e-11 1.9138604e-11 6.2264033e-10 3.4060743e-09\n",
      " 1.4132158e-10 9.1208321e-09 2.6951563e-09 1.4787095e-09 1.0657152e-08\n",
      " 8.8234427e-11 4.3469625e-10 5.1383564e-09 3.7794792e-11 3.8910813e-10\n",
      " 6.9255011e-09 4.1801242e-09 6.4728362e-12 4.4424453e-07 2.2366975e-11\n",
      " 6.9819983e-10]\n",
      "Max prob per class: [0.99952424 0.9980221  0.9989353  0.99999475 0.98880357 0.9958735\n",
      " 0.99772185 0.99726856 0.9658608  0.9907923  0.9544105  0.99995124\n",
      " 0.9942415  0.9505944  0.9967529  0.99999714 0.9998858  0.9998629\n",
      " 0.9745222  0.9999819  0.9987704  0.99848866 0.99991083 0.9780578\n",
      " 0.99754333 0.98448145 0.9993611  0.9767299  0.99994147 0.9967758\n",
      " 0.9999163  0.99999607 0.9702563  0.9999752  0.9986419  0.9999919\n",
      " 0.8955393  0.99978966 0.99914336 0.99969804 0.9801057  0.99959284\n",
      " 0.99979275 0.9876458  0.99952877 0.9998982  0.99999404 0.9926806\n",
      " 0.99975187 0.9992218  0.9985695  0.9998709  0.9992799  0.99163496\n",
      " 0.999894   0.9440558  0.9989894  0.9857028  0.9513559  0.9966624\n",
      " 0.9995097  0.9997713  0.97242254 0.99998736 0.9997497  0.9999937\n",
      " 0.99679714 0.9997799  0.65516293 0.9993056  0.9999007  0.9993678\n",
      " 0.9822512  0.99987876 0.99998295 0.99944633 0.9999541  0.9992448\n",
      " 0.9999727  0.99898523 0.99997354]\n",
      "Epoch 12: 100%|██████████| 640/640 [03:39<00:00,  2.92it/s, v_num=0, train_loss_step=0.131, val_loss=0.116, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.926, val_recall_all=0.704, val_f1_all=0.800, avg_threshold=0.362, val_acc=0.996, val_precision=0.926, val_recall=0.704, val_f1=0.800, train_loss_epoch=0.135, train_acc=0.993, train_precision=0.897, train_recall=0.466, train_f1=0.613] Min prob per class: [2.32202324e-09 2.13999877e-11 6.02622230e-10 2.22143215e-09\n",
      " 8.29876803e-13 1.10023310e-11 1.06650810e-12 1.09431865e-11\n",
      " 3.99571531e-09 4.28120768e-11 1.77649923e-10 4.65325861e-10\n",
      " 1.06018183e-09 2.27860786e-11 9.51894119e-09 4.14782304e-13\n",
      " 1.20865080e-11 6.11676820e-10 2.63890176e-09 5.24332044e-09\n",
      " 5.45093692e-11 3.35324368e-12 2.13519584e-11 1.75899937e-11\n",
      " 2.19292653e-10 7.64663832e-10 1.10792053e-09 3.67636997e-11\n",
      " 1.39740281e-12 1.08312033e-10 3.28682165e-10 3.47897489e-09\n",
      " 2.92342678e-10 2.78740832e-11 1.54717572e-09 7.99579070e-10\n",
      " 1.58959720e-10 7.55049929e-12 1.12758930e-13 4.35114306e-10\n",
      " 7.64174661e-12 5.17101223e-11 3.69984208e-12 9.35704847e-11\n",
      " 4.39781545e-11 9.01476810e-11 8.86372364e-11 4.87791023e-11\n",
      " 4.91887464e-09 2.75710899e-12 1.25395652e-10 8.11744283e-11\n",
      " 4.05280041e-08 1.95439151e-10 3.42080614e-10 2.75164808e-10\n",
      " 1.17618981e-09 1.26236036e-10 5.09437631e-12 3.33129857e-10\n",
      " 1.17190927e-11 6.63691602e-10 9.16863183e-11 5.91869748e-11\n",
      " 4.87659191e-10 1.05403158e-10 2.09532693e-08 3.66253278e-11\n",
      " 1.57389407e-10 1.80215776e-09 6.38414183e-11 1.26703836e-09\n",
      " 3.36470879e-10 1.27222417e-11 7.80864332e-11 5.92342120e-10\n",
      " 3.89275889e-10 3.53473162e-09 1.24585029e-07 8.95483514e-12\n",
      " 6.29327424e-11]\n",
      "Max prob per class: [0.99997175 0.99958926 0.99009424 0.9999292  0.97946334 0.9998822\n",
      " 0.999161   0.99796486 0.95596313 0.99791974 0.9589021  0.99997556\n",
      " 0.99368155 0.9943733  0.9999931  0.9996196  0.9997646  0.9986083\n",
      " 0.9731747  0.9999521  0.99914396 0.99964666 0.99986005 0.99969363\n",
      " 0.99918216 0.9892999  0.9964631  0.99026275 0.99938655 0.9977501\n",
      " 0.9999324  0.9999862  0.9933716  0.9999814  0.99795365 0.99999166\n",
      " 0.99774015 0.99981076 0.9989243  0.99813604 0.9660321  0.99979645\n",
      " 0.99990666 0.9671614  0.9974419  0.99994314 0.99993074 0.99490154\n",
      " 0.9999416  0.97009236 0.99961543 0.9998926  0.9989386  0.99557906\n",
      " 0.9993717  0.9994954  0.9993563  0.9997869  0.9012846  0.99891293\n",
      " 0.99811566 0.9998417  0.9962763  0.9999808  0.9601041  0.9999927\n",
      " 0.9958901  0.99368674 0.96928304 0.9992778  0.9999211  0.9998381\n",
      " 0.99344856 0.9998387  0.9999856  0.9996191  0.9888599  0.99973625\n",
      " 0.99998057 0.99941576 0.9988715 ]\n",
      "Epoch 13: 100%|██████████| 640/640 [03:37<00:00,  2.94it/s, v_num=0, train_loss_step=0.157, val_loss=0.115, val_ood_acc=0.929, val_acc_all=0.996, val_precision_all=0.914, val_recall_all=0.717, val_f1_all=0.803, avg_threshold=0.362, val_acc=0.996, val_precision=0.914, val_recall=0.717, val_f1=0.803, train_loss_epoch=0.132, train_acc=0.993, train_precision=0.899, train_recall=0.462, train_f1=0.610] Min prob per class: [6.91843782e-09 1.22581367e-09 5.03939424e-09 2.84224155e-09\n",
      " 1.06824427e-09 3.95665185e-12 1.63268045e-11 3.88634715e-11\n",
      " 8.06475650e-11 5.56715007e-10 1.52957563e-10 5.37861478e-10\n",
      " 3.94290711e-10 8.35294833e-10 6.10661466e-10 3.14512964e-11\n",
      " 8.82267315e-10 2.02387196e-09 2.46962379e-08 3.08553751e-08\n",
      " 3.57060242e-10 5.61135687e-11 1.47388809e-10 3.14609871e-09\n",
      " 6.08575468e-10 4.15869206e-09 1.34567752e-08 2.67949635e-10\n",
      " 9.66195735e-11 4.68861894e-10 7.12237247e-09 1.09215588e-08\n",
      " 9.40942419e-13 2.52404792e-10 1.15885121e-10 1.48332315e-08\n",
      " 1.38322565e-09 8.65998107e-11 8.20935073e-12 5.65473390e-10\n",
      " 1.66853076e-12 3.30917155e-10 3.44995782e-10 4.26828684e-10\n",
      " 4.85679682e-12 5.53629928e-12 5.16891474e-10 1.03079399e-10\n",
      " 1.12110925e-08 7.36426614e-11 4.36469194e-10 7.49126261e-10\n",
      " 2.18173124e-08 8.81908990e-10 1.21388553e-08 5.60398006e-10\n",
      " 3.92557320e-10 1.68161804e-11 7.37171366e-11 5.17627441e-10\n",
      " 1.28493943e-11 1.74579753e-10 4.59666749e-11 1.45994694e-09\n",
      " 9.50223011e-10 1.65145730e-10 4.35789804e-09 1.62917846e-10\n",
      " 7.08704428e-09 2.54095660e-08 1.16092629e-10 5.08524334e-11\n",
      " 8.64573760e-11 3.58448826e-10 3.14025778e-10 2.51644572e-08\n",
      " 1.03153408e-09 1.02696243e-08 5.65919038e-07 1.42351214e-10\n",
      " 1.36199843e-10]\n",
      "Max prob per class: [0.9999083  0.99961925 0.99911493 0.99997985 0.99355197 0.9997497\n",
      " 0.9998129  0.9993824  0.9172081  0.99791616 0.95341897 0.9999093\n",
      " 0.97534287 0.99176615 0.9992829  0.9999982  0.99967957 0.99968886\n",
      " 0.98410827 0.999736   0.9986877  0.9996419  0.9999416  0.9967174\n",
      " 0.99941695 0.9808133  0.9986002  0.9740458  0.9996996  0.998823\n",
      " 0.9999882  0.99999774 0.995038   0.9998234  0.99691594 0.99998975\n",
      " 0.99890697 0.9997718  0.99975973 0.9970956  0.9621105  0.9992766\n",
      " 0.99969447 0.9789094  0.99974805 0.99913955 0.99999034 0.99717593\n",
      " 0.99975246 0.9997129  0.99916744 0.99888784 0.9986451  0.9907514\n",
      " 0.99989545 0.9798728  0.9959849  0.9899092  0.9764871  0.998784\n",
      " 0.9967181  0.9996841  0.9943605  0.9999949  0.9995616  0.9999988\n",
      " 0.98866767 0.999238   0.89122623 0.99985886 0.9996997  0.99995136\n",
      " 0.9806489  0.9999863  0.99832875 0.99910045 0.9998481  0.9998362\n",
      " 0.9999832  0.99885416 0.9998822 ]\n",
      "Epoch 14: 100%|██████████| 640/640 [03:37<00:00,  2.94it/s, v_num=0, train_loss_step=0.136, val_loss=0.115, val_ood_acc=0.927, val_acc_all=0.996, val_precision_all=0.910, val_recall_all=0.737, val_f1_all=0.814, avg_threshold=0.366, val_acc=0.996, val_precision=0.910, val_recall=0.737, val_f1=0.814, train_loss_epoch=0.130, train_acc=0.993, train_precision=0.902, train_recall=0.464, train_f1=0.612] Min prob per class: [2.31514208e-09 5.53130153e-10 1.43758866e-10 2.52457899e-09\n",
      " 9.65116390e-11 7.77377240e-13 7.99943237e-11 5.32342486e-12\n",
      " 1.23169164e-09 2.21681340e-10 1.16868001e-10 1.16092629e-10\n",
      " 4.20946750e-11 2.47251525e-10 1.32685984e-09 1.65760475e-10\n",
      " 2.20238203e-11 5.64978864e-09 2.97536307e-08 1.33065461e-08\n",
      " 1.11690136e-10 3.44398149e-10 4.49489057e-11 5.04473907e-10\n",
      " 1.23438149e-10 5.55730517e-10 5.66501124e-10 5.03174412e-11\n",
      " 9.68536622e-12 1.56017599e-09 3.86712617e-09 2.32551578e-09\n",
      " 9.15265693e-12 5.90894333e-10 2.05516215e-10 2.79934437e-10\n",
      " 4.67124386e-12 3.23878944e-11 2.37699999e-11 2.14397500e-10\n",
      " 5.87235176e-13 7.21255000e-11 1.05699713e-10 5.91700688e-10\n",
      " 3.98134433e-13 3.27067297e-11 6.36500436e-11 7.61592248e-11\n",
      " 1.72169856e-09 5.23336105e-12 8.15534473e-10 1.63125541e-10\n",
      " 2.44774725e-08 1.27226418e-09 2.34146724e-09 2.61261263e-10\n",
      " 5.48052892e-09 4.08209612e-11 1.38625708e-10 3.27168714e-09\n",
      " 3.36410379e-11 4.27548760e-11 4.00924099e-10 2.27072153e-10\n",
      " 1.59936031e-09 2.64789371e-11 8.27043678e-09 9.47829523e-11\n",
      " 1.54629731e-08 4.31112124e-09 1.69748354e-10 3.70004132e-11\n",
      " 2.53246757e-10 1.18643827e-11 2.06358464e-09 1.25671358e-08\n",
      " 5.93504090e-08 2.84565904e-09 1.49727754e-07 8.68431660e-10\n",
      " 1.46712018e-10]\n",
      "Max prob per class: [0.99992    0.9971878  0.9874708  0.9999924  0.99347454 0.9981749\n",
      " 0.9998136  0.9985808  0.9708746  0.9943177  0.98293316 0.9998914\n",
      " 0.99824286 0.9791525  0.9995468  0.9999938  0.998517   0.9958597\n",
      " 0.9981817  0.99990046 0.9995708  0.99974126 0.9999391  0.99311525\n",
      " 0.9951374  0.9851511  0.9995751  0.9811445  0.9991091  0.99954\n",
      " 0.99998045 0.99997056 0.9970571  0.99997115 0.9998     0.999984\n",
      " 0.9981499  0.9980033  0.9991756  0.9941459  0.97779924 0.9996866\n",
      " 0.9995408  0.9902491  0.99318576 0.99970514 0.9999825  0.999619\n",
      " 0.9998416  0.9989058  0.9650311  0.99743795 0.99927586 0.99961275\n",
      " 0.99935967 0.99773103 0.99887663 0.9988071  0.9690127  0.9996166\n",
      " 0.9996724  0.99980885 0.99542063 0.99989235 0.9999335  0.99998593\n",
      " 0.9935516  0.9993242  0.97872144 0.999655   0.9997819  0.99972004\n",
      " 0.7918043  0.99991095 0.99999785 0.999668   0.9963888  0.9997131\n",
      " 0.9999671  0.9997423  0.99995637]\n",
      "Epoch 15: 100%|██████████| 640/640 [03:38<00:00,  2.93it/s, v_num=0, train_loss_step=0.174, val_loss=0.113, val_ood_acc=0.925, val_acc_all=0.996, val_precision_all=0.918, val_recall_all=0.730, val_f1_all=0.813, avg_threshold=0.375, val_acc=0.996, val_precision=0.918, val_recall=0.730, val_f1=0.813, train_loss_epoch=0.128, train_acc=0.993, train_precision=0.911, train_recall=0.466, train_f1=0.616] Min prob per class: [1.07470366e-10 2.10183329e-10 5.27225728e-11 2.08557616e-10\n",
      " 3.66180662e-11 4.87049854e-12 3.85123574e-12 1.60970577e-11\n",
      " 2.53109245e-09 1.57137164e-10 3.04431008e-10 5.39766842e-10\n",
      " 3.34651251e-10 1.14061725e-10 1.43448267e-10 2.98685284e-13\n",
      " 2.20386209e-10 2.84153168e-09 5.08348599e-08 2.10010018e-08\n",
      " 1.77775936e-11 8.69724223e-11 4.88637464e-11 8.38917200e-11\n",
      " 5.81479309e-10 4.78396034e-09 7.84392462e-09 1.54598612e-10\n",
      " 6.68062480e-12 2.25002295e-10 5.98703725e-11 2.01079353e-09\n",
      " 2.07120196e-10 4.97143438e-11 8.73431827e-10 1.22923205e-09\n",
      " 2.73931072e-10 1.83654741e-11 9.02105376e-13 1.16386234e-09\n",
      " 4.58028190e-12 2.30843747e-11 2.99040133e-11 6.27548957e-10\n",
      " 1.69806218e-11 2.13811496e-10 1.40472897e-10 1.64195935e-10\n",
      " 1.94508809e-09 9.83472401e-11 1.31479017e-09 2.10490236e-10\n",
      " 5.95890404e-09 1.79255200e-09 1.02026378e-08 2.51382049e-09\n",
      " 3.97867045e-10 1.87027477e-11 2.90615206e-11 1.10992360e-09\n",
      " 6.88954926e-12 2.79380095e-11 1.75799736e-10 4.14600021e-11\n",
      " 2.97067621e-10 1.00728592e-11 6.16455731e-09 1.21430865e-09\n",
      " 3.31355032e-09 2.35520470e-09 1.64913031e-11 6.17330853e-10\n",
      " 3.43800904e-10 8.26763224e-12 7.49550755e-10 4.20161061e-09\n",
      " 1.22284571e-09 1.63822428e-10 2.79087118e-07 1.56691202e-10\n",
      " 2.03461539e-10]\n",
      "Max prob per class: [0.99808097 0.99389195 0.99907035 0.99998057 0.97340065 0.9994862\n",
      " 0.99962354 0.993789   0.9622055  0.9969278  0.99135745 0.99986017\n",
      " 0.9927286  0.9961677  0.9996679  0.99998343 0.9999833  0.99260956\n",
      " 0.98615277 0.99981755 0.99511    0.99987483 0.9997557  0.99902046\n",
      " 0.9906226  0.9659077  0.9987135  0.98658085 0.99998045 0.998456\n",
      " 0.99985504 0.9999933  0.99126476 0.9999987  0.9962186  0.9999589\n",
      " 0.99900824 0.99917006 0.99949074 0.99958223 0.93540007 0.99941194\n",
      " 0.99786323 0.99976724 0.99900335 0.99990404 0.9999052  0.9981925\n",
      " 0.9997327  0.9993117  0.9936021  0.999974   0.9990534  0.99846977\n",
      " 0.9994123  0.99921286 0.9990903  0.9980379  0.92711264 0.9996836\n",
      " 0.9948453  0.99945825 0.9965024  0.99954945 0.99962807 0.99992394\n",
      " 0.99339783 0.99229616 0.9170955  0.9998659  0.9998976  0.9998925\n",
      " 0.9932239  0.99997926 0.99990535 0.9984848  0.9825295  0.9978654\n",
      " 0.9999896  0.99717665 0.9996604 ]\n",
      "Epoch 16: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.123, val_loss=0.113, val_ood_acc=0.927, val_acc_all=0.996, val_precision_all=0.919, val_recall_all=0.719, val_f1_all=0.807, avg_threshold=0.370, val_acc=0.996, val_precision=0.919, val_recall=0.719, val_f1=0.807, train_loss_epoch=0.129, train_acc=0.993, train_precision=0.915, train_recall=0.468, train_f1=0.619] Min prob per class: [3.43805029e-09 4.13512780e-10 2.33836422e-10 1.96518779e-09\n",
      " 7.61124636e-11 7.72492259e-13 7.32542846e-12 3.81763163e-11\n",
      " 1.56019017e-10 1.23358587e-10 3.44764044e-11 9.49387902e-10\n",
      " 7.81572473e-10 7.48737738e-10 1.50391355e-09 5.86902055e-11\n",
      " 1.35092756e-10 2.85909141e-09 3.23587983e-08 5.09284881e-09\n",
      " 8.37644468e-11 3.99301425e-11 5.02308264e-12 4.44353415e-10\n",
      " 3.65113983e-09 1.13360377e-09 5.58890767e-10 1.38333656e-09\n",
      " 1.35904260e-10 3.61595398e-09 8.37606484e-10 3.21297544e-09\n",
      " 2.14667978e-10 9.24143574e-12 8.09103062e-10 4.86170704e-09\n",
      " 2.12116449e-10 2.64160326e-11 2.43492431e-11 5.73831926e-10\n",
      " 4.86447549e-11 1.00451836e-09 7.09622083e-11 5.79016834e-10\n",
      " 8.24003539e-12 8.49781287e-11 5.46859884e-11 1.45053664e-10\n",
      " 6.52997478e-09 1.13448785e-10 1.96115416e-10 1.67347580e-10\n",
      " 4.33613367e-09 4.78794959e-09 3.08731996e-09 5.08480236e-09\n",
      " 1.29334510e-09 2.80139800e-12 3.59181129e-10 1.74667791e-09\n",
      " 1.21588406e-10 1.78547399e-10 5.10820483e-11 9.46263845e-10\n",
      " 6.88681423e-09 3.80739468e-10 2.59947641e-09 3.50379226e-10\n",
      " 1.48875232e-10 5.02946529e-09 2.59116867e-10 2.52907889e-10\n",
      " 3.55712987e-10 1.71083859e-11 7.67656660e-10 2.29472130e-09\n",
      " 4.99505859e-09 1.32598965e-09 1.31579853e-07 1.06548638e-10\n",
      " 3.06472868e-11]\n",
      "Max prob per class: [0.999762   0.9999083  0.9993174  0.99998236 0.9919946  0.9982784\n",
      " 0.9990766  0.99600875 0.9944812  0.9971     0.9798362  0.9995963\n",
      " 0.9833256  0.9731995  0.9997696  0.999998   0.9998529  0.99931633\n",
      " 0.9955664  0.9998282  0.99950576 0.99982446 0.99996567 0.96291333\n",
      " 0.98641276 0.9868003  0.9990326  0.9877252  0.9998802  0.9990675\n",
      " 0.9999484  0.99999714 0.99954504 0.9999211  0.99738806 0.99999285\n",
      " 0.9924919  0.99930644 0.9992594  0.99833316 0.9816596  0.9998323\n",
      " 0.9983144  0.9706772  0.9987281  0.9999027  0.9999869  0.99727505\n",
      " 0.99995697 0.9992632  0.9981988  0.9989868  0.9825447  0.99548197\n",
      " 0.99989414 0.999019   0.9995957  0.97223425 0.98759764 0.99844366\n",
      " 0.99706405 0.99970585 0.9909101  0.999622   0.999881   0.99996555\n",
      " 0.9951443  0.99902797 0.87428    0.99986565 0.9999082  0.99993336\n",
      " 0.98258406 0.9999894  0.999869   0.999764   0.9986822  0.9993412\n",
      " 0.9999733  0.99956876 0.9999331 ]\n",
      "Epoch 16: 100%|██████████| 640/640 [04:14<00:00,  2.52it/s, v_num=0, train_loss_step=0.123, val_loss=0.111, val_ood_acc=0.927, val_acc_all=0.996, val_precision_all=0.922, val_recall_all=0.729, val_f1_all=0.814, avg_threshold=0.369, val_acc=0.996, val_precision=0.922, val_recall=0.729, val_f1=0.814, train_loss_epoch=0.125, train_acc=0.993, train_precision=0.917, train_recall=0.464, train_f1=0.616]\n",
      "Fold 3/5\n",
      "Setup ran successfully\n",
      "Epoch 0: 100%|██████████| 1/1 [4:17:44<00:00,  0.00it/s, train_loss_step=1.050]\n",
      "Epoch 0:   0%|          | 0/1 [4:56:59<?, ?it/s]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name              | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | feature_extractor | Sequential          | 65.3 M | train\n",
      "1  | ood_classifier    | Sequential          | 2.0 K  | train\n",
      "2  | classifier        | Sequential          | 165 K  | train\n",
      "3  | loss_fn_class     | BCEWithLogitsLoss   | 0      | train\n",
      "4  | loss_fn_ood       | BCEWithLogitsLoss   | 0      | train\n",
      "5  | train_acc         | MultilabelAccuracy  | 0      | train\n",
      "6  | val_acc           | MultilabelAccuracy  | 0      | train\n",
      "7  | train_precision   | MultilabelPrecision | 0      | train\n",
      "8  | val_precision     | MultilabelPrecision | 0      | train\n",
      "9  | train_recall      | MultilabelRecall    | 0      | train\n",
      "10 | val_recall        | MultilabelRecall    | 0      | train\n",
      "11 | train_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1            | MultilabelF1Score   | 0      | train\n",
      "13 | ood_acc           | BinaryAccuracy      | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "168 K     Trainable params\n",
      "65.3 M    Non-trainable params\n",
      "65.5 M    Total params\n",
      "261.938   Total estimated model params size (MB)\n",
      "958       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.22it/s]Min prob per class: [0.47607133 0.4645635  0.4590166  0.47263545 0.45682618 0.46767557\n",
      " 0.46789247 0.4672302  0.45690304 0.49160925 0.48607317 0.46938318\n",
      " 0.46890217 0.47141746 0.462655   0.44099832 0.45264015 0.44052568\n",
      " 0.4329242  0.4614996  0.4380787  0.47369683 0.47806692 0.4793785\n",
      " 0.47659913 0.46900067 0.4817522  0.45883197 0.45868558 0.48714676\n",
      " 0.44131026 0.46699247 0.45976636 0.45776713 0.4689495  0.46715593\n",
      " 0.45186237 0.44880965 0.4514997  0.47170126 0.45506158 0.47073078\n",
      " 0.4565152  0.4890375  0.4701799  0.4542644  0.44441774 0.46783775\n",
      " 0.46884125 0.44204628 0.47256386 0.4729137  0.47553584 0.47266665\n",
      " 0.4648748  0.47069988 0.48068032 0.4692033  0.4623721  0.46728423\n",
      " 0.47934163 0.47123072 0.48352548 0.47750786 0.4760383  0.46304175\n",
      " 0.46517187 0.47362247 0.430504   0.47225234 0.4400271  0.4795421\n",
      " 0.4730953  0.46553063 0.42116976 0.47016275 0.45937675 0.46412602\n",
      " 0.46267873 0.49502054 0.47313288]\n",
      "Max prob per class: [0.5466284  0.5234696  0.54417014 0.54753226 0.53521025 0.5462857\n",
      " 0.53037125 0.5320468  0.51346093 0.5443333  0.5591518  0.55020726\n",
      " 0.51809466 0.5411963  0.53245944 0.5282616  0.52820855 0.53011626\n",
      " 0.51270956 0.5486488  0.51155686 0.5361603  0.54679894 0.553968\n",
      " 0.53874123 0.5327579  0.5738311  0.53766125 0.51261693 0.5355787\n",
      " 0.5334328  0.52981186 0.52840173 0.52676225 0.5440753  0.54333556\n",
      " 0.53506094 0.5230911  0.5180662  0.5306708  0.5108817  0.564208\n",
      " 0.51810724 0.5489122  0.54077303 0.5204271  0.5161366  0.54514945\n",
      " 0.54016286 0.54481584 0.5393285  0.5421281  0.5221566  0.53345513\n",
      " 0.5292694  0.5302691  0.5524693  0.53766245 0.52673113 0.51544887\n",
      " 0.5548528  0.52385706 0.5457381  0.5307858  0.5439385  0.5399368\n",
      " 0.5214675  0.530866   0.4998866  0.55802554 0.52049947 0.5335521\n",
      " 0.51927924 0.53073716 0.51957506 0.5308959  0.54615074 0.5163474\n",
      " 0.5303161  0.5365014  0.53589344]\n",
      "Warning: No positive samples for class 3, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 4, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 5, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 6, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 7, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 8, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 9, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 10, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 11, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 12, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 13, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 14, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 15, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 16, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 17, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 18, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 19, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 20, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 21, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 22, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 23, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 24, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 25, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 26, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 27, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 28, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 29, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 30, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 31, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 32, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 33, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 34, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 35, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 36, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 37, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 38, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 39, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 40, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 41, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 42, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 43, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 44, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 45, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 46, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 47, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 48, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 49, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 50, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 51, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 52, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 53, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 54, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 55, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 56, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 57, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 58, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 59, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 60, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 61, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 62, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 63, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 64, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 65, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 66, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 67, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 68, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 69, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 70, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 71, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 72, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 73, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 74, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 75, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 76, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 77, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 78, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 79, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 80, keeping threshold at 0.5\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.199] Min prob per class: [5.13372879e-06 4.40431249e-05 3.14432946e-05 7.70995539e-05\n",
      " 2.83384143e-05 3.11148797e-05 3.56818309e-05 4.75728702e-05\n",
      " 4.91812680e-05 3.01332529e-05 2.14836145e-05 1.50481937e-05\n",
      " 1.54988684e-05 2.47366770e-05 8.39184577e-05 2.76995088e-05\n",
      " 7.86927631e-05 4.56596317e-05 3.63036997e-05 1.49069028e-05\n",
      " 6.38196434e-05 4.96314060e-05 7.21842298e-05 6.05834284e-05\n",
      " 4.90402999e-05 1.51036620e-05 2.75860766e-05 5.29952194e-05\n",
      " 1.92381718e-04 3.44312612e-05 4.29667380e-05 3.45922672e-05\n",
      " 1.03640250e-05 2.37306158e-05 2.55127361e-05 2.45549982e-05\n",
      " 1.25350663e-04 1.34898164e-05 6.75416668e-05 3.91264002e-05\n",
      " 4.20811921e-05 1.60189629e-05 1.33699396e-05 2.43159575e-05\n",
      " 3.97320655e-05 6.16233301e-05 3.41818668e-05 1.12694921e-04\n",
      " 8.93543438e-06 6.37773046e-05 5.90722302e-05 1.01823745e-04\n",
      " 4.69784500e-05 7.05427374e-05 1.81932774e-05 5.19557325e-05\n",
      " 1.09840330e-05 2.23115785e-05 3.53184405e-05 1.31963561e-05\n",
      " 2.70703877e-05 2.04248372e-05 1.17444863e-04 5.98810184e-05\n",
      " 2.77376275e-05 5.62883070e-05 4.13644193e-05 5.78575273e-05\n",
      " 1.43736743e-05 3.20484287e-05 4.03491504e-05 4.74598601e-05\n",
      " 5.61550223e-05 5.32387639e-05 7.52474953e-05 6.26798865e-05\n",
      " 5.56963205e-05 5.62389978e-05 2.22102064e-03 1.42755889e-05\n",
      " 4.54986330e-05]\n",
      "Max prob per class: [0.1630957  0.19499882 0.1800715  0.21964316 0.1940974  0.20598729\n",
      " 0.23007803 0.19282827 0.41703284 0.21258035 0.20474698 0.17697217\n",
      " 0.16927245 0.18860304 0.24941705 0.15659109 0.19266027 0.20862922\n",
      " 0.18178661 0.24659443 0.1938115  0.19685622 0.25755322 0.19438577\n",
      " 0.2064163  0.19862603 0.18716474 0.19921903 0.36029875 0.24639481\n",
      " 0.19252305 0.19163525 0.17535912 0.33459064 0.16024679 0.21413304\n",
      " 0.23349889 0.17741649 0.20032597 0.179908   0.18548903 0.16829288\n",
      " 0.18242882 0.19301246 0.21055003 0.4969942  0.16780815 0.22125807\n",
      " 0.18593265 0.17236629 0.19590518 0.20378414 0.17478713 0.21323629\n",
      " 0.17140412 0.2038561  0.21036743 0.18342116 0.19152707 0.16036874\n",
      " 0.1807951  0.2744653  0.215258   0.18987694 0.18118109 0.28320757\n",
      " 0.1915002  0.18153046 0.1721803  0.2166932  0.21832807 0.18650986\n",
      " 0.24711916 0.18909214 0.23656444 0.19929036 0.18401258 0.18842673\n",
      " 0.9995516  0.17277074 0.20288107]\n",
      "Epoch 1: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.139, val_loss=0.178, val_ood_acc=0.902, val_acc_all=0.993, val_precision_all=0.879, val_recall_all=0.470, val_f1_all=0.612, avg_threshold=0.374, val_acc=0.993, val_precision=0.879, val_recall=0.470, val_f1=0.612, train_loss_epoch=0.309, train_acc=0.990, train_precision=0.665, train_recall=0.423, train_f1=0.517] Min prob per class: [8.65440416e-06 3.80372990e-06 6.52877770e-06 1.76930353e-05\n",
      " 1.84343264e-06 3.06751053e-06 8.75704518e-06 5.09599840e-06\n",
      " 3.59678643e-06 1.48271365e-05 4.48722659e-07 3.98805741e-06\n",
      " 1.86466787e-05 9.02458396e-06 4.21020695e-06 2.10048643e-06\n",
      " 2.67819928e-06 1.18052014e-06 4.87799443e-05 7.68968966e-05\n",
      " 1.54536042e-06 8.65277889e-06 2.42389160e-06 1.13165643e-05\n",
      " 3.27737416e-06 9.89197360e-06 1.18108919e-05 1.07332453e-05\n",
      " 9.23700156e-07 2.73570276e-05 9.72828843e-07 1.82702679e-05\n",
      " 9.15648616e-06 1.52053171e-05 6.67476661e-06 1.82800795e-05\n",
      " 5.65142454e-06 3.12526254e-06 3.75431046e-05 1.68495046e-06\n",
      " 1.44777357e-06 6.13814245e-06 5.51619542e-06 1.32651094e-05\n",
      " 9.38477751e-06 1.37774123e-06 1.01917549e-05 2.55228474e-06\n",
      " 1.81460600e-05 1.59669628e-06 7.76733543e-07 1.74127726e-05\n",
      " 1.44339356e-05 1.99298556e-05 1.22620922e-05 1.22033543e-05\n",
      " 1.94165150e-05 3.91927779e-06 7.68833343e-06 5.19997729e-06\n",
      " 1.14086170e-05 3.06054449e-06 7.79274160e-06 3.07158698e-05\n",
      " 1.78313585e-05 3.40542283e-05 1.02261520e-05 7.83558335e-06\n",
      " 5.58610518e-05 6.53701936e-05 9.15136116e-06 3.94102017e-06\n",
      " 4.35815346e-05 7.37871915e-06 7.81595554e-06 1.18794989e-04\n",
      " 2.33665032e-05 7.38149947e-06 2.22523795e-05 9.74939303e-06\n",
      " 3.53314044e-06]\n",
      "Max prob per class: [0.97830725 0.8646173  0.75728136 0.9652101  0.5728528  0.86968935\n",
      " 0.5207894  0.7204937  0.8113811  0.80756414 0.4379254  0.9513248\n",
      " 0.93992186 0.891778   0.6057635  0.672303   0.46770453 0.83191764\n",
      " 0.49503216 0.98688936 0.9074261  0.9229464  0.8551042  0.5040689\n",
      " 0.5940498  0.5498585  0.8332256  0.76987606 0.82186514 0.9875957\n",
      " 0.47463843 0.97950727 0.5642101  0.9751652  0.70385706 0.97397804\n",
      " 0.43091393 0.84794575 0.6885812  0.84714514 0.4286493  0.89286387\n",
      " 0.74516666 0.89987504 0.6666474  0.9727296  0.92790526 0.7496991\n",
      " 0.99325997 0.41873318 0.8096047  0.8696662  0.8877105  0.92550623\n",
      " 0.9190182  0.47416037 0.6951148  0.8625551  0.44907558 0.6927305\n",
      " 0.66087586 0.75950533 0.66288745 0.692523   0.91271716 0.98412895\n",
      " 0.89399034 0.7656528  0.6226521  0.9796783  0.8481152  0.74877024\n",
      " 0.70601743 0.96158195 0.96330386 0.66242903 0.71330434 0.5791611\n",
      " 0.99976605 0.5202684  0.9037791 ]\n",
      "Epoch 2: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.106, val_loss=0.143, val_ood_acc=0.912, val_acc_all=0.994, val_precision_all=0.920, val_recall_all=0.551, val_f1_all=0.689, avg_threshold=0.321, val_acc=0.994, val_precision=0.920, val_recall=0.551, val_f1=0.689, train_loss_epoch=0.165, train_acc=0.992, train_precision=0.904, train_recall=0.436, train_f1=0.589] Min prob per class: [2.50885546e-06 4.81382017e-07 5.13379860e-07 6.37299172e-06\n",
      " 6.26276062e-07 5.34342526e-08 4.39957120e-07 1.15746047e-06\n",
      " 4.47533267e-07 1.66804944e-06 2.29427542e-06 1.61627884e-06\n",
      " 1.31709669e-07 2.59630497e-06 1.16279443e-06 1.21916173e-06\n",
      " 2.58811212e-07 1.04324113e-06 6.26711972e-06 5.03920364e-06\n",
      " 3.87945784e-06 1.84361022e-06 2.70045831e-07 4.54642475e-07\n",
      " 2.55619443e-06 1.37533718e-07 1.26410650e-05 9.90270337e-07\n",
      " 1.82660642e-06 9.59725185e-07 1.66436450e-07 7.01331601e-06\n",
      " 2.25190007e-07 1.88525448e-07 4.19826875e-07 5.58562590e-07\n",
      " 4.21208824e-07 1.67364888e-07 1.33360629e-07 3.22310228e-07\n",
      " 9.02699711e-08 3.55295356e-07 1.92773641e-05 1.79644326e-06\n",
      " 8.25579516e-09 9.97000257e-07 1.09025757e-06 4.79797620e-07\n",
      " 2.35797089e-07 3.17987133e-06 1.45083629e-06 1.72020134e-06\n",
      " 1.81644991e-05 1.10229212e-06 1.54412146e-06 1.52594259e-06\n",
      " 6.09570361e-06 6.44795705e-07 1.55010653e-08 4.00606922e-07\n",
      " 1.54344718e-06 4.17479242e-07 3.13776212e-07 1.03852301e-06\n",
      " 6.42858276e-06 2.90946468e-06 7.38523795e-06 2.25138237e-07\n",
      " 9.81071753e-06 1.98244834e-06 7.93761572e-08 2.93705830e-06\n",
      " 5.36215964e-07 9.09604410e-08 5.07694097e-07 1.00451125e-05\n",
      " 3.39658800e-05 1.62786216e-06 5.81836002e-06 5.37545338e-06\n",
      " 3.04763404e-07]\n",
      "Max prob per class: [0.9745247  0.9235954  0.997787   0.9966677  0.84755015 0.96415895\n",
      " 0.9338718  0.8904226  0.7853837  0.93751013 0.94999063 0.9824613\n",
      " 0.9119674  0.91751754 0.9084892  0.9863984  0.7391573  0.97486556\n",
      " 0.7078172  0.99945    0.9939355  0.9882527  0.9952303  0.76379466\n",
      " 0.68316203 0.77980566 0.9919144  0.93030566 0.9845184  0.98789376\n",
      " 0.93367887 0.9979868  0.9788215  0.99794084 0.97328305 0.9991984\n",
      " 0.47191378 0.99153554 0.94850945 0.96095395 0.45836017 0.84898555\n",
      " 0.9511066  0.99460393 0.93613917 0.9980464  0.9966493  0.84459543\n",
      " 0.9919618  0.92273784 0.94941175 0.9874828  0.9729856  0.9958637\n",
      " 0.9927254  0.7889485  0.9411777  0.96259165 0.60870045 0.9664049\n",
      " 0.9474939  0.99646    0.8774299  0.95825225 0.96325207 0.9973406\n",
      " 0.9696173  0.9612983  0.713622   0.9962184  0.8887577  0.9735595\n",
      " 0.84164226 0.96609783 0.9878489  0.927987   0.98579144 0.98295593\n",
      " 0.99991083 0.9424392  0.98362213]\n",
      "Epoch 3: 100%|██████████| 640/640 [03:36<00:00,  2.95it/s, v_num=0, train_loss_step=0.147, val_loss=0.133, val_ood_acc=0.916, val_acc_all=0.995, val_precision_all=0.922, val_recall_all=0.607, val_f1_all=0.732, avg_threshold=0.296, val_acc=0.995, val_precision=0.922, val_recall=0.607, val_f1=0.732, train_loss_epoch=0.155, train_acc=0.992, train_precision=0.892, train_recall=0.442, train_f1=0.591] Min prob per class: [3.17365766e-06 1.50615108e-07 1.37490716e-07 2.63332026e-07\n",
      " 5.66309089e-09 1.20428512e-08 2.00106509e-08 1.36036666e-08\n",
      " 1.67755758e-07 1.30992305e-06 1.28220719e-07 8.87102516e-08\n",
      " 6.82665728e-08 3.68111159e-08 6.42961027e-08 2.01635579e-08\n",
      " 2.05663078e-08 6.13806819e-08 4.06714605e-07 5.03409410e-07\n",
      " 2.40284042e-08 2.04309885e-08 9.32337372e-08 4.42845227e-08\n",
      " 1.26426482e-06 4.73913929e-07 8.33718843e-07 1.00604707e-08\n",
      " 1.05922474e-07 4.80678295e-07 4.98870577e-07 2.44089028e-06\n",
      " 6.32729709e-08 3.25818996e-06 1.37375508e-07 2.08200890e-06\n",
      " 1.29911461e-06 1.95733314e-08 1.20897328e-08 3.56712121e-07\n",
      " 1.60468101e-08 5.21901704e-08 8.73212514e-07 4.64494224e-06\n",
      " 4.08181222e-09 1.80199713e-08 3.17205782e-08 1.47564677e-07\n",
      " 1.19086201e-06 1.79462489e-07 2.06935866e-07 4.06286631e-07\n",
      " 1.37792915e-06 1.56182992e-07 5.29136571e-07 5.97407634e-08\n",
      " 2.58560306e-07 1.38037791e-07 1.33611408e-07 7.43254702e-08\n",
      " 6.50928484e-08 1.01921394e-07 6.77770657e-08 2.46953906e-08\n",
      " 1.67119893e-07 5.28446662e-07 1.20677396e-06 5.42382530e-08\n",
      " 1.37058942e-05 1.12759415e-06 4.02057836e-08 2.38940061e-06\n",
      " 1.06005338e-07 1.06106683e-07 2.38161050e-07 4.26779934e-07\n",
      " 4.65664186e-07 1.27267796e-07 6.90482875e-06 9.95888740e-08\n",
      " 1.00040742e-09]\n",
      "Max prob per class: [0.9992743  0.99256974 0.9992835  0.9997421  0.8400594  0.9785363\n",
      " 0.99379456 0.9580965  0.9646725  0.9829108  0.99008    0.9949221\n",
      " 0.9921159  0.9519717  0.94111246 0.993948   0.8968606  0.988777\n",
      " 0.88820755 0.9995378  0.9773362  0.99468994 0.9987565  0.9155182\n",
      " 0.9871007  0.97121686 0.96373296 0.9748663  0.99487174 0.9976311\n",
      " 0.99961996 0.9928261  0.9982054  0.9998746  0.99753404 0.99996555\n",
      " 0.9692814  0.99702257 0.9491698  0.9965134  0.8719374  0.85374767\n",
      " 0.9859017  0.9985201  0.9872853  0.9957241  0.9934848  0.9867808\n",
      " 0.9979724  0.96950364 0.9829858  0.9953479  0.99810934 0.9980032\n",
      " 0.9852926  0.973355   0.9897718  0.99570423 0.96830744 0.99393207\n",
      " 0.94898105 0.991512   0.9783999  0.9976749  0.9941948  0.9997404\n",
      " 0.9948344  0.9904997  0.65461427 0.9989385  0.99147743 0.99759126\n",
      " 0.8821111  0.99227726 0.9964204  0.9513163  0.9949837  0.9725099\n",
      " 0.9999949  0.990176   0.81430054]\n",
      "Epoch 4: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.338, val_loss=0.131, val_ood_acc=0.921, val_acc_all=0.995, val_precision_all=0.885, val_recall_all=0.686, val_f1_all=0.773, avg_threshold=0.296, val_acc=0.995, val_precision=0.885, val_recall=0.686, val_f1=0.773, train_loss_epoch=0.151, train_acc=0.993, train_precision=0.899, train_recall=0.447, train_f1=0.598] Min prob per class: [1.08163181e-06 4.07657055e-08 7.91569033e-09 1.59000511e-08\n",
      " 1.06071063e-07 3.10720587e-08 9.28243438e-09 4.77306195e-08\n",
      " 3.77226108e-08 2.48839115e-07 6.82342929e-08 1.00857207e-07\n",
      " 1.15677921e-07 3.41611504e-08 1.89053448e-08 5.66510039e-09\n",
      " 5.44565326e-09 2.02006660e-08 1.21578069e-06 5.41059671e-06\n",
      " 1.36631373e-08 5.75524561e-09 1.75092396e-09 1.19716264e-08\n",
      " 8.16110912e-08 2.16267139e-07 3.67498075e-07 1.32769342e-08\n",
      " 4.38620269e-08 6.06965926e-08 1.18740457e-08 3.03786067e-07\n",
      " 4.51620696e-09 1.81171103e-07 3.16561852e-07 2.21467715e-08\n",
      " 1.33136339e-07 3.80252878e-08 4.07029219e-08 3.46986262e-09\n",
      " 3.34848238e-08 2.99801656e-07 9.86037634e-08 2.10379221e-06\n",
      " 2.10778794e-09 1.53307965e-07 5.03795512e-08 4.74232223e-07\n",
      " 3.91289132e-06 6.42498208e-07 2.52261202e-07 1.48802201e-07\n",
      " 4.64154084e-07 6.01436367e-09 6.64826203e-08 2.00587280e-07\n",
      " 7.82114569e-08 1.60296789e-08 2.70466405e-08 7.34015657e-08\n",
      " 2.85214856e-08 1.02682138e-08 2.27682584e-08 2.04974224e-08\n",
      " 5.20417665e-08 3.84675076e-07 1.72813350e-06 7.97149191e-09\n",
      " 4.87190164e-06 5.23247820e-07 1.46205705e-08 2.03467039e-07\n",
      " 1.22678898e-07 8.31061708e-09 3.36092789e-08 4.69728911e-06\n",
      " 3.75144305e-06 6.56018813e-08 4.10519351e-06 1.46641312e-07\n",
      " 9.88492177e-10]\n",
      "Max prob per class: [0.99941134 0.9918383  0.94269675 0.9992899  0.9786186  0.99803764\n",
      " 0.9582981  0.9891304  0.96409446 0.95697474 0.9935446  0.99803454\n",
      " 0.9968651  0.9655944  0.97809905 0.99921465 0.95377356 0.9966761\n",
      " 0.9172488  0.99960834 0.9978846  0.9960083  0.9959584  0.8796658\n",
      " 0.9620746  0.9967135  0.99523586 0.96828103 0.9989372  0.9986083\n",
      " 0.9990029  0.9996953  0.9781568  0.998563   0.9823056  0.99996936\n",
      " 0.81650203 0.99862003 0.9293013  0.9885443  0.8195226  0.92396694\n",
      " 0.86027026 0.995862   0.9744264  0.99985313 0.99934584 0.99651223\n",
      " 0.9997904  0.99118173 0.9994406  0.9954151  0.9986093  0.9812416\n",
      " 0.9250853  0.86705124 0.9837119  0.9302682  0.9886361  0.9970561\n",
      " 0.9821786  0.9518131  0.9985638  0.9707727  0.99163884 0.9999553\n",
      " 0.99032706 0.9986778  0.75681037 0.9997069  0.9966524  0.9950134\n",
      " 0.9463335  0.9989473  0.99751234 0.9726543  0.99965215 0.9873263\n",
      " 0.9999933  0.9995988  0.9681369 ]\n",
      "Epoch 5: 100%|██████████| 640/640 [03:34<00:00,  2.98it/s, v_num=0, train_loss_step=0.215, val_loss=0.135, val_ood_acc=0.914, val_acc_all=0.995, val_precision_all=0.865, val_recall_all=0.694, val_f1_all=0.770, avg_threshold=0.300, val_acc=0.995, val_precision=0.865, val_recall=0.694, val_f1=0.770, train_loss_epoch=0.145, train_acc=0.993, train_precision=0.904, train_recall=0.445, train_f1=0.597] Min prob per class: [2.25943566e-07 2.56079762e-08 1.30816119e-07 4.03595868e-09\n",
      " 1.11213865e-08 7.13829751e-09 4.07769551e-09 1.53573083e-08\n",
      " 6.59591759e-09 1.83368254e-09 5.69752530e-08 6.22562313e-09\n",
      " 1.88393283e-08 4.44776056e-08 3.11719042e-08 1.67799694e-08\n",
      " 1.35749474e-08 6.03709704e-09 6.44100098e-08 8.01789639e-08\n",
      " 1.51466644e-08 1.36857958e-09 9.42470990e-09 6.32815966e-10\n",
      " 8.38267340e-08 2.75804037e-08 3.84675375e-10 7.12875936e-09\n",
      " 7.12701564e-10 9.95649430e-08 1.26540078e-09 1.23124408e-07\n",
      " 1.75040982e-09 2.30192376e-09 5.15505683e-08 3.38614200e-07\n",
      " 1.09018439e-09 1.44423325e-07 2.82241146e-08 1.89740899e-08\n",
      " 1.33240183e-08 4.01848581e-08 1.42195256e-09 1.62893684e-07\n",
      " 1.73882547e-10 4.03685370e-08 3.51385115e-10 1.72567098e-08\n",
      " 7.32408534e-09 3.65999453e-09 3.54279912e-08 4.34236558e-09\n",
      " 2.44537944e-07 1.01708852e-09 5.35578124e-08 2.30636644e-07\n",
      " 1.82886684e-08 9.21917431e-10 2.85554891e-09 8.95281627e-09\n",
      " 1.98297867e-09 1.13237686e-08 4.97783681e-08 1.73950419e-08\n",
      " 1.26895436e-08 2.47941601e-09 4.41669945e-09 1.95655203e-09\n",
      " 9.18991361e-07 2.50702712e-08 1.77877624e-08 4.54378011e-08\n",
      " 4.17351975e-09 6.46170761e-09 2.51036539e-07 4.99170881e-07\n",
      " 6.10206854e-08 4.73792783e-10 1.43964172e-07 6.40746078e-09\n",
      " 2.76619416e-09]\n",
      "Max prob per class: [0.9996061  0.9988477  0.9995845  0.9998776  0.9099502  0.99861753\n",
      " 0.9769011  0.99398756 0.9664796  0.9811538  0.97924966 0.999047\n",
      " 0.99749064 0.9691236  0.9943236  0.99958414 0.9951455  0.9945825\n",
      " 0.9964122  0.9973858  0.9967069  0.99545056 0.9995049  0.9261008\n",
      " 0.9962612  0.97671604 0.9786266  0.9935958  0.99625456 0.99830556\n",
      " 0.852275   0.99917835 0.99870217 0.9999664  0.9988456  0.9999968\n",
      " 0.94095623 0.9998641  0.99632484 0.9983839  0.9174399  0.9798202\n",
      " 0.9275057  0.9969602  0.9819193  0.9997509  0.9997334  0.99479985\n",
      " 0.9995776  0.87452626 0.993451   0.9946785  0.9987943  0.9981523\n",
      " 0.98322564 0.99320674 0.9881683  0.9917401  0.8694267  0.99834824\n",
      " 0.9837531  0.99572104 0.9995486  0.9991304  0.9973579  0.99930453\n",
      " 0.97200024 0.99687624 0.90159464 0.9980434  0.99864525 0.99947673\n",
      " 0.7618337  0.9987857  0.9999194  0.99820566 0.9993844  0.99100834\n",
      " 0.9999188  0.9979656  0.99684334]\n",
      "Epoch 6: 100%|██████████| 640/640 [03:38<00:00,  2.92it/s, v_num=0, train_loss_step=0.0889, val_loss=0.125, val_ood_acc=0.921, val_acc_all=0.995, val_precision_all=0.914, val_recall_all=0.679, val_f1_all=0.779, avg_threshold=0.296, val_acc=0.995, val_precision=0.914, val_recall=0.679, val_f1=0.779, train_loss_epoch=0.141, train_acc=0.993, train_precision=0.905, train_recall=0.442, train_f1=0.594]Min prob per class: [3.56200083e-08 1.76006587e-08 2.78496266e-08 1.16989829e-09\n",
      " 5.40658129e-10 6.31808939e-10 1.80979443e-09 9.81665721e-11\n",
      " 1.18969357e-09 8.71989414e-08 4.96442709e-09 9.96142901e-09\n",
      " 2.19971508e-08 8.42686310e-10 1.08540760e-10 9.42241337e-12\n",
      " 2.25859775e-09 5.05537279e-10 8.61618066e-09 2.11279803e-06\n",
      " 2.19441080e-11 2.76367862e-09 5.54799595e-10 5.81431327e-11\n",
      " 6.25646877e-08 6.88146651e-10 6.60667396e-08 2.72703388e-10\n",
      " 6.70736383e-11 8.70901573e-10 2.11360929e-09 1.78763973e-08\n",
      " 2.39843878e-09 5.25187804e-09 2.03480241e-07 1.06475406e-08\n",
      " 1.92618188e-09 1.95949021e-10 2.65554401e-09 1.11144249e-09\n",
      " 4.89371352e-08 2.53030086e-09 4.35258746e-10 5.60181324e-09\n",
      " 5.56973134e-10 1.03293836e-08 3.46192985e-09 1.88096365e-08\n",
      " 1.32474824e-07 7.89402410e-09 1.52558766e-09 1.15368670e-09\n",
      " 4.61783278e-09 1.04795470e-08 2.96168167e-07 2.72748384e-08\n",
      " 9.46146606e-10 5.54793211e-10 3.72897979e-09 1.20243460e-09\n",
      " 3.54364038e-10 3.79778697e-09 2.73480683e-09 6.93036073e-10\n",
      " 8.63288538e-11 1.52950930e-09 6.45171028e-08 1.56906921e-09\n",
      " 3.31701472e-07 1.24292612e-07 4.00220657e-09 4.89435470e-09\n",
      " 4.30794822e-09 1.58218416e-09 1.01378525e-07 6.54230732e-08\n",
      " 7.90195909e-10 8.66433822e-08 1.92916886e-07 5.68072736e-08\n",
      " 1.92045913e-09]\n",
      "Max prob per class: [0.9994635  0.9998149  0.9997949  0.99996257 0.91052574 0.9988023\n",
      " 0.9948526  0.9850417  0.9043677  0.99770796 0.9831071  0.99924326\n",
      " 0.9988362  0.99097216 0.9550179  0.99876994 0.97739524 0.996774\n",
      " 0.8957334  0.9999975  0.9924671  0.9996309  0.9996996  0.8086117\n",
      " 0.99623704 0.98205596 0.9996525  0.9985885  0.998324   0.9995964\n",
      " 0.9997521  0.9995944  0.99955946 0.9998816  0.99993896 0.99999833\n",
      " 0.9740441  0.99903524 0.99795926 0.99890685 0.94951725 0.9738602\n",
      " 0.9232016  0.99806815 0.9894588  0.99995804 0.999969   0.99981993\n",
      " 0.9998685  0.9854137  0.9911816  0.99921906 0.9854154  0.99983203\n",
      " 0.99844956 0.9959869  0.99459994 0.99827754 0.96736264 0.99370885\n",
      " 0.9925298  0.995654   0.99672073 0.99782    0.9616604  0.9993337\n",
      " 0.98934853 0.9987393  0.807121   0.9999343  0.9982659  0.9996043\n",
      " 0.8882048  0.99995255 0.9992823  0.98864824 0.9986455  0.998657\n",
      " 0.9999747  0.9995697  0.9927961 ]\n",
      "Epoch 7: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.228, val_loss=0.125, val_ood_acc=0.918, val_acc_all=0.995, val_precision_all=0.899, val_recall_all=0.693, val_f1_all=0.783, avg_threshold=0.304, val_acc=0.995, val_precision=0.899, val_recall=0.693, val_f1=0.783, train_loss_epoch=0.142, train_acc=0.992, train_precision=0.890, train_recall=0.447, train_f1=0.595] Min prob per class: [2.13857394e-08 5.36966638e-10 4.11037315e-10 3.39381288e-08\n",
      " 8.18698220e-10 2.36889952e-10 2.54210177e-11 5.47791368e-10\n",
      " 6.14130924e-09 5.28380895e-09 8.28386318e-11 1.62132407e-09\n",
      " 3.23773480e-10 2.22301635e-10 5.71440228e-09 3.22313204e-10\n",
      " 1.11362752e-10 2.26348407e-09 1.05869340e-07 3.11569681e-09\n",
      " 1.44744483e-09 2.17011149e-11 6.11179718e-11 1.25025301e-09\n",
      " 1.26344257e-09 5.13057763e-10 5.39915632e-08 5.69218353e-11\n",
      " 2.90185120e-09 3.99735994e-10 4.50644944e-09 6.97769167e-08\n",
      " 1.25730821e-08 4.24580548e-09 2.94675164e-08 9.25708044e-09\n",
      " 1.64452807e-08 1.31432696e-08 3.37681307e-11 4.05585299e-09\n",
      " 6.41311837e-10 1.71911694e-08 2.96567464e-08 3.85991195e-09\n",
      " 2.60535586e-11 5.52034030e-10 1.63387515e-09 1.18534755e-08\n",
      " 3.32757866e-09 9.10188480e-10 1.44390361e-10 2.75447576e-08\n",
      " 3.42645805e-08 4.27003988e-09 4.65347405e-09 6.81032591e-11\n",
      " 2.46663973e-07 2.27096383e-10 3.66972314e-10 7.36484207e-11\n",
      " 5.33221023e-10 5.92064231e-10 8.02348032e-12 1.54001545e-09\n",
      " 4.31715080e-10 3.04526959e-09 1.83798221e-07 3.06483970e-11\n",
      " 4.70122030e-09 2.11167208e-08 3.28540979e-11 2.96775138e-08\n",
      " 1.40566581e-08 2.03902076e-10 1.33661446e-08 6.66131655e-07\n",
      " 9.15359610e-09 2.53928523e-08 3.16175374e-07 5.02159314e-10\n",
      " 3.44260696e-11]\n",
      "Max prob per class: [0.99621767 0.99969864 0.998781   0.9999323  0.9893282  0.9988337\n",
      " 0.9712179  0.98479456 0.9958747  0.9979411  0.99578947 0.9994512\n",
      " 0.99480116 0.9855139  0.99982435 0.99992836 0.9660889  0.99962544\n",
      " 0.9778561  0.9998597  0.9979547  0.99795157 0.9966928  0.9680874\n",
      " 0.9710517  0.96444184 0.99907076 0.98056877 0.99836284 0.99953556\n",
      " 0.99951196 0.99991333 0.99909186 0.99996257 0.9994892  0.99999845\n",
      " 0.99393517 0.9999043  0.9983109  0.9994777  0.8970952  0.99902177\n",
      " 0.9640981  0.99842864 0.9940158  0.999841   0.9998827  0.9997414\n",
      " 0.9995369  0.93918914 0.99438727 0.9997861  0.9993911  0.9994991\n",
      " 0.9998733  0.97056735 0.9990845  0.9897085  0.9664933  0.9941695\n",
      " 0.98695344 0.9965443  0.99291855 0.9998036  0.99888116 0.99956995\n",
      " 0.99938726 0.9943867  0.72144425 0.99959475 0.9947789  0.99982846\n",
      " 0.98592675 0.998767   0.99991405 0.99959594 0.99970907 0.9970456\n",
      " 0.99999917 0.9997032  0.9969932 ]\n",
      "Epoch 8: 100%|██████████| 640/640 [03:37<00:00,  2.95it/s, v_num=0, train_loss_step=0.193, val_loss=0.123, val_ood_acc=0.918, val_acc_all=0.995, val_precision_all=0.874, val_recall_all=0.719, val_f1_all=0.789, avg_threshold=0.313, val_acc=0.995, val_precision=0.874, val_recall=0.719, val_f1=0.789, train_loss_epoch=0.144, train_acc=0.993, train_precision=0.891, train_recall=0.450, train_f1=0.598] Min prob per class: [8.9210435e-09 9.3405994e-09 1.2683730e-09 2.4013214e-10 3.0410528e-11\n",
      " 2.6264097e-12 6.5222008e-11 3.4228276e-11 2.7583569e-11 1.7480110e-09\n",
      " 1.1973288e-09 4.8524336e-09 1.9413141e-10 2.2252222e-10 1.3766063e-09\n",
      " 6.6303713e-11 1.5813070e-08 1.1500260e-10 3.2936702e-09 6.0894877e-08\n",
      " 6.0893079e-10 3.3254434e-11 1.8216793e-10 1.9160272e-10 1.8509526e-10\n",
      " 5.8052212e-09 7.6762774e-09 7.4685964e-09 3.5162179e-10 1.5875377e-09\n",
      " 7.3538126e-10 1.2747280e-08 6.4946916e-11 1.7359435e-09 8.1439931e-11\n",
      " 2.4581150e-08 3.2101575e-09 2.3378871e-10 3.3912233e-09 3.3565042e-10\n",
      " 5.7852202e-11 2.9747471e-10 9.5577297e-09 1.4553373e-09 2.2334502e-10\n",
      " 5.0383731e-10 7.4232742e-10 1.2918560e-09 1.4227312e-09 1.3996049e-10\n",
      " 2.5956912e-09 5.8868750e-09 2.3359908e-08 5.0844631e-09 5.6061853e-09\n",
      " 2.4684075e-09 1.1840240e-07 1.0218377e-11 3.8511347e-11 6.2483805e-09\n",
      " 7.7614576e-10 1.9074403e-10 3.1033590e-10 3.3073698e-12 1.3866518e-09\n",
      " 4.4125550e-10 2.7433120e-09 2.5660463e-10 4.0363609e-08 2.8027705e-09\n",
      " 4.9402438e-10 6.6788097e-10 1.7292837e-10 7.2452450e-10 3.7006188e-08\n",
      " 9.2012433e-09 1.3657457e-08 3.2590781e-11 2.0614561e-07 4.8848889e-09\n",
      " 2.3621801e-11]\n",
      "Max prob per class: [0.9999064  0.99928445 0.95580804 0.9999964  0.9904546  0.9988475\n",
      " 0.99153566 0.9712494  0.90963423 0.9959739  0.9995809  0.99986315\n",
      " 0.9851977  0.9869398  0.9991999  0.9999243  0.9993647  0.99908483\n",
      " 0.9361711  0.99993336 0.9985474  0.9986743  0.9994117  0.9747704\n",
      " 0.9299821  0.9937256  0.998214   0.9978265  0.999423   0.9923529\n",
      " 0.9982344  0.9995914  0.9998229  0.99991727 0.99780947 0.9999994\n",
      " 0.9906487  0.9999697  0.9988728  0.9982089  0.81131846 0.9983027\n",
      " 0.9887523  0.9962035  0.99794334 0.99994195 0.99986804 0.9980635\n",
      " 0.99991465 0.99790764 0.98999923 0.99915385 0.9967001  0.9996729\n",
      " 0.99844736 0.9993051  0.9925011  0.95667034 0.9877773  0.9993374\n",
      " 0.99958235 0.9970663  0.9943786  0.9987564  0.9998839  0.9999273\n",
      " 0.9857006  0.99708635 0.8622945  0.9998684  0.9994578  0.99399066\n",
      " 0.6782457  0.99998987 0.9999778  0.99960893 0.9999635  0.98487455\n",
      " 0.99999154 0.9999912  0.99868065]\n",
      "Epoch 9: 100%|██████████| 640/640 [03:38<00:00,  2.92it/s, v_num=0, train_loss_step=0.159, val_loss=0.124, val_ood_acc=0.923, val_acc_all=0.995, val_precision_all=0.900, val_recall_all=0.707, val_f1_all=0.792, avg_threshold=0.315, val_acc=0.995, val_precision=0.900, val_recall=0.707, val_f1=0.792, train_loss_epoch=0.138, train_acc=0.993, train_precision=0.899, train_recall=0.449, train_f1=0.599] Min prob per class: [1.76838655e-09 1.05979302e-10 9.15123533e-10 8.23743462e-10\n",
      " 1.55359475e-10 1.96328544e-12 6.84069815e-11 6.32645825e-10\n",
      " 3.51253110e-10 1.66896219e-10 5.68937508e-10 5.56125235e-09\n",
      " 2.59449817e-09 2.46191290e-10 1.81050919e-09 9.61524263e-11\n",
      " 1.51025970e-10 7.14192983e-10 6.94816260e-10 9.94772087e-09\n",
      " 1.16438859e-09 1.77426102e-10 4.07188783e-10 4.70980947e-13\n",
      " 3.14708759e-10 9.23261911e-10 1.80005273e-08 6.22248919e-09\n",
      " 1.78918058e-09 9.75618364e-10 8.74060047e-10 6.43599023e-08\n",
      " 8.38480466e-11 6.83584966e-10 1.01857556e-09 2.10491158e-09\n",
      " 2.01077537e-08 2.66035360e-10 4.66005545e-09 3.23273808e-09\n",
      " 4.35380586e-11 9.08727468e-11 4.65940453e-10 3.10757797e-09\n",
      " 1.40939795e-11 6.29353861e-11 3.82951934e-11 3.12826720e-09\n",
      " 1.46143175e-09 6.47935261e-10 3.70988712e-10 5.77820902e-10\n",
      " 2.25727148e-09 1.61794456e-09 2.95161762e-08 3.62024655e-09\n",
      " 3.42037225e-08 1.05699401e-12 8.61771696e-11 1.48561019e-09\n",
      " 1.88294658e-10 4.41657333e-09 4.18503621e-10 2.99368273e-11\n",
      " 1.39512055e-08 9.77644563e-11 7.12506534e-08 4.71725783e-11\n",
      " 2.11709704e-07 9.58006918e-09 9.88482213e-11 5.94792049e-10\n",
      " 9.95151073e-10 2.64778921e-10 1.06738884e-09 2.73547962e-09\n",
      " 3.03096104e-10 3.62025160e-10 1.78876235e-07 1.09318277e-09\n",
      " 1.37816009e-11]\n",
      "Max prob per class: [0.9999651  0.9998369  0.9998839  0.9999801  0.9821618  0.99926895\n",
      " 0.99827063 0.9991572  0.97609985 0.99452037 0.9957516  0.9999471\n",
      " 0.9983872  0.98103344 0.9930326  0.9999932  0.958296   0.9998679\n",
      " 0.9128375  0.99996114 0.9996847  0.99988663 0.999905   0.96096593\n",
      " 0.9600798  0.9915993  0.99986005 0.9993868  0.9995944  0.99799013\n",
      " 0.99930966 0.9995865  0.9998971  0.9999411  0.9997583  0.9999993\n",
      " 0.9940759  0.9998975  0.99972445 0.9998405  0.80775094 0.9754512\n",
      " 0.9572071  0.998944   0.9989722  0.9997706  0.9997296  0.9998286\n",
      " 0.9999888  0.9966798  0.9977005  0.99949336 0.9863412  0.99892575\n",
      " 0.99950194 0.96652883 0.99972695 0.9926254  0.97974646 0.9996232\n",
      " 0.9982217  0.99974376 0.9993211  0.99987566 0.9999399  0.998648\n",
      " 0.9991518  0.9993998  0.9094748  0.9999745  0.99992144 0.998744\n",
      " 0.9755898  0.99643075 0.99996257 0.9967796  0.9996854  0.9972761\n",
      " 0.9999908  0.99996674 0.9999254 ]\n",
      "Epoch 10: 100%|██████████| 640/640 [03:35<00:00,  2.96it/s, v_num=0, train_loss_step=0.0661, val_loss=0.120, val_ood_acc=0.924, val_acc_all=0.996, val_precision_all=0.897, val_recall_all=0.722, val_f1_all=0.800, avg_threshold=0.326, val_acc=0.996, val_precision=0.897, val_recall=0.722, val_f1=0.800, train_loss_epoch=0.139, train_acc=0.993, train_precision=0.898, train_recall=0.451, train_f1=0.601]Min prob per class: [1.18130759e-08 7.76672615e-11 2.38099429e-10 1.42127130e-10\n",
      " 1.86835408e-10 1.82986807e-12 1.02634450e-10 1.63577651e-10\n",
      " 1.92224103e-08 4.35738057e-10 2.71619477e-10 2.22246643e-09\n",
      " 2.07546494e-10 5.85333892e-10 2.20069185e-09 1.71139006e-11\n",
      " 9.62151747e-11 1.70090914e-10 1.08821752e-08 1.37991449e-07\n",
      " 2.02698192e-09 9.74856567e-12 5.08500082e-11 5.65908485e-13\n",
      " 9.74717668e-11 6.07836559e-10 2.51724619e-09 4.90891598e-11\n",
      " 7.51291196e-10 4.26144564e-10 2.03969383e-10 2.87282980e-08\n",
      " 3.38567438e-11 8.02174022e-11 2.22423480e-09 2.37768250e-09\n",
      " 8.12167139e-11 1.91397467e-10 6.72940453e-11 3.37666034e-10\n",
      " 3.09888870e-09 3.95192712e-10 3.69699549e-10 7.25282590e-09\n",
      " 2.18075020e-11 2.73274441e-11 4.51915796e-11 2.23083707e-09\n",
      " 5.06668651e-10 2.70919426e-10 5.66395758e-12 1.64515387e-10\n",
      " 1.31391276e-09 2.63793032e-09 1.69595129e-08 1.17272891e-09\n",
      " 5.00972153e-10 3.72201409e-10 1.54835866e-10 3.12095932e-10\n",
      " 5.47595719e-11 1.68006081e-10 5.11064845e-12 1.64093572e-10\n",
      " 8.62133753e-10 1.46240069e-11 1.45250105e-08 4.74411066e-11\n",
      " 1.96326577e-09 9.84792869e-09 1.58612026e-10 3.01594444e-10\n",
      " 6.82272328e-09 6.52331704e-12 7.52336582e-10 1.15044392e-08\n",
      " 5.99535088e-09 1.38354495e-09 7.76469847e-07 6.79565085e-11\n",
      " 3.59631352e-11]\n",
      "Max prob per class: [0.9999733  0.99811804 0.9997974  0.9999958  0.99268603 0.9900199\n",
      " 0.99633753 0.9919899  0.998418   0.99736434 0.9980343  0.9998248\n",
      " 0.9996444  0.98995787 0.994609   0.99988604 0.99738485 0.98842233\n",
      " 0.92497474 0.9999745  0.99860686 0.9996519  0.999561   0.98123324\n",
      " 0.9881542  0.99183005 0.9971986  0.9980198  0.99987745 0.99833775\n",
      " 0.99937135 0.9999918  0.9999193  0.9999275  0.9979159  0.99999905\n",
      " 0.9845739  0.9999597  0.98557997 0.999884   0.9928854  0.9524488\n",
      " 0.99419963 0.9995105  0.9990181  0.99990344 0.99996185 0.9995389\n",
      " 0.99997735 0.9979365  0.98779535 0.98752415 0.9928976  0.9993262\n",
      " 0.99943453 0.99803394 0.9896716  0.99945873 0.9927619  0.9990037\n",
      " 0.9990349  0.99952424 0.9998024  0.9987627  0.9998785  0.99959594\n",
      " 0.99795294 0.9987883  0.79547465 0.9994504  0.9991704  0.9998567\n",
      " 0.99659544 0.99892116 0.99991703 0.99881184 0.99998164 0.9996227\n",
      " 0.9999871  0.99990964 0.99919134]\n",
      "Epoch 11: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.156, val_loss=0.120, val_ood_acc=0.924, val_acc_all=0.996, val_precision_all=0.901, val_recall_all=0.727, val_f1_all=0.805, avg_threshold=0.333, val_acc=0.996, val_precision=0.901, val_recall=0.727, val_f1=0.805, train_loss_epoch=0.132, train_acc=0.993, train_precision=0.901, train_recall=0.456, train_f1=0.605] Min prob per class: [1.31410327e-09 9.30261285e-11 1.21769664e-10 4.22339025e-10\n",
      " 1.26490304e-10 4.42593336e-13 1.11261479e-10 7.96318636e-11\n",
      " 7.87498178e-10 2.60585772e-11 6.51649679e-10 8.72512618e-10\n",
      " 5.88156052e-11 4.64998484e-10 2.30324559e-09 3.81327782e-12\n",
      " 8.20860324e-11 3.88437993e-09 1.23774706e-08 6.93458180e-09\n",
      " 2.20692867e-10 1.52106008e-10 1.48691268e-11 1.03270489e-10\n",
      " 7.62815255e-10 2.06642059e-09 1.06429876e-09 1.38988418e-10\n",
      " 8.78303208e-10 6.92177999e-12 9.53128618e-11 2.67366360e-08\n",
      " 2.21155004e-11 2.11080028e-10 2.35210157e-10 7.01752545e-10\n",
      " 5.77863091e-09 3.14031773e-10 2.72388362e-10 6.87376545e-10\n",
      " 1.07484664e-11 1.13962562e-10 2.78099632e-09 5.54123014e-09\n",
      " 4.62122701e-12 5.42220602e-10 3.78949788e-11 4.86950924e-10\n",
      " 8.58294946e-09 1.00114095e-08 2.86983687e-10 1.84052149e-10\n",
      " 4.43902115e-09 2.37193443e-09 1.33051799e-07 3.97604794e-09\n",
      " 5.84990456e-09 1.15977263e-11 6.09110540e-11 7.37254757e-10\n",
      " 6.61087071e-12 3.98622857e-10 4.38470954e-10 3.83970189e-10\n",
      " 7.50005127e-11 8.34224426e-11 7.32310701e-09 3.40320688e-11\n",
      " 5.44256196e-08 2.62125899e-09 4.73033390e-10 5.90505644e-10\n",
      " 1.34410438e-09 3.22666568e-12 3.37526473e-09 1.15206227e-08\n",
      " 1.51034985e-09 1.22535782e-09 1.46641440e-07 1.14820742e-09\n",
      " 1.71563794e-12]\n",
      "Max prob per class: [0.99998057 0.9998752  0.99862576 0.9999857  0.950276   0.9704889\n",
      " 0.99939764 0.9935247  0.9670584  0.9974541  0.99900573 0.99994123\n",
      " 0.99728596 0.9964529  0.9897864  0.9988998  0.99938285 0.99940777\n",
      " 0.84773165 0.9999572  0.9980318  0.9999516  0.9995196  0.98386246\n",
      " 0.9659395  0.9821868  0.999713   0.99699867 0.99965465 0.9979766\n",
      " 0.9980045  0.99998474 0.9998965  0.99974364 0.9991528  0.999998\n",
      " 0.9939819  0.9999616  0.9948368  0.9998524  0.9417127  0.99797016\n",
      " 0.9983626  0.99962234 0.99888355 0.999946   0.99946445 0.9998048\n",
      " 0.9999975  0.99882656 0.99987066 0.9977737  0.996988   0.9996847\n",
      " 0.9994062  0.99927527 0.9985197  0.99845207 0.99268556 0.99978405\n",
      " 0.9984831  0.9993955  0.99987686 0.9998211  0.9995474  0.99864\n",
      " 0.9986457  0.9993799  0.8160557  0.9999765  0.9999049  0.99511176\n",
      " 0.9502608  0.999833   0.9997186  0.98330194 0.9995264  0.9909225\n",
      " 0.9999869  0.9999764  0.99945885]\n",
      "Epoch 12: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.157, val_loss=0.129, val_ood_acc=0.913, val_acc_all=0.995, val_precision_all=0.883, val_recall_all=0.717, val_f1_all=0.792, avg_threshold=0.330, val_acc=0.995, val_precision=0.883, val_recall=0.717, val_f1=0.792, train_loss_epoch=0.131, train_acc=0.993, train_precision=0.901, train_recall=0.457, train_f1=0.606] Min prob per class: [2.30750219e-09 2.68485878e-09 6.75583270e-11 1.17150156e-09\n",
      " 3.81903710e-11 5.00965172e-11 8.75747445e-13 1.75385019e-11\n",
      " 5.04202624e-10 1.55262608e-10 2.14949863e-10 1.00127606e-09\n",
      " 4.71112342e-12 2.34899566e-09 2.44390197e-09 2.45531243e-12\n",
      " 2.47973048e-11 3.05902592e-09 1.46732591e-08 2.43970231e-08\n",
      " 4.50505258e-11 4.74595675e-11 1.09528124e-11 1.31427239e-11\n",
      " 4.11607692e-10 1.45056778e-09 6.55866472e-11 3.93999805e-10\n",
      " 8.06451086e-11 3.20084431e-10 2.88127244e-10 1.42291361e-08\n",
      " 3.20787286e-10 1.20352839e-10 1.62239444e-09 9.07577277e-11\n",
      " 7.22253091e-11 1.54621524e-11 2.02263501e-11 8.23732055e-11\n",
      " 3.42849416e-10 9.03837213e-11 3.61784963e-10 1.53221602e-09\n",
      " 7.74174214e-12 4.91233720e-10 7.47745477e-11 6.99323432e-10\n",
      " 5.83144255e-10 9.52261811e-11 6.83592488e-11 2.18772223e-09\n",
      " 2.45438567e-08 8.94712748e-10 2.06799875e-08 4.65728123e-10\n",
      " 8.52120952e-09 1.34034997e-11 3.42692992e-11 8.72264705e-10\n",
      " 2.32857289e-10 2.27617109e-11 1.92099461e-12 7.01128738e-11\n",
      " 4.05091249e-09 2.59077843e-10 1.14694716e-08 3.41046531e-11\n",
      " 2.14735767e-08 4.69649741e-09 1.96983940e-11 6.99676983e-10\n",
      " 3.62989722e-10 5.72920314e-12 3.18466392e-10 5.27344879e-09\n",
      " 1.49384183e-09 3.99981576e-10 1.63229424e-06 3.53747476e-10\n",
      " 1.65337492e-12]\n",
      "Max prob per class: [0.9999887  0.9998764  0.9987381  0.9999466  0.9744204  0.99961615\n",
      " 0.99729186 0.9970649  0.992363   0.9994091  0.99444133 0.9994473\n",
      " 0.96224505 0.99628544 0.9998379  0.9999765  0.9828808  0.9997347\n",
      " 0.8703374  0.9999951  0.997615   0.9998547  0.999874   0.99410963\n",
      " 0.99192315 0.99371886 0.99865484 0.99838805 0.9997441  0.9959728\n",
      " 0.9999614  0.99987555 0.9983923  0.99967    0.9998492  0.9999831\n",
      " 0.9751754  0.9997826  0.9985833  0.9998079  0.9935981  0.99964917\n",
      " 0.9752914  0.9940976  0.996852   0.99988234 0.99998176 0.99930453\n",
      " 0.9995504  0.9975574  0.99879825 0.9983594  0.9988846  0.99753004\n",
      " 0.9999064  0.995523   0.9931114  0.9975998  0.96805733 0.9991135\n",
      " 0.9994479  0.9985876  0.9992267  0.99807405 0.99998665 0.9998312\n",
      " 0.99895716 0.9999033  0.8316417  0.99996996 0.9946156  0.99989986\n",
      " 0.98415345 0.9999745  0.9999584  0.99433184 0.9998253  0.9989479\n",
      " 0.999992   0.9993506  0.99218386]\n",
      "Epoch 13: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.0709, val_loss=0.126, val_ood_acc=0.918, val_acc_all=0.996, val_precision_all=0.897, val_recall_all=0.728, val_f1_all=0.803, avg_threshold=0.338, val_acc=0.996, val_precision=0.897, val_recall=0.728, val_f1=0.803, train_loss_epoch=0.129, train_acc=0.993, train_precision=0.905, train_recall=0.457, train_f1=0.608]Min prob per class: [2.23476815e-09 1.64290004e-09 6.93299507e-09 4.00019712e-10\n",
      " 4.78756999e-11 8.70289192e-13 2.29150546e-10 1.52606414e-10\n",
      " 2.95489855e-10 2.73379874e-10 1.96539590e-11 1.82925319e-09\n",
      " 1.07157764e-11 8.12647893e-10 1.74103579e-10 2.38548742e-12\n",
      " 5.17820682e-11 7.61375823e-11 2.30833965e-08 3.57353391e-09\n",
      " 2.65454492e-10 6.22028384e-12 7.04903020e-12 2.74767136e-11\n",
      " 1.49956974e-11 7.79983467e-10 2.71227218e-09 4.91481821e-11\n",
      " 7.07159331e-10 1.47438502e-11 1.29122213e-10 7.84669307e-09\n",
      " 1.07526966e-10 5.94481298e-10 4.56998306e-09 5.23635446e-09\n",
      " 3.89389493e-10 4.54882867e-11 1.82487081e-11 4.10968343e-10\n",
      " 7.15706314e-11 6.06304673e-10 2.67702360e-10 1.10233056e-09\n",
      " 1.92112277e-12 3.92730320e-10 1.59611088e-10 7.67597721e-11\n",
      " 8.82695339e-09 2.03430120e-10 4.36770446e-11 7.83397736e-10\n",
      " 8.09469292e-09 2.42557707e-09 9.53282342e-09 2.77596679e-09\n",
      " 3.67640873e-09 1.30954991e-12 5.86429863e-11 4.03688027e-10\n",
      " 1.26355454e-11 3.86628340e-10 1.98558045e-11 1.25962088e-10\n",
      " 8.94885110e-10 3.88068813e-11 1.56780153e-08 2.03410205e-11\n",
      " 1.39127820e-08 2.47613152e-09 2.20668026e-10 5.56434787e-10\n",
      " 4.07243156e-10 3.99224709e-10 1.33930866e-09 4.70778883e-09\n",
      " 1.15567544e-09 2.24456096e-11 8.37290827e-07 9.29700761e-11\n",
      " 1.21650343e-10]\n",
      "Max prob per class: [0.9999832  0.9999907  0.9933135  0.99999464 0.9853194  0.99693704\n",
      " 0.9998     0.99982315 0.98963445 0.99884737 0.99589986 0.9997992\n",
      " 0.9970981  0.99837756 0.9998074  0.99998224 0.9744143  0.998601\n",
      " 0.9941619  0.99995255 0.99729365 0.9997663  0.99992394 0.96639556\n",
      " 0.9863473  0.99803895 0.9987166  0.9959907  0.9998989  0.9988445\n",
      " 0.9999715  0.99998283 0.99711406 0.9996965  0.9991078  1.\n",
      " 0.9949367  0.9998524  0.9975713  0.9986008  0.9605966  0.9994985\n",
      " 0.99770015 0.9978148  0.99815625 0.9999504  0.9999889  0.99323225\n",
      " 0.9999888  0.99834025 0.9992939  0.9893561  0.99915075 0.99970835\n",
      " 0.9997584  0.998716   0.99827385 0.9912928  0.9931479  0.9965587\n",
      " 0.9699536  0.9989115  0.99863476 0.9995321  0.9999479  0.9999223\n",
      " 0.9977647  0.9989899  0.8814856  0.99992764 0.999956   0.99817157\n",
      " 0.98512715 0.9995697  0.9998031  0.99765116 0.99922955 0.99678195\n",
      " 0.9999856  0.99771094 0.9995559 ]\n",
      "Epoch 13: 100%|██████████| 640/640 [04:14<00:00,  2.51it/s, v_num=0, train_loss_step=0.0709, val_loss=0.124, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.888, val_recall_all=0.730, val_f1_all=0.802, avg_threshold=0.340, val_acc=0.996, val_precision=0.888, val_recall=0.730, val_f1=0.802, train_loss_epoch=0.128, train_acc=0.993, train_precision=0.907, train_recall=0.457, train_f1=0.607]\n",
      "Fold 4/5\n",
      "Setup ran successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name              | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | feature_extractor | Sequential          | 65.3 M | train\n",
      "1  | ood_classifier    | Sequential          | 2.0 K  | train\n",
      "2  | classifier        | Sequential          | 165 K  | train\n",
      "3  | loss_fn_class     | BCEWithLogitsLoss   | 0      | train\n",
      "4  | loss_fn_ood       | BCEWithLogitsLoss   | 0      | train\n",
      "5  | train_acc         | MultilabelAccuracy  | 0      | train\n",
      "6  | val_acc           | MultilabelAccuracy  | 0      | train\n",
      "7  | train_precision   | MultilabelPrecision | 0      | train\n",
      "8  | val_precision     | MultilabelPrecision | 0      | train\n",
      "9  | train_recall      | MultilabelRecall    | 0      | train\n",
      "10 | val_recall        | MultilabelRecall    | 0      | train\n",
      "11 | train_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1            | MultilabelF1Score   | 0      | train\n",
      "13 | ood_acc           | BinaryAccuracy      | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "168 K     Trainable params\n",
      "65.3 M    Non-trainable params\n",
      "65.5 M    Total params\n",
      "261.938   Total estimated model params size (MB)\n",
      "958       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.37it/s]Min prob per class: [0.4678335  0.4714891  0.48130092 0.45971453 0.4431855  0.46131808\n",
      " 0.49393857 0.43141067 0.44180566 0.46305567 0.4854305  0.46843883\n",
      " 0.46439016 0.46720403 0.46887144 0.46178988 0.47247827 0.47419307\n",
      " 0.45162293 0.46546662 0.4389176  0.47801197 0.4648905  0.50182813\n",
      " 0.46129394 0.4482804  0.46469843 0.46852702 0.44140053 0.46729872\n",
      " 0.43815544 0.4349673  0.46245652 0.44703943 0.4723736  0.4833778\n",
      " 0.46955723 0.47590187 0.47526458 0.44504836 0.4640035  0.47311014\n",
      " 0.45330095 0.45907933 0.46690446 0.47837627 0.4697544  0.47222304\n",
      " 0.44341025 0.4809055  0.47335762 0.46486098 0.47526646 0.45559162\n",
      " 0.4569073  0.47037756 0.47165275 0.4549268  0.47098127 0.4833256\n",
      " 0.46606216 0.48434934 0.4822559  0.4699415  0.48763174 0.46572247\n",
      " 0.48165    0.4676942  0.46082765 0.49915195 0.4706629  0.46677652\n",
      " 0.48013833 0.4696506  0.4570015  0.46766582 0.4943096  0.46929958\n",
      " 0.4616705  0.4721887  0.46169204]\n",
      "Max prob per class: [0.552344   0.5322291  0.55117166 0.5156192  0.52407163 0.53376746\n",
      " 0.5569395  0.50870866 0.5066284  0.53386    0.5528689  0.5115992\n",
      " 0.5228372  0.5355716  0.5268554  0.5140305  0.52339417 0.55028236\n",
      " 0.542227   0.52302843 0.52031124 0.5355739  0.53786546 0.5580974\n",
      " 0.53643286 0.5341749  0.5214914  0.5522518  0.53051084 0.5403707\n",
      " 0.51947266 0.5111218  0.5452403  0.5276312  0.5240683  0.542177\n",
      " 0.5265829  0.5397537  0.5279142  0.52026784 0.56259334 0.5268267\n",
      " 0.52548975 0.52641714 0.5297858  0.53562695 0.55032146 0.532472\n",
      " 0.5028906  0.5588676  0.5472392  0.5223406  0.5436571  0.51975334\n",
      " 0.55692786 0.526943   0.5504092  0.52363926 0.5284033  0.54254085\n",
      " 0.5362844  0.53423136 0.5492423  0.5230275  0.5468716  0.55117303\n",
      " 0.5469442  0.548869   0.53413993 0.5719667  0.5255285  0.5459234\n",
      " 0.54281265 0.5504847  0.5196355  0.53563523 0.5844257  0.53472096\n",
      " 0.52804863 0.5586503  0.52814364]\n",
      "Warning: No positive samples for class 2, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 3, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 4, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 5, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 6, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 7, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 8, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 9, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 10, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 11, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 12, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 13, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 14, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 15, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 16, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 17, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 18, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 19, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 20, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 21, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 22, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 23, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 24, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 25, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 26, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 27, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 28, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 29, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 30, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 31, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 32, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 33, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 34, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 35, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 36, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 37, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 38, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 39, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 40, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 41, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 42, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 43, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 44, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 45, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 46, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 47, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 48, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 49, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 50, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 51, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 52, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 53, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 54, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 55, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 56, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 57, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 58, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 59, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 60, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 61, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 62, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 63, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 64, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 65, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 66, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 67, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 68, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 69, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 70, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 71, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 72, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 73, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 74, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 75, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 76, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 77, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 78, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 79, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 80, keeping threshold at 0.5\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.270] Min prob per class: [2.01454604e-05 4.85057935e-05 2.97798724e-05 2.73255810e-05\n",
      " 1.36969429e-05 1.56081114e-05 9.73905026e-06 3.34674114e-05\n",
      " 8.04336014e-06 7.37812115e-06 1.22539332e-05 1.65183119e-05\n",
      " 2.92249861e-05 5.63174617e-05 3.53743962e-05 6.34202079e-05\n",
      " 4.37630733e-05 2.19310568e-05 8.69343567e-05 2.66103798e-05\n",
      " 5.95475285e-06 2.42710576e-05 4.79613336e-05 3.14246790e-05\n",
      " 2.82530109e-05 2.90088992e-05 1.07342175e-05 3.93586852e-05\n",
      " 2.06667173e-05 4.09187269e-05 1.29241862e-05 4.36410883e-05\n",
      " 4.00203899e-05 2.34778072e-05 1.78075334e-05 9.42917177e-05\n",
      " 2.18709283e-05 1.60243253e-05 7.60532930e-05 1.75620298e-05\n",
      " 2.32161310e-05 6.28027192e-05 2.07615976e-05 1.30719918e-05\n",
      " 2.95271457e-05 1.15785751e-05 2.69377870e-05 1.95107150e-05\n",
      " 1.07374317e-05 2.41733469e-05 8.46149305e-06 1.17971695e-05\n",
      " 1.49136276e-05 4.48670253e-05 2.48833476e-05 1.85338704e-05\n",
      " 1.14025906e-05 3.70971647e-05 8.09420726e-06 1.69827017e-05\n",
      " 5.03246119e-05 2.68159638e-05 2.11795941e-05 2.47727967e-05\n",
      " 1.86410780e-05 5.30986690e-05 4.49572217e-05 3.39999388e-05\n",
      " 1.15340627e-05 1.78918453e-05 9.87143358e-05 1.93294982e-05\n",
      " 1.07384158e-05 3.62472783e-05 2.08226484e-05 7.05062212e-06\n",
      " 2.80037311e-05 1.38501910e-05 1.66190008e-03 1.20816403e-05\n",
      " 1.19772685e-05]\n",
      "Max prob per class: [0.35009387 0.37980857 0.36891422 0.3606177  0.3768566  0.36471933\n",
      " 0.37292778 0.37407973 0.36939406 0.36834058 0.36577657 0.3739026\n",
      " 0.3620794  0.3683891  0.37549093 0.39207652 0.36417297 0.3769313\n",
      " 0.38387516 0.35717037 0.35963905 0.37387922 0.3776383  0.38135746\n",
      " 0.36073214 0.37017256 0.35671288 0.3724825  0.36525768 0.38199997\n",
      " 0.367126   0.37084955 0.385396   0.3754314  0.36340904 0.43852326\n",
      " 0.37589014 0.36533582 0.36393526 0.34960824 0.38508952 0.3667833\n",
      " 0.37456605 0.36107638 0.38263604 0.3536348  0.36653224 0.37457585\n",
      " 0.35851794 0.37009013 0.3588824  0.3501665  0.3628961  0.38622195\n",
      " 0.37743753 0.3746251  0.37629235 0.34950167 0.36863008 0.3691433\n",
      " 0.36489487 0.3865364  0.37997413 0.36172244 0.36515892 0.36931494\n",
      " 0.37185043 0.36820543 0.37621483 0.3648304  0.36573073 0.38095313\n",
      " 0.3634277  0.37443078 0.3691073  0.35182747 0.3644106  0.37259433\n",
      " 0.9989213  0.3616979  0.35847425]\n",
      "Epoch 1: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.148, val_loss=0.176, val_ood_acc=0.902, val_acc_all=0.993, val_precision_all=0.881, val_recall_all=0.468, val_f1_all=0.611, avg_threshold=0.372, val_acc=0.993, val_precision=0.881, val_recall=0.468, val_f1=0.611, train_loss_epoch=0.313, train_acc=0.988, train_precision=0.521, train_recall=0.429, train_f1=0.471] Min prob per class: [6.77168209e-05 2.02833526e-05 9.27508154e-07 1.31078559e-05\n",
      " 3.23860195e-06 7.44462341e-06 9.21705293e-07 5.20364893e-06\n",
      " 3.43441570e-06 3.04676701e-06 1.97002078e-06 5.37122105e-06\n",
      " 5.17310127e-07 3.91519461e-06 4.17330193e-06 1.83648217e-06\n",
      " 1.61794014e-05 1.69333453e-05 4.09442546e-05 1.32245759e-05\n",
      " 7.87669251e-06 4.83404847e-06 1.24777705e-06 1.07983025e-04\n",
      " 1.06491041e-06 1.00086900e-05 3.66139375e-06 1.61514527e-05\n",
      " 2.15635446e-05 4.12487452e-06 1.58961154e-06 1.69167033e-05\n",
      " 1.54530721e-06 2.32747971e-06 2.11837641e-06 1.22741085e-05\n",
      " 8.01330043e-06 4.83381791e-06 1.69750256e-05 3.66123345e-06\n",
      " 5.83584870e-06 4.31446097e-05 2.29089947e-05 5.08759786e-06\n",
      " 5.20184267e-06 4.64758750e-06 4.32827574e-06 2.31183240e-05\n",
      " 1.79348626e-05 2.02326123e-06 3.36173730e-06 9.46145883e-06\n",
      " 1.13038805e-05 3.28325245e-06 2.71375393e-06 6.74661669e-06\n",
      " 1.56825045e-05 3.54783833e-06 6.43377462e-07 9.90167609e-06\n",
      " 7.43849841e-06 5.58909596e-06 3.90184277e-06 9.41305734e-06\n",
      " 2.47988228e-05 1.43480502e-05 3.82727603e-06 5.64045695e-06\n",
      " 8.56465522e-06 1.84951659e-06 1.66353275e-05 1.00136640e-05\n",
      " 1.53003748e-06 5.73870284e-06 3.83306951e-06 1.39764934e-05\n",
      " 1.42098361e-05 6.02507180e-06 3.82108883e-05 5.19077184e-06\n",
      " 2.28215981e-06]\n",
      "Max prob per class: [0.98602    0.88953954 0.6753471  0.9464493  0.5815925  0.9529435\n",
      " 0.6297126  0.8209319  0.9006375  0.86076427 0.6619369  0.9570384\n",
      " 0.43055698 0.86007065 0.72035986 0.96205556 0.45471448 0.6077773\n",
      " 0.5954051  0.9902794  0.91995424 0.79864854 0.8660827  0.66291183\n",
      " 0.5890098  0.6148311  0.9205384  0.91485196 0.9427504  0.9579907\n",
      " 0.343637   0.8681307  0.83934164 0.98601556 0.6297848  0.98200214\n",
      " 0.68460786 0.9251054  0.6815372  0.8469971  0.5245592  0.94636387\n",
      " 0.6399749  0.85377914 0.7796702  0.9846865  0.9674229  0.8683374\n",
      " 0.99102134 0.35407528 0.60889757 0.8547581  0.841097   0.91813487\n",
      " 0.88392943 0.6496211  0.53447384 0.8591172  0.4277646  0.8932303\n",
      " 0.95785457 0.64376634 0.46028998 0.68145376 0.9657936  0.966398\n",
      " 0.5838876  0.49062344 0.42595708 0.9630716  0.932428   0.8666326\n",
      " 0.35194692 0.93285906 0.89163476 0.5656305  0.5008053  0.88896483\n",
      " 0.99997747 0.517276   0.8282997 ]\n",
      "Epoch 2: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.0962, val_loss=0.144, val_ood_acc=0.910, val_acc_all=0.994, val_precision_all=0.896, val_recall_all=0.571, val_f1_all=0.697, avg_threshold=0.323, val_acc=0.994, val_precision=0.896, val_recall=0.571, val_f1=0.697, train_loss_epoch=0.168, train_acc=0.992, train_precision=0.906, train_recall=0.435, train_f1=0.587]Min prob per class: [1.43407590e-06 1.89689587e-07 3.09023818e-07 2.74482147e-07\n",
      " 1.23981438e-08 2.25874814e-07 1.10837604e-07 1.21565279e-07\n",
      " 6.95620372e-07 9.07171795e-08 7.94462878e-08 3.66606514e-07\n",
      " 1.55208500e-06 9.79945639e-07 1.70039561e-06 2.43583008e-06\n",
      " 3.41378041e-07 1.99468275e-07 8.10578797e-07 3.17309195e-06\n",
      " 2.37173737e-07 1.62441324e-07 7.51499613e-07 2.30839532e-07\n",
      " 1.32373077e-06 9.00031182e-07 5.25414009e-07 1.92875135e-07\n",
      " 8.05015873e-07 3.95420471e-07 3.05299430e-08 1.23230511e-05\n",
      " 7.29407290e-08 2.11883329e-08 2.14706148e-07 2.62178855e-06\n",
      " 1.59523211e-07 1.65266172e-07 1.02388003e-06 1.37806822e-07\n",
      " 1.53752808e-06 8.15878877e-07 6.07208449e-07 1.73485569e-06\n",
      " 1.59688352e-06 2.85820192e-07 1.72591669e-07 6.60916939e-08\n",
      " 1.13591136e-06 1.01004282e-07 4.83127813e-08 6.20454870e-08\n",
      " 2.12976108e-08 7.49082517e-07 1.32498496e-06 2.38545592e-08\n",
      " 1.76257416e-07 6.17855633e-08 1.91668590e-07 1.28090846e-06\n",
      " 1.21009833e-07 9.39747167e-07 8.26014599e-08 7.01486840e-07\n",
      " 1.68002080e-05 7.94098355e-07 1.15610692e-06 5.63218840e-08\n",
      " 5.00230635e-06 2.41678458e-06 9.38084597e-08 4.68942471e-06\n",
      " 3.66389372e-08 3.65874428e-07 4.04506545e-06 3.83058796e-06\n",
      " 8.59912404e-07 8.04624506e-07 1.54196846e-06 3.19047581e-06\n",
      " 2.04470567e-07]\n",
      "Max prob per class: [0.9768246  0.9948684  0.99082714 0.9991911  0.9139105  0.9897592\n",
      " 0.9548146  0.9336236  0.8690724  0.9544     0.8943246  0.9739217\n",
      " 0.9734298  0.9896941  0.982917   0.9999248  0.7283715  0.95953864\n",
      " 0.5116905  0.9996561  0.94198877 0.86431396 0.97892433 0.6497986\n",
      " 0.9516123  0.9496846  0.9090895  0.9581028  0.9804528  0.9686396\n",
      " 0.73720133 0.99746585 0.9848163  0.9915258  0.9209983  0.9993057\n",
      " 0.56854564 0.9927817  0.99079597 0.9839011  0.7654743  0.9864607\n",
      " 0.97664285 0.973042   0.9454341  0.9967079  0.98793375 0.91176575\n",
      " 0.999321   0.89804965 0.8618534  0.8384188  0.9629338  0.9526632\n",
      " 0.9810839  0.74281466 0.8218242  0.96902007 0.8650958  0.9862722\n",
      " 0.9665943  0.9086825  0.9424786  0.98424697 0.99673176 0.9986749\n",
      " 0.8581424  0.9829768  0.8529635  0.9981358  0.98912114 0.99331397\n",
      " 0.73874885 0.9868356  0.9969318  0.9810898  0.99626595 0.933775\n",
      " 0.9999757  0.99124205 0.99905473]\n",
      "Epoch 3: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.110, val_loss=0.135, val_ood_acc=0.912, val_acc_all=0.995, val_precision_all=0.909, val_recall_all=0.629, val_f1_all=0.744, avg_threshold=0.313, val_acc=0.995, val_precision=0.909, val_recall=0.629, val_f1=0.744, train_loss_epoch=0.154, train_acc=0.993, train_precision=0.911, train_recall=0.441, train_f1=0.594] Min prob per class: [4.79030859e-07 1.16780757e-06 7.61504623e-07 1.19007291e-06\n",
      " 1.09999132e-06 1.34504115e-08 2.74435994e-08 2.38168574e-09\n",
      " 3.46057249e-07 1.17536514e-08 2.15763372e-08 4.11961523e-08\n",
      " 5.39400986e-08 2.06652384e-08 9.74520091e-08 4.14823660e-08\n",
      " 3.05785477e-08 2.47385259e-08 1.28405503e-07 9.90459171e-07\n",
      " 4.25991198e-09 3.16134887e-08 1.71127841e-08 5.66509755e-07\n",
      " 5.87543347e-09 1.73600387e-08 9.76632109e-08 8.04665348e-08\n",
      " 3.58959312e-07 5.44264473e-08 1.07703436e-07 1.16481885e-06\n",
      " 1.10372014e-07 4.27683950e-08 1.01843469e-07 2.47204213e-07\n",
      " 2.37094554e-07 7.48699875e-08 1.72719592e-06 9.11212368e-08\n",
      " 2.81007914e-07 9.91938052e-08 2.62557904e-07 1.06491245e-06\n",
      " 8.45739805e-07 1.33584905e-07 4.70442458e-08 1.16614665e-07\n",
      " 2.93905686e-07 2.42632670e-09 1.65068172e-07 4.97458586e-06\n",
      " 2.85343269e-08 3.72815009e-07 1.00846967e-06 3.02258574e-09\n",
      " 3.35216828e-06 5.11369080e-08 6.03544166e-08 2.00482688e-07\n",
      " 3.15448823e-08 1.26675189e-07 7.03491088e-09 1.12288163e-07\n",
      " 2.71245767e-06 1.41751954e-07 2.28186309e-07 4.85188082e-07\n",
      " 1.77495701e-06 8.14509860e-07 2.32647093e-08 2.99866478e-08\n",
      " 2.40092994e-08 1.14254192e-06 2.14828816e-07 1.94792065e-07\n",
      " 2.08045165e-07 4.08734557e-08 1.00401521e-05 6.43548049e-07\n",
      " 1.15180514e-08]\n",
      "Max prob per class: [0.9970169  0.99945086 0.998007   0.99821126 0.9913789  0.9818595\n",
      " 0.9815029  0.990391   0.94809556 0.9956108  0.9739259  0.98702973\n",
      " 0.99650526 0.9288242  0.9926311  0.9999175  0.9884596  0.9464675\n",
      " 0.64717084 0.9997874  0.9773965  0.99839884 0.99749327 0.9262514\n",
      " 0.9002475  0.8288101  0.9952642  0.9916741  0.9900393  0.99392384\n",
      " 0.8476004  0.9970029  0.97600967 0.9973199  0.949641   0.9989699\n",
      " 0.9457896  0.9986688  0.98982376 0.9876923  0.962127   0.99476457\n",
      " 0.9311243  0.9761795  0.98897105 0.9974126  0.99523735 0.9880427\n",
      " 0.9999064  0.83638316 0.911862   0.994759   0.99428225 0.9720328\n",
      " 0.99672216 0.7218359  0.98261845 0.99896073 0.89724314 0.98645043\n",
      " 0.9952651  0.99319047 0.96227276 0.9930139  0.9992418  0.99709594\n",
      " 0.9677796  0.98767406 0.8773825  0.9991823  0.9925821  0.9885959\n",
      " 0.74710006 0.9960891  0.99718696 0.9985417  0.98638153 0.99149686\n",
      " 0.99999404 0.955891   0.99573475]\n",
      "Epoch 4: 100%|██████████| 640/640 [03:27<00:00,  3.08it/s, v_num=0, train_loss_step=0.171, val_loss=0.144, val_ood_acc=0.914, val_acc_all=0.995, val_precision_all=0.871, val_recall_all=0.687, val_f1_all=0.768, avg_threshold=0.323, val_acc=0.995, val_precision=0.871, val_recall=0.687, val_f1=0.768, train_loss_epoch=0.146, train_acc=0.993, train_precision=0.905, train_recall=0.446, train_f1=0.597] Min prob per class: [7.21485804e-08 4.61266836e-07 2.20530097e-07 1.12977766e-07\n",
      " 6.98138535e-07 6.24671248e-09 2.46066190e-08 8.98927544e-10\n",
      " 3.15612596e-07 6.07422779e-10 1.07462084e-08 5.58674913e-07\n",
      " 6.06692518e-09 1.15162733e-08 9.37673263e-08 2.09088356e-08\n",
      " 6.15598310e-08 1.22017212e-07 3.04729292e-06 1.16320862e-07\n",
      " 4.63158685e-08 7.01850622e-09 2.07167421e-08 1.88241046e-07\n",
      " 1.11807310e-07 3.47015316e-07 1.24022135e-08 5.84702846e-08\n",
      " 1.59257951e-07 1.99498146e-07 1.93597183e-09 1.21203399e-07\n",
      " 3.38260726e-08 7.83836427e-08 8.25982660e-09 4.13623269e-07\n",
      " 3.20481291e-07 6.14817708e-09 3.49367255e-08 3.20622284e-09\n",
      " 2.74427112e-08 3.10136237e-08 2.54006025e-08 1.30754884e-07\n",
      " 2.26584419e-07 1.15271334e-07 2.32520208e-08 1.90972820e-08\n",
      " 9.90735458e-08 1.10865459e-08 7.62131052e-08 5.96165194e-07\n",
      " 1.80808115e-07 1.09786595e-06 1.27083126e-06 2.37473774e-08\n",
      " 8.04029241e-07 2.17326557e-09 1.13788144e-06 3.11168151e-07\n",
      " 5.75214813e-07 4.62104204e-08 6.26047978e-08 5.87942708e-08\n",
      " 3.08709183e-08 1.34974156e-08 8.95997516e-07 1.74457124e-08\n",
      " 1.53330788e-07 5.04575404e-08 1.04758309e-08 1.65399978e-08\n",
      " 1.05084943e-07 8.66028671e-09 5.30328776e-08 1.09491154e-07\n",
      " 6.30026705e-07 4.49891724e-09 2.72470788e-06 3.73043960e-08\n",
      " 3.14937800e-08]\n",
      "Max prob per class: [0.99985886 0.99933064 0.9967294  0.9999641  0.9984524  0.9834926\n",
      " 0.9939826  0.94656587 0.99134225 0.90874416 0.97495675 0.9998079\n",
      " 0.99576414 0.9843641  0.9966924  0.99998724 0.9969994  0.99952805\n",
      " 0.9647567  0.99984527 0.99361235 0.99102473 0.98876524 0.94867355\n",
      " 0.9400295  0.9815139  0.98757523 0.9977137  0.99571013 0.98766124\n",
      " 0.9803191  0.998041   0.9881841  0.9998466  0.99489516 0.9997366\n",
      " 0.9877153  0.99776435 0.98406583 0.9717667  0.93361956 0.9979913\n",
      " 0.70305777 0.9840289  0.99824846 0.9994766  0.9980501  0.99295133\n",
      " 0.99934334 0.9920141  0.93251145 0.9918099  0.9835533  0.99841106\n",
      " 0.99816006 0.9963335  0.9303288  0.9707644  0.9932902  0.99913424\n",
      " 0.9998939  0.9636189  0.99314994 0.9997812  0.9808992  0.9933315\n",
      " 0.973075   0.9257778  0.70099205 0.9986628  0.9975048  0.95605\n",
      " 0.88035434 0.99712026 0.99490464 0.9953804  0.99635065 0.9824076\n",
      " 0.99997425 0.975571   0.9980603 ]\n",
      "Epoch 5: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.188, val_loss=0.131, val_ood_acc=0.913, val_acc_all=0.995, val_precision_all=0.880, val_recall_all=0.703, val_f1_all=0.781, avg_threshold=0.322, val_acc=0.995, val_precision=0.880, val_recall=0.703, val_f1=0.781, train_loss_epoch=0.144, train_acc=0.993, train_precision=0.909, train_recall=0.448, train_f1=0.600] Min prob per class: [5.70640779e-09 2.29832757e-08 4.31356417e-09 8.65296030e-08\n",
      " 2.64587197e-09 5.95836658e-10 1.44405299e-09 2.63578293e-09\n",
      " 3.05318423e-07 7.72229001e-08 1.09183766e-07 7.79329952e-08\n",
      " 3.71661746e-09 4.79791362e-09 4.91327867e-08 2.28628960e-09\n",
      " 4.57864893e-08 6.62616462e-10 2.78890207e-06 5.35569988e-08\n",
      " 1.20353887e-08 4.03735978e-09 1.54707607e-08 1.19517996e-08\n",
      " 2.57696673e-08 2.20170495e-08 7.59953345e-09 7.55663692e-08\n",
      " 2.22304308e-09 2.21979839e-08 3.23777360e-09 3.87792660e-08\n",
      " 1.37491973e-08 4.76691842e-09 9.42825196e-09 3.80923353e-07\n",
      " 1.57183475e-08 6.89650159e-09 1.43631967e-10 2.39713671e-08\n",
      " 1.16063470e-09 9.00161936e-08 6.75769840e-08 1.38714729e-08\n",
      " 4.73408441e-08 3.50144891e-09 5.93218878e-08 2.97319458e-09\n",
      " 3.84445855e-07 3.86432220e-10 9.61523838e-08 1.88633837e-08\n",
      " 1.10223235e-08 2.62473576e-09 2.09697674e-07 5.26143083e-07\n",
      " 5.00764372e-08 3.16909138e-10 2.29525643e-08 3.63776067e-08\n",
      " 1.81686743e-09 3.34984773e-09 9.06045727e-09 5.51452350e-09\n",
      " 1.53402979e-08 2.83956645e-08 5.46810315e-06 2.23489477e-10\n",
      " 5.90981074e-07 2.43417553e-08 8.79317774e-09 1.01537934e-08\n",
      " 1.73324786e-08 4.25973568e-08 3.55591556e-10 2.20764562e-09\n",
      " 3.14827311e-08 4.16906465e-09 3.13337682e-06 8.34889047e-09\n",
      " 1.31957174e-08]\n",
      "Max prob per class: [0.9950806  0.9992893  0.99961686 0.99999964 0.99488825 0.9939813\n",
      " 0.9993661  0.9795751  0.9871853  0.9874998  0.9960341  0.999949\n",
      " 0.99635124 0.9930241  0.99918157 0.9999659  0.99993575 0.98375386\n",
      " 0.98721296 0.9998957  0.9990175  0.998147   0.9976458  0.98622084\n",
      " 0.9948344  0.99808925 0.99888617 0.99868816 0.9940679  0.9930686\n",
      " 0.9823387  0.99982387 0.9938192  0.99973625 0.99571687 0.9996201\n",
      " 0.9659681  0.99929976 0.9995629  0.9990559  0.9654786  0.99486583\n",
      " 0.97320604 0.9977301  0.9980763  0.99976736 0.9986695  0.98848397\n",
      " 0.9999671  0.96659243 0.9975915  0.99185514 0.9245275  0.998755\n",
      " 0.9985821  0.9997342  0.9749488  0.98424053 0.9134476  0.997575\n",
      " 0.97197974 0.97960746 0.99760205 0.99906653 0.99520385 0.996642\n",
      " 0.99316955 0.9869543  0.8002497  0.99985397 0.9998832  0.9910492\n",
      " 0.8811157  0.99726653 0.99946076 0.9965695  0.9987872  0.997297\n",
      " 0.9999882  0.99621576 0.9989151 ]\n",
      "Epoch 6: 100%|██████████| 640/640 [03:27<00:00,  3.09it/s, v_num=0, train_loss_step=0.197, val_loss=0.129, val_ood_acc=0.917, val_acc_all=0.995, val_precision_all=0.884, val_recall_all=0.712, val_f1_all=0.789, avg_threshold=0.330, val_acc=0.995, val_precision=0.884, val_recall=0.712, val_f1=0.789, train_loss_epoch=0.146, train_acc=0.993, train_precision=0.902, train_recall=0.446, train_f1=0.597] Min prob per class: [5.08686107e-08 3.80746826e-07 1.83033855e-09 2.40211932e-07\n",
      " 2.60362465e-09 1.86812954e-10 1.30584595e-12 5.71557690e-10\n",
      " 2.06242778e-09 7.25565441e-11 1.47559548e-10 8.53628901e-09\n",
      " 9.40105993e-10 3.33206907e-09 4.97566766e-10 1.83426219e-10\n",
      " 6.09335984e-11 1.27075084e-09 1.58145085e-07 9.93666873e-08\n",
      " 5.22344390e-10 5.17022328e-11 4.70282147e-11 7.84863508e-10\n",
      " 7.20692894e-10 4.34261587e-08 3.67062092e-09 7.55701543e-11\n",
      " 2.08134887e-09 2.25398988e-08 3.84289628e-10 2.43962770e-08\n",
      " 5.13664888e-10 1.54651865e-08 4.46920545e-09 1.25374299e-07\n",
      " 4.99647150e-08 2.43684073e-09 2.49162219e-10 2.00866129e-10\n",
      " 8.03872593e-11 2.06204334e-08 3.73667403e-10 1.23087440e-09\n",
      " 1.18795729e-08 2.25015668e-07 3.99723987e-09 1.34584877e-09\n",
      " 4.31107878e-07 1.94675630e-11 2.83772406e-09 3.85397492e-09\n",
      " 1.14814270e-07 3.27735350e-08 3.29972778e-07 1.31257076e-08\n",
      " 2.17320473e-08 1.33620093e-09 2.02926529e-08 1.51893417e-07\n",
      " 1.23219275e-08 4.54275380e-07 5.26813082e-09 1.08222320e-08\n",
      " 5.23486904e-07 3.76681464e-10 4.42876269e-09 8.56659454e-09\n",
      " 2.02574615e-08 3.11566578e-10 2.09185180e-09 2.45130605e-09\n",
      " 5.79310955e-09 3.21459992e-09 1.88914950e-09 2.28137278e-08\n",
      " 2.45572323e-07 1.22861332e-09 3.09309513e-07 2.83739920e-09\n",
      " 1.23419142e-09]\n",
      "Max prob per class: [0.99967146 0.99993527 0.99985814 0.99998903 0.9641112  0.99841094\n",
      " 0.9961488  0.9962819  0.8055658  0.99637824 0.9377277  0.9998305\n",
      " 0.99818015 0.9931117  0.9953003  0.99870753 0.9872709  0.9941863\n",
      " 0.9110639  0.999974   0.9990569  0.9792327  0.9995166  0.8881808\n",
      " 0.9942912  0.9962684  0.9964644  0.98827446 0.998536   0.997486\n",
      " 0.9654578  0.9999131  0.99907136 0.9993623  0.99574584 0.9999751\n",
      " 0.993736   0.9997955  0.9999757  0.99526817 0.9466036  0.99350375\n",
      " 0.98950875 0.9918168  0.9958988  0.9999832  0.99957687 0.95214176\n",
      " 0.9999484  0.9856456  0.98185813 0.9992555  0.99563247 0.99978405\n",
      " 0.99832875 0.999734   0.9664865  0.9957634  0.9879192  0.98051053\n",
      " 0.9979437  0.99869823 0.9996458  0.99644655 0.9999894  0.9940844\n",
      " 0.9127597  0.98151726 0.95080465 0.999863   0.98713684 0.99803406\n",
      " 0.9873461  0.99945945 0.998706   0.9997464  0.99964106 0.99800557\n",
      " 0.9999573  0.9952413  0.9974407 ]\n",
      "Epoch 7: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.0571, val_loss=0.137, val_ood_acc=0.904, val_acc_all=0.995, val_precision_all=0.896, val_recall_all=0.681, val_f1_all=0.774, avg_threshold=0.340, val_acc=0.995, val_precision=0.896, val_recall=0.681, val_f1=0.774, train_loss_epoch=0.141, train_acc=0.993, train_precision=0.899, train_recall=0.447, train_f1=0.597]Min prob per class: [3.53697716e-09 1.98616212e-09 8.56301197e-10 3.51089327e-08\n",
      " 3.39774098e-10 5.39960743e-09 3.37803258e-10 8.25614854e-11\n",
      " 2.21262408e-09 1.19218940e-10 8.84035622e-11 2.86506374e-09\n",
      " 4.38725456e-09 6.41768250e-10 3.22932264e-10 3.49637513e-10\n",
      " 1.50097546e-10 2.36503955e-10 3.66825681e-08 7.14711561e-08\n",
      " 5.98101083e-11 3.20461324e-10 4.24541305e-11 9.32684138e-12\n",
      " 1.83959592e-09 2.94827407e-09 4.36152947e-10 6.18461060e-10\n",
      " 4.12395318e-09 1.85499758e-08 7.06878445e-11 6.82095660e-08\n",
      " 1.71359898e-10 2.58484656e-09 2.68420581e-11 1.54388502e-10\n",
      " 1.27482025e-08 6.23249896e-10 2.56748827e-08 7.92234722e-10\n",
      " 5.95622718e-11 3.08914583e-09 3.85816119e-12 1.50540602e-09\n",
      " 3.21090321e-10 6.37248726e-11 3.08084169e-10 1.37442016e-10\n",
      " 2.18297217e-07 5.64899905e-10 4.91886254e-10 1.95029259e-09\n",
      " 6.73380196e-09 1.74869119e-09 3.68362429e-09 4.09526457e-09\n",
      " 7.78430831e-09 2.88193891e-09 3.12554760e-09 6.83120334e-08\n",
      " 7.09969084e-10 2.07390476e-11 3.03314707e-10 1.27576172e-08\n",
      " 6.52890648e-08 5.00515573e-10 3.56008401e-09 6.69110101e-10\n",
      " 6.83785117e-08 3.64803521e-09 9.14211040e-10 9.54930579e-10\n",
      " 2.25940683e-10 4.71276618e-09 9.86363990e-10 9.70121121e-08\n",
      " 1.15737059e-07 3.27639998e-10 3.29553870e-07 2.67011885e-10\n",
      " 4.88893415e-09]\n",
      "Max prob per class: [0.9999769  0.9999578  0.9934369  0.9999944  0.995171   0.9998129\n",
      " 0.9997569  0.99709857 0.95004696 0.99254143 0.960092   0.9993759\n",
      " 0.9983064  0.9928677  0.98366517 0.9999995  0.997221   0.9880499\n",
      " 0.99096525 0.99999654 0.99328434 0.99548507 0.99994266 0.99514884\n",
      " 0.9942545  0.99149644 0.9903887  0.99286604 0.9999306  0.99959904\n",
      " 0.93039757 0.9998938  0.99670094 0.99934    0.9876088  0.9998048\n",
      " 0.997216   0.99878365 0.9999871  0.9954064  0.9212372  0.9996706\n",
      " 0.9843433  0.9994203  0.9977418  0.9990381  0.9993043  0.978131\n",
      " 0.9999461  0.99840754 0.9838987  0.9991749  0.99090236 0.9982988\n",
      " 0.99236274 0.9997365  0.9833239  0.9999002  0.9680913  0.99953663\n",
      " 0.9996171  0.98490614 0.9987839  0.9999442  0.9987413  0.999853\n",
      " 0.9847408  0.99565303 0.9693702  0.99994206 0.9999416  0.9948719\n",
      " 0.94846267 0.9992848  0.99698883 0.99935454 0.9994111  0.9945214\n",
      " 0.99998903 0.99558663 0.9993529 ]\n",
      "Epoch 8: 100%|██████████| 640/640 [03:38<00:00,  2.94it/s, v_num=0, train_loss_step=0.252, val_loss=0.129, val_ood_acc=0.913, val_acc_all=0.995, val_precision_all=0.890, val_recall_all=0.705, val_f1_all=0.787, avg_threshold=0.344, val_acc=0.995, val_precision=0.890, val_recall=0.705, val_f1=0.787, train_loss_epoch=0.140, train_acc=0.993, train_precision=0.901, train_recall=0.453, train_f1=0.603] Min prob per class: [7.84844758e-08 2.55921784e-10 9.27122989e-10 6.46426454e-11\n",
      " 6.61086116e-11 4.94707747e-11 5.07241514e-12 6.69370503e-10\n",
      " 2.95656166e-10 1.01197887e-11 8.02064193e-10 1.04547313e-08\n",
      " 3.25814549e-11 1.82202683e-10 2.06807543e-10 7.84490008e-12\n",
      " 1.56699645e-13 1.42757772e-09 3.06567465e-08 4.76206541e-09\n",
      " 5.60212814e-11 1.00389107e-11 1.57031905e-11 2.63725641e-09\n",
      " 2.86029156e-09 1.85922406e-11 2.68826650e-09 5.38949753e-11\n",
      " 6.77570944e-10 5.89642390e-10 1.39973366e-09 8.00468691e-10\n",
      " 1.06803594e-10 1.34657230e-10 2.47964902e-08 2.64045275e-09\n",
      " 1.63934732e-09 2.74573947e-10 1.77887016e-09 1.34702943e-10\n",
      " 9.97656818e-12 9.81173406e-11 6.18763152e-10 2.71628053e-08\n",
      " 1.66960126e-11 2.58931099e-10 1.54390278e-10 5.52307866e-10\n",
      " 2.78001372e-10 2.32264988e-11 1.22605856e-10 8.94431640e-09\n",
      " 3.54352346e-11 2.03006445e-10 6.44423848e-10 4.53311971e-10\n",
      " 6.69701294e-10 1.05889852e-09 6.23441299e-10 1.66858749e-09\n",
      " 3.47127771e-10 3.07949638e-10 9.33265243e-10 5.57559589e-11\n",
      " 2.44897542e-08 1.21059811e-11 5.80486947e-08 1.92966323e-10\n",
      " 3.05491543e-09 5.18133447e-09 2.01770406e-10 2.06267559e-09\n",
      " 9.92922011e-09 3.66713040e-12 5.30573141e-10 5.14463228e-09\n",
      " 3.35782957e-10 8.01715153e-11 9.26088148e-08 9.93232052e-10\n",
      " 1.19920802e-08]\n",
      "Max prob per class: [0.99993753 0.99960643 0.99972874 0.9999747  0.9813818  0.9995003\n",
      " 0.9886824  0.9999045  0.9836848  0.9997521  0.98155874 0.99993277\n",
      " 0.9989761  0.9813557  0.99983704 0.99816257 0.9853498  0.9990872\n",
      " 0.93367213 0.9999021  0.9994273  0.99942976 0.999912   0.9469049\n",
      " 0.9453131  0.9897372  0.9969066  0.99931467 0.9992132  0.9916888\n",
      " 0.9942372  0.9998473  0.9855526  0.99993014 0.9996915  0.99999976\n",
      " 0.99412894 0.9996412  0.9999944  0.99578625 0.92952543 0.99845374\n",
      " 0.97883385 0.99875927 0.9996854  0.99979633 0.9998672  0.9985796\n",
      " 0.9995012  0.9986498  0.9872792  0.9994368  0.99108046 0.99941194\n",
      " 0.99671495 0.97008836 0.97394764 0.9996941  0.9938747  0.999508\n",
      " 0.9997336  0.9874713  0.9989103  0.9996761  0.9998549  0.9980416\n",
      " 0.99250036 0.9976265  0.76070714 0.99941003 0.99993527 0.99796766\n",
      " 0.9727131  0.9966234  0.99839336 0.99960774 0.99948967 0.9993274\n",
      " 0.99998033 0.9998318  0.9999993 ]\n",
      "Epoch 8: 100%|██████████| 640/640 [04:27<00:00,  2.39it/s, v_num=0, train_loss_step=0.252, val_loss=0.125, val_ood_acc=0.918, val_acc_all=0.995, val_precision_all=0.908, val_recall_all=0.696, val_f1_all=0.788, avg_threshold=0.345, val_acc=0.995, val_precision=0.908, val_recall=0.696, val_f1=0.788, train_loss_epoch=0.141, train_acc=0.993, train_precision=0.898, train_recall=0.456, train_f1=0.605]\n",
      "Fold 5/5\n",
      "Setup ran successfully\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n",
      "\n",
      "   | Name              | Type                | Params | Mode \n",
      "-------------------------------------------------------------------\n",
      "0  | feature_extractor | Sequential          | 65.3 M | train\n",
      "1  | ood_classifier    | Sequential          | 2.0 K  | train\n",
      "2  | classifier        | Sequential          | 165 K  | train\n",
      "3  | loss_fn_class     | BCEWithLogitsLoss   | 0      | train\n",
      "4  | loss_fn_ood       | BCEWithLogitsLoss   | 0      | train\n",
      "5  | train_acc         | MultilabelAccuracy  | 0      | train\n",
      "6  | val_acc           | MultilabelAccuracy  | 0      | train\n",
      "7  | train_precision   | MultilabelPrecision | 0      | train\n",
      "8  | val_precision     | MultilabelPrecision | 0      | train\n",
      "9  | train_recall      | MultilabelRecall    | 0      | train\n",
      "10 | val_recall        | MultilabelRecall    | 0      | train\n",
      "11 | train_f1          | MultilabelF1Score   | 0      | train\n",
      "12 | val_f1            | MultilabelF1Score   | 0      | train\n",
      "13 | ood_acc           | BinaryAccuracy      | 0      | train\n",
      "-------------------------------------------------------------------\n",
      "168 K     Trainable params\n",
      "65.3 M    Non-trainable params\n",
      "65.5 M    Total params\n",
      "261.938   Total estimated model params size (MB)\n",
      "958       Modules in train mode\n",
      "0         Modules in eval mode\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking: |          | 0/? [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:425: The 'val_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=9` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  3.24it/s]Min prob per class: [0.4383197  0.48074412 0.4809654  0.46545428 0.4792385  0.50253403\n",
      " 0.46327585 0.48101383 0.44423515 0.4865572  0.4808612  0.47394562\n",
      " 0.4484189  0.4663606  0.46804366 0.46802548 0.47742763 0.45312014\n",
      " 0.460666   0.4651361  0.4635306  0.45293587 0.45980546 0.47081718\n",
      " 0.44973263 0.45205328 0.45396164 0.48635268 0.47552845 0.464676\n",
      " 0.46436018 0.47962382 0.45093763 0.4574163  0.4460278  0.46978518\n",
      " 0.46804866 0.47699013 0.43519455 0.44866112 0.45739177 0.46461383\n",
      " 0.47145054 0.47006285 0.47508758 0.4395605  0.46092018 0.46738112\n",
      " 0.46216184 0.4735565  0.45495236 0.4486762  0.47627437 0.46010998\n",
      " 0.4737329  0.4741783  0.455088   0.45302844 0.4576917  0.481038\n",
      " 0.4662836  0.47168222 0.46572495 0.44689685 0.45756426 0.48277307\n",
      " 0.45790905 0.46022877 0.46917796 0.47646844 0.46423933 0.47395483\n",
      " 0.48247963 0.4679949  0.4612945  0.47317323 0.4564904  0.42724732\n",
      " 0.47062314 0.47340992 0.46346158]\n",
      "Max prob per class: [0.51744556 0.55305374 0.5460919  0.5192819  0.55637455 0.5616886\n",
      " 0.5378984  0.548861   0.5149463  0.53754854 0.5526298  0.536249\n",
      " 0.519795   0.5227021  0.53209805 0.54932344 0.55444276 0.5124749\n",
      " 0.5215673  0.54754657 0.54874545 0.5265008  0.51757884 0.55824417\n",
      " 0.52011365 0.5225578  0.5336229  0.5379764  0.5241513  0.55742615\n",
      " 0.51397413 0.55743164 0.538753   0.5312616  0.54031307 0.5428063\n",
      " 0.52706945 0.5401546  0.5151311  0.5125682  0.53059375 0.52977186\n",
      " 0.53187567 0.5337994  0.52561563 0.5293194  0.53347    0.5383244\n",
      " 0.5375302  0.53827447 0.5287166  0.5392805  0.54017407 0.5296355\n",
      " 0.5482614  0.54312944 0.5232582  0.5270149  0.54615057 0.55803025\n",
      " 0.5084809  0.5331042  0.5263564  0.5099445  0.54123706 0.5496751\n",
      " 0.52030766 0.5435989  0.53005344 0.53176194 0.5321845  0.5407929\n",
      " 0.57483923 0.54652023 0.5324511  0.5713014  0.5168326  0.5390851\n",
      " 0.52444184 0.5354402  0.5298965 ]\n",
      "Warning: No positive samples for class 3, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 4, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 5, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 6, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 7, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 8, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 9, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 10, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 11, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 12, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 13, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 14, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 15, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 16, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 17, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 18, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 19, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 20, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 21, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 22, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 23, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 24, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 25, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 26, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 27, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 28, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 29, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 30, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 31, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 32, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 33, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 34, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 35, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 36, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 37, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 38, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 39, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 40, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 41, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 42, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 43, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 44, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 45, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 46, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 47, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 48, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 49, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 50, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 51, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 52, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 53, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 54, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 55, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 56, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 57, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 58, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 59, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 60, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 61, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 62, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 63, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 64, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 65, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 66, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 67, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 68, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 69, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 70, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 71, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 72, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 73, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 74, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 75, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 76, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 77, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 78, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 79, keeping threshold at 0.5\n",
      "Warning: No positive samples for class 80, keeping threshold at 0.5\n",
      "                                                                           "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric BinaryAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelAccuracy was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelPrecision was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelRecall was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n",
      "/Users/beszabo/Documents/meme/.venv/lib/python3.13/site-packages/torchmetrics/utilities/prints.py:43: UserWarning: The ``compute`` method of metric MultilabelF1Score was called before the ``update`` method which may lead to errors, as metric states have not yet been updated.\n",
      "  warnings.warn(*args, **kwargs)  # noqa: B028\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0: 100%|██████████| 640/640 [03:34<00:00,  2.98it/s, v_num=0, train_loss_step=0.167] Min prob per class: [2.84465059e-05 2.88435003e-05 4.00547906e-05 3.88958215e-05\n",
      " 3.19092433e-05 2.86175418e-05 5.29242607e-05 2.97304996e-05\n",
      " 2.64978444e-05 2.00272898e-05 1.92738153e-05 1.51766189e-05\n",
      " 3.67254906e-05 2.22720591e-05 2.73415844e-05 3.63380241e-05\n",
      " 2.41254456e-05 2.54726474e-05 1.71078849e-04 3.52782481e-05\n",
      " 5.75215054e-05 9.87989642e-06 2.49519017e-05 8.77369457e-05\n",
      " 3.07932423e-05 2.82610436e-05 2.88765532e-05 2.75533184e-05\n",
      " 2.25426920e-05 1.28833672e-05 2.09100053e-05 6.91904206e-05\n",
      " 1.19478236e-05 1.26072828e-05 8.33996673e-05 4.63653196e-05\n",
      " 5.11767721e-05 4.28708809e-05 5.50541881e-05 4.20774559e-05\n",
      " 7.14470807e-05 1.03597245e-04 2.75499533e-05 1.94029290e-05\n",
      " 2.10390808e-05 2.85414480e-05 6.71263551e-05 1.59509436e-05\n",
      " 3.40504630e-05 4.73408400e-05 3.12787743e-05 6.04259621e-05\n",
      " 1.08162130e-04 4.01299221e-05 3.59841106e-05 1.37878687e-05\n",
      " 1.28208221e-05 1.76483609e-05 4.60245719e-05 2.79981778e-05\n",
      " 1.23661393e-05 4.17231531e-05 4.56290763e-05 8.63820969e-05\n",
      " 5.90033196e-05 6.30590293e-05 5.16566906e-05 2.06750756e-05\n",
      " 1.18687140e-05 6.17983460e-05 3.75028430e-05 2.50828562e-05\n",
      " 1.01174210e-05 4.42809869e-05 3.10407850e-05 4.28095991e-05\n",
      " 1.76764341e-04 5.10980135e-05 1.31932436e-03 2.68091862e-05\n",
      " 6.27647023e-05]\n",
      "Max prob per class: [0.38021156 0.3966471  0.40264326 0.4030965  0.39979938 0.39966536\n",
      " 0.38798895 0.3997521  0.39112258 0.38626975 0.3838944  0.37318805\n",
      " 0.38894397 0.38179156 0.38728887 0.38568553 0.38098976 0.37634113\n",
      " 0.39788017 0.38721025 0.38931003 0.38626903 0.39739564 0.3992195\n",
      " 0.37714252 0.3848865  0.38048828 0.39807668 0.38987115 0.37369007\n",
      " 0.3945272  0.4075903  0.38546145 0.37183565 0.39317754 0.3893819\n",
      " 0.38957196 0.3919914  0.38831747 0.39414912 0.38860726 0.39822084\n",
      " 0.37620735 0.3886631  0.3830029  0.39140028 0.39636937 0.38299608\n",
      " 0.38563913 0.38019675 0.38002858 0.3938662  0.39215884 0.39481112\n",
      " 0.38962668 0.38129762 0.3813139  0.37249768 0.40031183 0.38531974\n",
      " 0.37847543 0.3970556  0.39403322 0.39693502 0.38890827 0.3913942\n",
      " 0.39674452 0.36982995 0.38557833 0.39416438 0.39433026 0.38001165\n",
      " 0.38949412 0.3887603  0.3990935  0.38408983 0.39305425 0.37723976\n",
      " 0.99934274 0.39840725 0.39042437]\n",
      "Epoch 1: 100%|██████████| 640/640 [03:40<00:00,  2.90it/s, v_num=0, train_loss_step=0.0989, val_loss=0.173, val_ood_acc=0.901, val_acc_all=0.993, val_precision_all=0.892, val_recall_all=0.462, val_f1_all=0.609, avg_threshold=0.372, val_acc=0.993, val_precision=0.892, val_recall=0.462, val_f1=0.609, train_loss_epoch=0.311, train_acc=0.986, train_precision=0.450, train_recall=0.429, train_f1=0.439]Min prob per class: [5.48884964e-06 2.33354945e-06 5.06406695e-06 4.08772303e-06\n",
      " 1.71176191e-06 2.54001998e-06 1.61089451e-06 1.58866885e-06\n",
      " 2.51585902e-06 4.45540036e-06 1.27133308e-06 4.02537762e-06\n",
      " 1.44752914e-06 1.56431929e-06 1.85084866e-06 2.05351444e-06\n",
      " 8.94464165e-07 5.50789491e-06 1.05248646e-05 5.71091914e-06\n",
      " 2.34127128e-06 2.48508104e-06 9.48743093e-07 2.69590373e-06\n",
      " 9.09624987e-07 3.66829317e-06 7.78336107e-06 1.71338615e-05\n",
      " 2.05006654e-06 8.90041247e-06 4.10952589e-06 3.12984666e-05\n",
      " 2.03384707e-06 6.25935684e-07 2.52404902e-06 4.56352336e-06\n",
      " 7.68858263e-06 6.48642708e-06 4.89208423e-06 2.96678354e-06\n",
      " 1.67713523e-07 1.35578612e-05 4.99802118e-05 1.31148336e-05\n",
      " 1.07743181e-05 1.76875710e-06 4.35535958e-06 6.64719164e-06\n",
      " 1.05886829e-05 1.39087831e-06 8.83472057e-06 5.40232122e-06\n",
      " 1.01213873e-05 6.88050113e-06 4.22518178e-06 2.83852978e-06\n",
      " 1.23969119e-06 9.09089067e-07 6.61683298e-05 8.06528897e-06\n",
      " 2.34372851e-06 1.00028792e-05 1.43022976e-06 5.43463329e-06\n",
      " 5.45966668e-06 1.16852998e-05 1.43513071e-05 2.27678811e-06\n",
      " 1.10687870e-05 8.81930737e-06 2.01577677e-06 3.55344878e-06\n",
      " 2.14248689e-06 4.41152588e-06 2.24152632e-06 4.37750441e-06\n",
      " 1.11430745e-05 4.60801748e-05 1.94939585e-05 2.71136378e-06\n",
      " 8.35520041e-07]\n",
      "Max prob per class: [0.91389394 0.77468634 0.83894056 0.98145294 0.5051699  0.9377128\n",
      " 0.79196763 0.525699   0.8822594  0.8926935  0.77981114 0.92394507\n",
      " 0.8109981  0.8416741  0.56266695 0.85967493 0.39906105 0.86250156\n",
      " 0.5875806  0.9735377  0.92748505 0.91786814 0.77237225 0.4238585\n",
      " 0.5925025  0.88138443 0.8139963  0.84664696 0.90296686 0.9808894\n",
      " 0.8725503  0.9694959  0.88458127 0.98367774 0.77007246 0.9323761\n",
      " 0.40477914 0.9691515  0.682963   0.87939954 0.42528844 0.94952744\n",
      " 0.81232554 0.9353834  0.9152117  0.95943    0.95606357 0.89026725\n",
      " 0.98539287 0.40522352 0.8119057  0.7438588  0.6740137  0.99108666\n",
      " 0.79334533 0.4406356  0.72501475 0.6189345  0.7134684  0.9610984\n",
      " 0.71289384 0.91984445 0.47916508 0.7678619  0.9161628  0.99298614\n",
      " 0.8434458  0.7198652  0.6333401  0.9825984  0.9432674  0.61558926\n",
      " 0.4284732  0.96033335 0.9783572  0.9123325  0.770749   0.9002427\n",
      " 0.9999404  0.42945746 0.7166142 ]\n",
      "Epoch 2: 100%|██████████| 640/640 [03:36<00:00,  2.95it/s, v_num=0, train_loss_step=0.188, val_loss=0.139, val_ood_acc=0.914, val_acc_all=0.994, val_precision_all=0.911, val_recall_all=0.568, val_f1_all=0.700, avg_threshold=0.329, val_acc=0.994, val_precision=0.911, val_recall=0.568, val_f1=0.700, train_loss_epoch=0.166, train_acc=0.992, train_precision=0.909, train_recall=0.436, train_f1=0.589] Min prob per class: [6.93548373e-07 3.10838504e-06 8.93304787e-07 8.61394165e-06\n",
      " 2.69867940e-07 4.26219735e-08 3.27651009e-08 4.40786607e-07\n",
      " 1.77664708e-06 1.22338020e-06 1.82565725e-06 2.46990294e-06\n",
      " 1.24141025e-06 3.02638398e-07 1.47241519e-06 1.00215266e-06\n",
      " 1.72087084e-06 8.22602397e-06 1.53884685e-06 7.88850230e-07\n",
      " 8.30390832e-07 6.33874819e-08 7.75025129e-08 2.10006803e-08\n",
      " 1.77437158e-07 7.59739748e-07 1.66452301e-05 8.96171002e-07\n",
      " 1.39986469e-06 1.93875039e-07 1.15858813e-06 5.56862119e-07\n",
      " 2.06472421e-07 5.17352078e-07 1.31684203e-06 3.71911256e-06\n",
      " 8.81027063e-06 1.30208525e-06 5.40296838e-08 5.40566418e-07\n",
      " 4.70209960e-09 9.48000491e-07 1.29467560e-06 2.93462540e-06\n",
      " 2.85423084e-07 5.22017217e-07 1.35788369e-07 3.08627932e-08\n",
      " 5.70956729e-07 1.07951621e-06 3.04816194e-06 2.87024750e-06\n",
      " 8.52483595e-07 5.63357446e-07 1.47013782e-06 1.43709474e-06\n",
      " 3.02392579e-07 2.86567229e-07 1.71779334e-07 1.16823919e-07\n",
      " 1.92401529e-07 2.02671870e-07 2.89325641e-07 2.61395917e-06\n",
      " 5.58603733e-06 1.01736350e-06 8.15659519e-07 1.15993416e-07\n",
      " 1.03956227e-05 2.89756400e-07 2.96001508e-06 1.76109711e-06\n",
      " 1.66347422e-06 2.10323563e-07 4.26485167e-06 7.56582949e-07\n",
      " 8.02349462e-07 1.25011720e-05 2.22161088e-06 1.24221401e-07\n",
      " 3.39894115e-07]\n",
      "Max prob per class: [0.9838018  0.9875394  0.89178425 0.999374   0.91875046 0.97721624\n",
      " 0.72336066 0.9303858  0.9535681  0.92988914 0.87165487 0.989419\n",
      " 0.9701946  0.9478387  0.97998214 0.9995597  0.8760147  0.9823093\n",
      " 0.746662   0.99815565 0.9879814  0.9741034  0.9874685  0.79019356\n",
      " 0.88511944 0.7991595  0.99367875 0.86798906 0.9615955  0.98402184\n",
      " 0.9820088  0.9971667  0.9828864  0.99964774 0.9617248  0.9966889\n",
      " 0.8339094  0.99115187 0.9134579  0.897443   0.3248555  0.99528176\n",
      " 0.8230639  0.9087582  0.8074809  0.9967002  0.997168   0.9169944\n",
      " 0.9996594  0.8715793  0.98555565 0.9563592  0.65953195 0.9956456\n",
      " 0.95571524 0.8498621  0.83201844 0.98080623 0.916335   0.9928068\n",
      " 0.88837284 0.96593004 0.87669027 0.9693196  0.9949397  0.99901116\n",
      " 0.6817953  0.9687562  0.79547524 0.99202555 0.97884506 0.9228595\n",
      " 0.73562765 0.98671407 0.99925    0.9522515  0.98910564 0.93893653\n",
      " 0.9999659  0.98443466 0.96763915]\n",
      "Epoch 3: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.140, val_loss=0.132, val_ood_acc=0.915, val_acc_all=0.995, val_precision_all=0.910, val_recall_all=0.640, val_f1_all=0.751, avg_threshold=0.323, val_acc=0.995, val_precision=0.910, val_recall=0.640, val_f1=0.751, train_loss_epoch=0.152, train_acc=0.993, train_precision=0.914, train_recall=0.439, train_f1=0.593] Min prob per class: [4.98333657e-06 7.87049066e-06 1.82627048e-07 6.92869435e-07\n",
      " 6.68195987e-09 2.71612510e-08 1.19978381e-07 1.12034435e-07\n",
      " 1.83017619e-07 5.43984271e-08 4.10978600e-07 3.21411926e-06\n",
      " 4.79664450e-08 1.61427764e-07 4.32787282e-08 1.09275227e-07\n",
      " 8.82215971e-08 6.84425743e-07 2.11674887e-06 4.93989000e-06\n",
      " 1.53209456e-07 1.04606155e-08 1.02061612e-08 3.88206786e-07\n",
      " 2.08383710e-07 2.93700282e-07 1.46159221e-06 1.77272767e-07\n",
      " 9.94412630e-07 2.02372490e-07 3.36205588e-08 7.93670765e-08\n",
      " 9.80843637e-08 2.27591830e-08 7.91594701e-09 7.73552074e-06\n",
      " 6.90396632e-07 4.29613095e-07 8.40997700e-07 5.56249624e-08\n",
      " 1.92923082e-08 1.33592382e-08 2.88936036e-07 7.08037874e-07\n",
      " 8.57802647e-08 1.84554196e-07 7.83855896e-08 5.48668169e-08\n",
      " 3.18672789e-07 9.37869604e-09 3.16291164e-07 4.00793823e-07\n",
      " 8.15049077e-07 9.70585745e-08 1.33687518e-05 4.82816006e-07\n",
      " 3.54693498e-06 5.42281100e-08 7.56905383e-09 9.41752631e-08\n",
      " 5.06079971e-07 1.46109301e-07 2.53498627e-07 1.09072616e-07\n",
      " 4.68964316e-08 1.82986881e-07 2.10990652e-06 1.83962385e-07\n",
      " 4.30755472e-06 1.64901550e-07 1.79037855e-07 7.94747677e-07\n",
      " 1.10428651e-07 2.48423703e-07 1.14500494e-06 3.62445576e-08\n",
      " 2.75045522e-06 3.76234539e-07 3.38623659e-06 1.97942256e-07\n",
      " 1.42533708e-07]\n",
      "Max prob per class: [0.9882068  0.9989374  0.9820759  0.99989307 0.9168578  0.9987721\n",
      " 0.95865446 0.99013805 0.9494933  0.9444515  0.92632455 0.9990361\n",
      " 0.86181915 0.95488733 0.933921   0.9997749  0.96837324 0.988466\n",
      " 0.8967251  0.99964345 0.9885661  0.9754117  0.99446917 0.8170631\n",
      " 0.98136383 0.9278145  0.9879589  0.62365067 0.9996043  0.9936132\n",
      " 0.94945556 0.99819946 0.98492706 0.9996238  0.968695   0.9982956\n",
      " 0.6845425  0.99754155 0.99616015 0.9407897  0.9495242  0.9960848\n",
      " 0.90462    0.9914976  0.9967533  0.9992792  0.99638915 0.9951951\n",
      " 0.99874914 0.92716944 0.96669835 0.9953016  0.9835186  0.9985442\n",
      " 0.99849    0.9449261  0.9962237  0.9967719  0.94921964 0.9986344\n",
      " 0.9817058  0.9988986  0.9778993  0.9944561  0.9793559  0.9996698\n",
      " 0.96270025 0.99075794 0.85058856 0.9990225  0.9947143  0.969517\n",
      " 0.5052649  0.990035   0.99590576 0.9454634  0.9932562  0.6311967\n",
      " 0.9999925  0.99768305 0.9793505 ]\n",
      "Epoch 4: 100%|██████████| 640/640 [03:39<00:00,  2.92it/s, v_num=0, train_loss_step=0.150, val_loss=0.128, val_ood_acc=0.917, val_acc_all=0.995, val_precision_all=0.891, val_recall_all=0.679, val_f1_all=0.771, avg_threshold=0.330, val_acc=0.995, val_precision=0.891, val_recall=0.679, val_f1=0.771, train_loss_epoch=0.153, train_acc=0.993, train_precision=0.907, train_recall=0.442, train_f1=0.594] Min prob per class: [6.01451244e-09 1.61524429e-08 1.42112953e-07 1.24212988e-07\n",
      " 1.25773036e-08 2.08824025e-09 3.06686032e-09 2.60521449e-09\n",
      " 7.68909914e-09 1.40460186e-08 1.68662737e-07 3.15012940e-08\n",
      " 6.61918591e-08 5.13546354e-07 1.84860539e-07 4.02146050e-08\n",
      " 3.91583264e-08 8.15376211e-09 3.62087675e-08 1.10162364e-06\n",
      " 4.52457682e-10 1.04618281e-09 5.77051615e-08 3.46789939e-08\n",
      " 6.50403242e-09 4.70821320e-08 4.23644018e-08 4.76314845e-08\n",
      " 5.69537306e-08 1.86190281e-08 5.88582196e-08 1.24459314e-06\n",
      " 1.41350498e-09 2.53197618e-09 1.74875545e-08 1.12551639e-08\n",
      " 7.60395906e-08 3.87155588e-08 1.23401591e-07 1.41739331e-08\n",
      " 4.32752572e-08 7.35192671e-08 6.29855492e-08 1.61169602e-08\n",
      " 8.42670644e-09 2.95900804e-07 9.29283672e-08 3.79466094e-08\n",
      " 6.02206285e-07 1.93280343e-08 7.02173608e-08 1.35100024e-07\n",
      " 2.13248100e-07 9.39354663e-07 1.20477154e-07 1.30343238e-08\n",
      " 1.39890204e-07 2.05815609e-09 7.50257301e-10 4.03137932e-07\n",
      " 6.52864163e-09 7.60534835e-09 2.34740245e-08 6.42414264e-08\n",
      " 3.18476987e-06 1.40687009e-08 1.88165487e-07 1.21484218e-08\n",
      " 3.58082914e-08 1.01967235e-08 7.92052290e-09 1.58846589e-07\n",
      " 4.64898733e-08 3.49516371e-09 1.62907966e-07 5.88359399e-07\n",
      " 2.06376267e-08 6.33467234e-09 2.54695010e-06 5.71843977e-08\n",
      " 4.47887629e-08]\n",
      "Max prob per class: [0.99440926 0.9991829  0.98932356 0.99999297 0.97433734 0.9950949\n",
      " 0.9924678  0.988052   0.89876956 0.97929996 0.96138155 0.99897325\n",
      " 0.9859502  0.9974806  0.99929786 0.99998796 0.9482907  0.97843057\n",
      " 0.6356125  0.99989486 0.9548299  0.97121453 0.99989736 0.9775889\n",
      " 0.93819934 0.97048855 0.987677   0.9921206  0.9988336  0.9960375\n",
      " 0.94073147 0.9999471  0.96146345 0.9981317  0.99759585 0.9990061\n",
      " 0.94367176 0.9999281  0.9938508  0.997449   0.90754586 0.99694747\n",
      " 0.9743154  0.9952242  0.99284416 0.9998729  0.9996855  0.9869495\n",
      " 0.9994696  0.9714897  0.98885554 0.9953713  0.9634158  0.9998673\n",
      " 0.9841537  0.9898977  0.99616504 0.9986124  0.90338856 0.9986093\n",
      " 0.89872515 0.9953302  0.990437   0.9940598  0.999894   0.9998468\n",
      " 0.9631083  0.96260655 0.59403634 0.9997707  0.99656445 0.9881373\n",
      " 0.6737628  0.9941081  0.99975973 0.9987469  0.9954904  0.86657506\n",
      " 0.99998426 0.99209565 0.9915541 ]\n",
      "Epoch 5: 100%|██████████| 640/640 [03:43<00:00,  2.86it/s, v_num=0, train_loss_step=0.176, val_loss=0.123, val_ood_acc=0.922, val_acc_all=0.995, val_precision_all=0.909, val_recall_all=0.684, val_f1_all=0.781, avg_threshold=0.328, val_acc=0.995, val_precision=0.909, val_recall=0.684, val_f1=0.781, train_loss_epoch=0.148, train_acc=0.993, train_precision=0.906, train_recall=0.445, train_f1=0.597] Min prob per class: [1.7642133e-08 1.1825276e-11 5.2232892e-08 1.4331719e-07 5.2945662e-08\n",
      " 2.2200219e-09 6.4229639e-09 2.1270491e-08 2.3615620e-08 7.4443198e-09\n",
      " 1.0185392e-08 5.8774752e-08 3.7580543e-09 6.6188576e-08 2.1585066e-08\n",
      " 3.3251915e-10 3.5347401e-08 8.0620616e-08 2.2517597e-08 1.1483524e-08\n",
      " 7.5355658e-08 3.9322372e-09 7.4136000e-09 8.9572694e-10 8.3740446e-08\n",
      " 6.8916393e-08 7.1541393e-08 1.3959450e-09 1.6381158e-09 6.7448596e-08\n",
      " 3.0353842e-08 2.0173931e-07 3.4556498e-09 7.2821984e-09 4.7196389e-09\n",
      " 5.9749084e-08 9.7774489e-08 3.4319978e-08 3.2011588e-08 2.7076048e-09\n",
      " 5.1461142e-09 7.7204749e-09 4.9836202e-09 9.5923136e-10 8.4298207e-10\n",
      " 3.2751335e-09 1.9333663e-07 2.3684352e-09 9.8674269e-09 2.0619842e-08\n",
      " 1.1264378e-08 5.2807865e-09 2.5412911e-08 1.1732106e-08 4.5175966e-08\n",
      " 4.8369305e-08 3.0387764e-07 1.8348861e-09 2.9889560e-09 2.1765219e-07\n",
      " 6.8920530e-10 1.7099017e-08 5.7017839e-09 1.0433536e-08 1.2602331e-07\n",
      " 4.5322488e-09 6.3046130e-08 3.0071918e-09 8.7932818e-08 1.1262487e-08\n",
      " 3.4855903e-09 2.5534482e-07 1.0235188e-08 2.3415399e-08 1.3454034e-07\n",
      " 3.7787498e-08 9.3180937e-08 3.2281186e-08 1.4751180e-06 6.4115227e-08\n",
      " 3.7406071e-08]\n",
      "Max prob per class: [0.99974746 0.99764687 0.999548   0.9999685  0.9962608  0.9978436\n",
      " 0.99666107 0.9958275  0.9309491  0.96701753 0.97764564 0.999584\n",
      " 0.99510694 0.99286944 0.9988445  0.99991095 0.97296214 0.99881494\n",
      " 0.8294533  0.99948055 0.99607784 0.99833244 0.99940133 0.9793482\n",
      " 0.9986327  0.9700542  0.99839765 0.9950321  0.9995234  0.9906927\n",
      " 0.9890085  0.9999794  0.9955432  0.99981266 0.99729854 0.99769056\n",
      " 0.9957604  0.9999808  0.9917225  0.9355741  0.91023713 0.9969193\n",
      " 0.9427204  0.94375324 0.94839907 0.9994443  0.9999341  0.99760723\n",
      " 0.9987336  0.99519557 0.97857    0.9842375  0.9471053  0.9998567\n",
      " 0.9977859  0.9988902  0.9937297  0.99571913 0.9869537  0.99945444\n",
      " 0.9750166  0.998811   0.9882371  0.99293    0.9985771  0.99927837\n",
      " 0.84251004 0.99321127 0.9226581  0.9998061  0.9985245  0.9915142\n",
      " 0.7544574  0.9999752  0.99969673 0.99899715 0.9994241  0.94381523\n",
      " 0.9999839  0.9985274  0.9964353 ]\n",
      "Epoch 6: 100%|██████████| 640/640 [03:38<00:00,  2.93it/s, v_num=0, train_loss_step=0.133, val_loss=0.123, val_ood_acc=0.920, val_acc_all=0.995, val_precision_all=0.904, val_recall_all=0.694, val_f1_all=0.785, avg_threshold=0.334, val_acc=0.995, val_precision=0.904, val_recall=0.694, val_f1=0.785, train_loss_epoch=0.146, train_acc=0.993, train_precision=0.899, train_recall=0.447, train_f1=0.597] Min prob per class: [5.1411039e-08 6.2506539e-10 1.7878297e-09 3.4693466e-09 8.8094311e-08\n",
      " 2.9147205e-09 5.9000207e-11 5.4917537e-10 1.3973550e-08 5.2707111e-10\n",
      " 5.6722258e-09 2.7703615e-08 4.4070200e-10 2.8049951e-09 1.5200321e-08\n",
      " 3.1397610e-10 3.3167813e-09 2.0256032e-08 2.8320897e-09 6.3929249e-08\n",
      " 2.7224152e-09 1.2452410e-09 4.3663653e-10 1.6679572e-09 3.0648426e-09\n",
      " 1.0635789e-08 4.3058619e-09 3.0893519e-09 6.9399011e-09 1.0183275e-08\n",
      " 1.3162178e-09 1.2089823e-08 2.1309383e-09 1.3268017e-09 1.1414426e-09\n",
      " 4.5930353e-09 1.6869597e-08 8.8573393e-10 7.6200241e-10 4.8684784e-10\n",
      " 3.7791171e-08 3.4031958e-10 1.8912623e-09 1.6884428e-07 1.4349595e-09\n",
      " 5.8990673e-10 8.2455465e-10 5.3729576e-09 1.5931667e-10 1.1406499e-10\n",
      " 7.2470869e-09 8.0194384e-09 2.0313959e-11 1.8756747e-09 1.0611356e-07\n",
      " 1.0199714e-09 1.1982312e-09 6.7175807e-11 6.1473587e-12 1.6317037e-10\n",
      " 4.7256782e-11 1.0108070e-09 1.8449764e-08 4.7211625e-08 6.4441807e-09\n",
      " 7.6312234e-10 6.3658028e-08 4.2507059e-10 3.1424517e-07 2.0159174e-08\n",
      " 9.6977981e-10 3.3824632e-09 1.2653604e-08 7.1722628e-10 1.4416623e-08\n",
      " 3.1740535e-09 2.4194213e-10 3.1597678e-09 2.5515567e-07 1.0382532e-09\n",
      " 8.2958487e-11]\n",
      "Max prob per class: [0.9996309  0.9990928  0.996874   0.9999918  0.9973979  0.99936074\n",
      " 0.98014796 0.9748948  0.98464406 0.99111545 0.9960089  0.99995446\n",
      " 0.99524105 0.9980883  0.99787045 0.9999348  0.9779213  0.99476284\n",
      " 0.74951196 0.9998933  0.9958462  0.9978816  0.99943095 0.982054\n",
      " 0.99919516 0.99617445 0.9971603  0.9987739  0.999479   0.9958902\n",
      " 0.9985397  0.9999776  0.9972223  0.99996626 0.9998418  0.99979\n",
      " 0.971924   0.99914205 0.98760146 0.9983456  0.9644068  0.99597967\n",
      " 0.96812975 0.99961036 0.97607034 0.9995603  0.99995244 0.9989477\n",
      " 0.9999902  0.9286002  0.99590105 0.9974826  0.9928316  0.99989307\n",
      " 0.9577978  0.9932742  0.9944659  0.9965011  0.9808522  0.9987469\n",
      " 0.99776304 0.9957047  0.9995289  0.99918956 0.9998349  0.9994522\n",
      " 0.96314305 0.9978946  0.98316115 0.9999403  0.99891853 0.99919695\n",
      " 0.89198065 0.9998933  0.9999565  0.99608874 0.9999589  0.99402833\n",
      " 0.9999738  0.95128876 0.9965365 ]\n",
      "Epoch 7: 100%|██████████| 640/640 [03:37<00:00,  2.94it/s, v_num=0, train_loss_step=0.092, val_loss=0.129, val_ood_acc=0.910, val_acc_all=0.995, val_precision_all=0.916, val_recall_all=0.678, val_f1_all=0.779, avg_threshold=0.349, val_acc=0.995, val_precision=0.916, val_recall=0.678, val_f1=0.779, train_loss_epoch=0.140, train_acc=0.993, train_precision=0.897, train_recall=0.451, train_f1=0.601] Min prob per class: [1.1193416e-09 5.0549609e-09 4.6060167e-10 8.7451030e-10 4.5083235e-09\n",
      " 2.8935812e-10 9.2049240e-10 3.4481979e-12 2.2224749e-09 1.4088108e-09\n",
      " 3.2991339e-09 8.0360660e-09 1.4674784e-10 4.0002618e-08 6.2266262e-11\n",
      " 4.2379176e-09 9.4516590e-09 2.5272008e-08 9.6746952e-08 1.2483379e-08\n",
      " 3.9181899e-10 2.6648175e-10 8.8073933e-13 9.1424325e-12 5.0934290e-10\n",
      " 2.2423821e-08 1.6892781e-08 7.5167428e-09 2.5603020e-10 2.2425576e-08\n",
      " 1.8710661e-08 3.3993977e-08 4.4583417e-09 6.7525635e-10 3.5453320e-09\n",
      " 1.5433411e-09 1.1970809e-07 4.8774593e-09 9.6422133e-09 1.4905218e-11\n",
      " 7.3742068e-10 1.0287616e-09 2.5183746e-09 2.2458405e-08 5.0972099e-10\n",
      " 2.1939620e-08 7.2522410e-10 1.8322387e-09 1.3981140e-09 3.3340921e-08\n",
      " 2.3687245e-09 7.3428419e-09 2.4874558e-08 2.7186804e-08 1.7651457e-08\n",
      " 1.0314185e-08 2.8396151e-08 4.2782122e-11 5.7318790e-09 3.1241726e-08\n",
      " 6.7335255e-11 3.3534231e-08 1.4443368e-09 7.1118786e-09 4.0256431e-08\n",
      " 1.3099241e-09 1.5421326e-08 4.9225370e-08 2.5755565e-08 1.8223570e-10\n",
      " 8.0215906e-10 2.2358307e-08 4.4073167e-09 8.3353182e-09 1.5139038e-08\n",
      " 4.0234934e-08 1.3425166e-08 7.0526318e-09 5.8118044e-08 8.8649110e-10\n",
      " 1.2271214e-09]\n",
      "Max prob per class: [0.9649912  0.99996054 0.9960989  0.9999757  0.9906503  0.9998803\n",
      " 0.9907974  0.99957734 0.9813306  0.9878928  0.9963806  0.9999187\n",
      " 0.98473823 0.99735403 0.9985185  0.999998   0.97049326 0.99431926\n",
      " 0.97158414 0.9999733  0.9993303  0.9975501  0.99922717 0.87342155\n",
      " 0.9966456  0.993737   0.9995321  0.9987447  0.9994324  0.99893695\n",
      " 0.99866617 0.9999682  0.99935645 0.99969256 0.9981275  0.9999577\n",
      " 0.97903264 0.99984074 0.999551   0.99900347 0.76996154 0.99967694\n",
      " 0.9550422  0.9998018  0.9046324  0.9995474  0.99981457 0.9925668\n",
      " 0.99984944 0.997168   0.99758816 0.9975916  0.9779959  0.9999225\n",
      " 0.94893    0.9144306  0.99213326 0.99810684 0.98888224 0.99854785\n",
      " 0.98342717 0.99984765 0.99652135 0.99954224 0.9999902  0.99999547\n",
      " 0.99642974 0.9929167  0.80149454 0.9997763  0.9992186  0.9952997\n",
      " 0.92846406 0.9990645  0.9999833  0.99925035 0.9997789  0.98856306\n",
      " 0.9999808  0.9974952  0.99953234]\n",
      "Epoch 8: 100%|██████████| 640/640 [03:41<00:00,  2.89it/s, v_num=0, train_loss_step=0.0841, val_loss=0.124, val_ood_acc=0.918, val_acc_all=0.995, val_precision_all=0.899, val_recall_all=0.712, val_f1_all=0.795, avg_threshold=0.352, val_acc=0.995, val_precision=0.899, val_recall=0.712, val_f1=0.795, train_loss_epoch=0.141, train_acc=0.993, train_precision=0.897, train_recall=0.456, train_f1=0.604]Min prob per class: [5.19740828e-09 5.44860858e-11 1.10723197e-09 3.49165874e-09\n",
      " 5.35986899e-08 7.15913526e-12 3.39724382e-09 5.92209615e-11\n",
      " 5.28890820e-10 6.10755779e-10 1.08296805e-09 1.14820896e-10\n",
      " 2.31671349e-10 8.85477647e-09 9.47537160e-10 6.03061434e-10\n",
      " 8.65622418e-09 2.03868744e-09 1.21504140e-08 2.50655390e-08\n",
      " 2.58004929e-09 1.39986106e-10 2.00091899e-09 3.08431475e-11\n",
      " 6.16272033e-10 5.96983352e-10 1.13539208e-07 5.18222132e-10\n",
      " 1.83987575e-10 3.71486508e-10 1.18157506e-09 4.09352285e-09\n",
      " 1.58184010e-09 1.50320159e-10 2.45067118e-12 3.77182563e-10\n",
      " 6.36659223e-08 4.54396416e-11 8.94088803e-09 5.60425761e-10\n",
      " 6.75547562e-10 9.08385189e-09 5.66313940e-11 2.27197774e-10\n",
      " 1.63396463e-10 2.86298207e-09 2.26076580e-09 7.57219787e-10\n",
      " 1.48243213e-08 2.40415488e-09 3.17241150e-10 1.15229737e-08\n",
      " 3.01279512e-09 1.81271709e-09 2.60921382e-08 3.87136018e-10\n",
      " 1.99572758e-09 1.57895241e-09 9.42742303e-13 1.57298032e-08\n",
      " 4.43507508e-10 1.95180157e-11 1.14786856e-08 5.84206294e-10\n",
      " 2.80779631e-08 2.18289671e-08 5.20641983e-08 1.26233213e-09\n",
      " 2.63432689e-08 8.75894246e-10 4.64586258e-10 1.24302524e-08\n",
      " 8.05488801e-11 1.67416445e-11 1.04505444e-08 2.50140051e-08\n",
      " 1.82140936e-09 1.41042594e-10 2.75705190e-07 1.17266463e-08\n",
      " 3.19292032e-12]\n",
      "Max prob per class: [0.9996276  0.9985304  0.99794513 0.9999981  0.9963773  0.99994874\n",
      " 0.99907506 0.9952057  0.95958704 0.9939521  0.9962774  0.9997185\n",
      " 0.9969222  0.9963325  0.9848141  0.99999785 0.9999045  0.9716703\n",
      " 0.7489735  0.999998   0.9996884  0.9996815  0.99969816 0.963733\n",
      " 0.996236   0.99140185 0.9998118  0.844795   0.99779874 0.99858046\n",
      " 0.9865935  0.9999485  0.99910814 0.99999654 0.99811685 0.9999838\n",
      " 0.995443   0.9998492  0.99990785 0.9996131  0.94585776 0.9999206\n",
      " 0.99356705 0.986684   0.9821362  0.99998844 0.9999987  0.9997094\n",
      " 0.99979633 0.9997085  0.9978642  0.9995796  0.96903795 0.9999931\n",
      " 0.99974054 0.9959133  0.9936341  0.9998287  0.9755849  0.999754\n",
      " 0.99840146 0.99782515 0.9997873  0.997122   0.9989813  0.9999846\n",
      " 0.99622655 0.98892576 0.968257   0.9999957  0.99969256 0.9911032\n",
      " 0.9340084  0.9926899  0.9999877  0.9995603  0.99710196 0.9933562\n",
      " 0.99999154 0.99597967 0.9868029 ]\n",
      "Epoch 9: 100%|██████████| 640/640 [03:34<00:00,  2.99it/s, v_num=0, train_loss_step=0.0766, val_loss=0.123, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.897, val_recall_all=0.724, val_f1_all=0.801, avg_threshold=0.365, val_acc=0.996, val_precision=0.897, val_recall=0.724, val_f1=0.801, train_loss_epoch=0.139, train_acc=0.993, train_precision=0.900, train_recall=0.457, train_f1=0.606]Min prob per class: [4.14197926e-10 3.76481589e-11 1.34771055e-10 1.50963553e-09\n",
      " 5.56055246e-09 2.46690973e-10 8.37495906e-11 4.80294485e-11\n",
      " 9.00067521e-12 1.28224695e-10 1.60654823e-09 2.70324430e-09\n",
      " 1.23536126e-10 5.25690359e-11 3.55178054e-10 4.05714871e-11\n",
      " 1.51880758e-10 3.51024876e-09 2.93128011e-08 8.25685333e-08\n",
      " 2.08418335e-11 8.47935090e-12 3.18742921e-10 2.38450947e-12\n",
      " 1.25595345e-09 2.76185519e-09 3.81275278e-09 3.08519738e-11\n",
      " 9.12444675e-10 6.60235547e-11 1.26643673e-09 8.02453304e-09\n",
      " 6.41216413e-10 4.34859621e-10 7.05919545e-10 4.12400993e-08\n",
      " 4.68975470e-10 1.50449309e-09 4.16321394e-10 2.69741918e-09\n",
      " 5.98785951e-11 2.03813944e-09 2.42342091e-10 1.47157975e-09\n",
      " 3.32593370e-10 3.91903454e-10 3.36452921e-10 1.14235743e-09\n",
      " 3.28695648e-08 7.21036286e-11 8.15988388e-11 2.01947614e-09\n",
      " 4.95134209e-08 3.69205111e-09 2.88552080e-08 8.66858318e-09\n",
      " 4.47974546e-09 2.33344122e-10 1.69525741e-10 2.69936984e-09\n",
      " 7.56427310e-10 8.49208148e-10 9.89648585e-10 2.66172761e-11\n",
      " 2.15208007e-09 4.26735695e-11 4.57596433e-10 1.93286520e-09\n",
      " 2.02795363e-08 4.39927073e-09 1.54099677e-09 7.22184534e-10\n",
      " 1.68126701e-09 1.39746625e-09 6.72130396e-09 1.56465596e-08\n",
      " 1.04203345e-07 1.06806855e-10 4.85345879e-07 8.61481220e-10\n",
      " 2.26052316e-10]\n",
      "Max prob per class: [0.9998511  0.98930675 0.998206   1.         0.9953921  0.9999957\n",
      " 0.9980428  0.9981743  0.83841217 0.98580223 0.99606675 0.9997706\n",
      " 0.9767799  0.99240595 0.9990357  0.99999905 0.9809156  0.98775417\n",
      " 0.9948768  0.9999647  0.99688333 0.99939835 0.9996654  0.9855597\n",
      " 0.99887055 0.99791235 0.9989526  0.9919886  0.99891317 0.9982311\n",
      " 0.9842332  0.9999933  0.9996288  0.99996793 0.9962728  0.999982\n",
      " 0.9370471  0.9992842  0.99968374 0.9993774  0.8131275  0.99980706\n",
      " 0.98207253 0.9995741  0.98817563 0.99992466 0.99996877 0.9978598\n",
      " 0.9999778  0.9924655  0.99763024 0.9963498  0.9945703  0.9999697\n",
      " 0.99896824 0.9970515  0.9935998  0.9993037  0.9987123  0.9998247\n",
      " 0.99880946 0.99833953 0.9993736  0.9994568  0.99977845 0.9993542\n",
      " 0.9892526  0.9897827  0.90512305 0.9999505  0.99997973 0.99931693\n",
      " 0.76864606 0.9994349  0.9999995  0.9937913  0.99933356 0.99416405\n",
      " 0.99999213 0.99995077 0.9977319 ]\n",
      "Epoch 10: 100%|██████████| 640/640 [03:33<00:00,  3.00it/s, v_num=0, train_loss_step=0.209, val_loss=0.132, val_ood_acc=0.913, val_acc_all=0.996, val_precision_all=0.894, val_recall_all=0.731, val_f1_all=0.804, avg_threshold=0.355, val_acc=0.996, val_precision=0.894, val_recall=0.731, val_f1=0.804, train_loss_epoch=0.142, train_acc=0.993, train_precision=0.895, train_recall=0.459, train_f1=0.607] Min prob per class: [6.1743712e-09 1.3787893e-10 2.4737616e-09 2.6006566e-09 2.5948023e-11\n",
      " 3.4518302e-12 1.2463119e-10 2.8168088e-11 1.9644533e-09 3.2674138e-10\n",
      " 1.0892595e-09 3.2177527e-09 1.4450222e-10 5.4676524e-10 1.6891862e-10\n",
      " 8.0341006e-10 2.6241677e-11 4.6797738e-10 3.3601587e-08 1.4567696e-10\n",
      " 8.0044998e-11 2.1670464e-11 5.8684413e-10 2.3647091e-11 1.1820191e-09\n",
      " 6.3439463e-09 6.6530631e-10 9.6183062e-11 9.4801800e-10 8.4807833e-10\n",
      " 1.8495313e-09 1.8090973e-09 1.2197701e-10 3.4931207e-11 2.8031752e-10\n",
      " 6.7579147e-09 9.1215746e-10 8.7185226e-10 3.4103484e-11 3.9787695e-10\n",
      " 1.0759153e-11 2.9313946e-10 8.1171965e-11 5.4328481e-10 5.4737154e-10\n",
      " 5.8561005e-09 2.9359268e-10 2.7836178e-09 6.9773294e-09 3.8765970e-11\n",
      " 2.8664821e-11 1.2337083e-10 1.9577515e-08 3.2193720e-10 1.5492642e-08\n",
      " 1.7227061e-10 8.2933923e-08 8.8126957e-11 1.6525413e-10 1.1958384e-09\n",
      " 1.8257970e-11 1.7838657e-08 2.7542735e-10 2.6439989e-10 2.7156730e-09\n",
      " 3.7102690e-10 1.5846433e-08 1.4421744e-11 6.6177694e-09 1.1465491e-08\n",
      " 2.1962963e-10 3.0654683e-09 1.9148091e-08 9.1375252e-10 1.3044905e-10\n",
      " 2.3630657e-09 6.1306005e-09 3.2352516e-11 4.9804676e-08 7.3482220e-09\n",
      " 9.1843484e-11]\n",
      "Max prob per class: [0.99948597 0.9999703  0.9997751  0.99999964 0.96947014 0.999803\n",
      " 0.9988617  0.9905262  0.9973252  0.9987998  0.99648607 0.9999231\n",
      " 0.9758066  0.9991671  0.9987418  0.99999654 0.9914352  0.99597365\n",
      " 0.98229295 0.9992071  0.9995628  0.9996606  0.9999653  0.99088657\n",
      " 0.99665666 0.9887045  0.99901557 0.9940441  0.99860746 0.99980277\n",
      " 0.98881185 0.99984336 0.99470735 0.99999356 0.9983991  0.99997354\n",
      " 0.95081735 0.999956   0.99963534 0.9988576  0.9234925  0.9988392\n",
      " 0.9749161  0.997131   0.9992631  0.9997092  0.9999776  0.9997998\n",
      " 0.9998714  0.95705783 0.97864425 0.9900485  0.977784   0.9998989\n",
      " 0.9948873  0.84763306 0.99742776 0.99941623 0.9991001  0.9996062\n",
      " 0.9843538  0.9995235  0.99874485 0.99950325 0.9995695  0.9997304\n",
      " 0.9955841  0.992021   0.9237599  0.99997425 0.9999231  0.99966955\n",
      " 0.9634681  0.96877563 0.99973935 0.9997738  0.9998908  0.8940177\n",
      " 0.9999925  0.99833846 0.9995577 ]\n",
      "Epoch 11: 100%|██████████| 640/640 [03:36<00:00,  2.96it/s, v_num=0, train_loss_step=0.218, val_loss=0.125, val_ood_acc=0.918, val_acc_all=0.996, val_precision_all=0.891, val_recall_all=0.727, val_f1_all=0.801, avg_threshold=0.355, val_acc=0.996, val_precision=0.891, val_recall=0.727, val_f1=0.801, train_loss_epoch=0.135, train_acc=0.993, train_precision=0.905, train_recall=0.458, train_f1=0.608] Min prob per class: [6.96719238e-10 1.45635282e-10 7.63306129e-09 1.86760776e-08\n",
      " 1.44089379e-10 3.20896499e-12 1.39592764e-11 1.31905389e-11\n",
      " 1.47912138e-09 3.77539555e-10 2.03614214e-09 3.77789711e-09\n",
      " 2.10807934e-11 1.51641658e-10 5.48308787e-10 9.40151689e-12\n",
      " 5.80488331e-11 9.56629151e-11 7.72203919e-08 1.78501960e-08\n",
      " 2.08575796e-11 2.21109468e-11 1.54907580e-11 7.98697306e-12\n",
      " 1.88365523e-09 2.31788260e-08 1.49519508e-10 1.35107189e-10\n",
      " 8.50144000e-12 1.24771360e-09 1.99346217e-11 1.16913386e-08\n",
      " 3.51671227e-11 6.67016234e-11 1.18375865e-09 9.86101067e-09\n",
      " 1.54363555e-09 2.53125049e-10 1.47846901e-10 1.52221222e-10\n",
      " 1.88766913e-09 5.87816817e-09 1.89116736e-10 4.46162302e-10\n",
      " 3.14423591e-11 5.21739985e-10 1.52789115e-09 1.46766943e-09\n",
      " 5.80351678e-09 5.98312511e-10 5.31319488e-10 1.92623972e-10\n",
      " 4.06706420e-08 1.05538349e-10 7.60678454e-09 3.96948391e-10\n",
      " 2.82997314e-09 1.78070492e-11 5.50250123e-10 1.56736113e-09\n",
      " 2.49304355e-10 2.24857843e-09 1.83284596e-10 2.59285233e-08\n",
      " 1.18565735e-08 4.46775478e-10 1.68279640e-10 6.80727419e-11\n",
      " 1.64871778e-08 3.11053516e-10 5.26305666e-10 1.55361335e-09\n",
      " 1.86391208e-10 2.54920168e-10 2.30157693e-09 6.59155397e-09\n",
      " 3.06687165e-09 1.31168920e-09 4.88419744e-07 6.47136685e-12\n",
      " 1.43321847e-11]\n",
      "Max prob per class: [0.99273974 0.9989392  0.99968743 0.9999999  0.99865294 0.99996877\n",
      " 0.99543214 0.9981232  0.96574044 0.9968362  0.998386   0.9999821\n",
      " 0.99894875 0.997531   0.99925584 0.99999094 0.990306   0.9994758\n",
      " 0.9589021  0.9999969  0.9985764  0.9994025  0.9999305  0.97834307\n",
      " 0.9980648  0.99770385 0.9986154  0.9908454  0.99993527 0.99946934\n",
      " 0.99541336 0.9999994  0.99940574 0.99992776 0.99986804 0.9999993\n",
      " 0.98916143 0.99993217 0.9998288  0.9988489  0.9932439  0.9998386\n",
      " 0.98978245 0.99850875 0.9932301  0.9998221  0.9999914  0.99949837\n",
      " 0.9999443  0.9984092  0.99421906 0.9993284  0.9984357  0.99967015\n",
      " 0.9907278  0.95161456 0.98445535 0.99975187 0.99710983 0.99967957\n",
      " 0.9971865  0.9999672  0.9981889  0.99954337 0.9999833  0.9999968\n",
      " 0.9927186  0.99681634 0.9676366  0.9999963  0.99998546 0.99687356\n",
      " 0.93142325 0.9998642  0.99999356 0.9997929  0.9999087  0.996126\n",
      " 0.9999765  0.9942021  0.9988494 ]\n",
      "Epoch 12: 100%|██████████| 640/640 [03:40<00:00,  2.90it/s, v_num=0, train_loss_step=0.136, val_loss=0.122, val_ood_acc=0.919, val_acc_all=0.996, val_precision_all=0.914, val_recall_all=0.720, val_f1_all=0.806, avg_threshold=0.364, val_acc=0.996, val_precision=0.914, val_recall=0.720, val_f1=0.806, train_loss_epoch=0.134, train_acc=0.993, train_precision=0.899, train_recall=0.456, train_f1=0.605] Min prob per class: [9.8940085e-11 8.6107010e-11 1.0133281e-09 1.4872489e-09 5.9984134e-10\n",
      " 2.9778510e-11 9.2176180e-12 3.9399661e-11 3.9430501e-10 6.4829678e-12\n",
      " 8.7668761e-11 1.4690214e-10 6.6490924e-10 2.2367454e-10 1.8790530e-11\n",
      " 1.4660034e-11 1.8731890e-10 1.9815003e-09 3.5821202e-08 2.6295378e-08\n",
      " 5.1843047e-11 6.5030273e-13 6.2875150e-11 2.6258450e-11 2.1647299e-10\n",
      " 4.2431991e-09 8.7388718e-09 1.9178920e-10 1.2957962e-10 1.2961381e-09\n",
      " 8.1779933e-10 1.8469156e-09 2.0043798e-09 1.4727697e-10 1.8015588e-10\n",
      " 4.3894395e-09 4.0205667e-10 4.1369419e-10 8.2202446e-11 1.4245884e-09\n",
      " 1.0561346e-10 9.1355173e-11 5.3402175e-11 3.4262909e-10 3.7112299e-11\n",
      " 1.4018890e-10 6.3140415e-10 3.5156208e-10 1.2698956e-09 6.0307148e-11\n",
      " 6.3868945e-11 6.3050637e-10 1.4914880e-08 1.7862107e-09 3.9144815e-08\n",
      " 7.6370812e-09 1.1681413e-09 7.1912171e-11 1.3612990e-11 1.2698043e-08\n",
      " 5.9688053e-11 2.9000663e-11 2.3524303e-09 2.0083748e-09 4.7754090e-09\n",
      " 3.3066019e-11 3.9443986e-09 6.3594102e-11 1.4356630e-09 1.0418483e-08\n",
      " 4.0507253e-10 7.5389517e-10 3.6151729e-10 9.0325650e-11 5.3055014e-09\n",
      " 3.8110969e-08 3.1814238e-09 2.2286756e-10 6.0672113e-07 7.5167459e-12\n",
      " 4.0812765e-12]\n",
      "Max prob per class: [0.9995285  0.999876   0.9989864  0.99999964 0.9968086  0.9999815\n",
      " 0.99895906 0.99834037 0.9962949  0.9971802  0.9957327  0.9999764\n",
      " 0.9938612  0.987298   0.9984419  0.9999728  0.99900323 0.9983432\n",
      " 0.8270177  0.99997115 0.99993575 0.99813783 0.9997738  0.9991836\n",
      " 0.998055   0.99975985 0.9998178  0.99330586 0.9999745  0.9996327\n",
      " 0.9976495  0.99999547 0.9998807  0.9999167  0.9990938  0.9997049\n",
      " 0.958741   0.99997175 0.9998518  0.99790597 0.9976343  0.9992623\n",
      " 0.992667   0.9992299  0.98842645 0.99982506 0.9999856  0.99967647\n",
      " 0.99989617 0.99241734 0.99841714 0.99943346 0.98911417 0.9999057\n",
      " 0.99980885 0.986439   0.99813074 0.999938   0.99687344 0.9999113\n",
      " 0.99479586 0.9990237  0.99810874 0.9992748  0.9999243  0.99998796\n",
      " 0.97928303 0.9973634  0.9070222  0.99999523 0.999966   0.9875422\n",
      " 0.89731896 0.9989992  0.99998367 0.9997862  0.999966   0.9774384\n",
      " 0.99998987 0.9986687  0.99813026]\n",
      "Epoch 13: 100%|██████████| 640/640 [03:39<00:00,  2.92it/s, v_num=0, train_loss_step=0.105, val_loss=0.122, val_ood_acc=0.921, val_acc_all=0.996, val_precision_all=0.914, val_recall_all=0.731, val_f1_all=0.813, avg_threshold=0.368, val_acc=0.996, val_precision=0.914, val_recall=0.731, val_f1=0.813, train_loss_epoch=0.134, train_acc=0.993, train_precision=0.903, train_recall=0.460, train_f1=0.609] Min prob per class: [1.44017545e-08 1.30684893e-10 1.23898181e-09 1.92860439e-09\n",
      " 9.79051631e-11 7.16749663e-12 2.16239318e-11 2.34313142e-11\n",
      " 1.22655913e-10 3.45084248e-12 1.22437205e-09 2.30148789e-10\n",
      " 4.30155050e-10 3.92775847e-11 6.61651289e-11 5.08977513e-11\n",
      " 2.94648299e-11 2.69151784e-10 1.19465353e-08 7.40367656e-10\n",
      " 1.44974296e-10 2.89229404e-11 2.06911606e-11 1.92820516e-11\n",
      " 8.27851343e-10 4.32353087e-09 3.66337988e-10 8.83093931e-10\n",
      " 9.27690702e-10 5.74614696e-11 6.80838108e-10 5.43780665e-09\n",
      " 7.71042008e-10 1.68543360e-10 5.56190441e-11 2.18143059e-09\n",
      " 3.84731136e-10 7.25406985e-13 1.26707630e-10 7.57494234e-10\n",
      " 2.23439496e-11 3.46410151e-10 3.10273751e-10 7.37374328e-10\n",
      " 1.74074177e-12 1.72477407e-10 5.13548024e-11 6.21026119e-10\n",
      " 3.00194962e-08 8.09560665e-12 2.03649847e-10 5.34327693e-10\n",
      " 3.12661497e-09 3.00473063e-10 7.75857512e-09 9.12304188e-09\n",
      " 1.13299903e-08 5.71590908e-12 3.21329852e-10 5.03475150e-10\n",
      " 2.19517703e-11 4.02463340e-10 9.28787949e-11 4.09280998e-10\n",
      " 1.26494726e-09 2.24186225e-10 1.99900407e-09 8.36480996e-10\n",
      " 5.10761851e-08 2.10813766e-09 4.40422226e-10 1.54379465e-09\n",
      " 3.21541527e-09 2.67134023e-11 1.53712487e-09 2.63643152e-09\n",
      " 4.67742023e-09 7.40850367e-11 1.79411643e-06 1.90626903e-10\n",
      " 2.80819069e-13]\n",
      "Max prob per class: [0.99967635 0.99948764 0.99945253 0.9999944  0.9963158  0.99995005\n",
      " 0.99973136 0.99747366 0.96742535 0.99700636 0.9974335  0.99992204\n",
      " 0.99555355 0.99654466 0.9998418  0.9999999  0.99863786 0.9942311\n",
      " 0.99165994 0.99983776 0.9998604  0.99930143 0.9999232  0.9965835\n",
      " 0.9945018  0.9970643  0.9993716  0.9986852  0.99969697 0.9990482\n",
      " 0.9989096  0.9999726  0.9992884  0.99993396 0.9999734  0.9999231\n",
      " 0.99646    0.999716   0.99976236 0.9973757  0.89281714 0.99913484\n",
      " 0.9854085  0.99984384 0.99768376 0.9999479  0.99993455 0.9993279\n",
      " 0.99949205 0.9976047  0.99968314 0.9988882  0.9664896  0.9998933\n",
      " 0.99723935 0.9983041  0.9848317  0.9998528  0.9937372  0.99911207\n",
      " 0.9971956  0.9959109  0.9952193  0.99961084 0.99970764 0.99997616\n",
      " 0.94612134 0.9953306  0.96275496 0.9999896  0.9997702  0.9978436\n",
      " 0.98326427 0.999585   0.9999987  0.9983065  0.99995553 0.98444426\n",
      " 0.9999949  0.9999825  0.99594706]\n",
      "Epoch 14: 100%|██████████| 640/640 [03:35<00:00,  2.97it/s, v_num=0, train_loss_step=0.133, val_loss=0.122, val_ood_acc=0.917, val_acc_all=0.996, val_precision_all=0.913, val_recall_all=0.723, val_f1_all=0.807, avg_threshold=0.360, val_acc=0.996, val_precision=0.913, val_recall=0.723, val_f1=0.807, train_loss_epoch=0.129, train_acc=0.993, train_precision=0.910, train_recall=0.460, train_f1=0.611] Min prob per class: [8.92162166e-09 5.48305623e-10 4.54765114e-10 3.18509086e-09\n",
      " 4.02111150e-10 2.53771478e-13 5.97097718e-12 2.69626292e-12\n",
      " 4.61663291e-10 3.93500727e-12 1.93042901e-10 5.26531541e-10\n",
      " 2.47581621e-12 3.36210570e-09 1.91028837e-09 6.73785541e-11\n",
      " 1.10814977e-11 8.63504712e-10 1.42721905e-08 2.06245243e-08\n",
      " 3.32948634e-11 1.76025444e-11 4.79248282e-12 2.48655541e-11\n",
      " 1.00838460e-09 5.79118420e-10 2.85110247e-09 2.77448741e-11\n",
      " 4.91094365e-09 3.65705494e-10 3.05236253e-10 7.30552907e-09\n",
      " 8.19813994e-10 1.48191057e-10 1.83049825e-10 6.79189593e-10\n",
      " 1.37211853e-09 2.89526514e-10 4.30839600e-11 7.43696438e-10\n",
      " 4.74203905e-11 6.38910369e-10 2.81466184e-10 9.60926616e-11\n",
      " 2.61389209e-11 5.69640002e-10 9.33451719e-11 4.04950434e-10\n",
      " 6.95182401e-09 1.35860725e-10 3.06919351e-10 1.29851596e-09\n",
      " 1.37749474e-08 1.80048018e-10 2.46598511e-08 2.74548428e-09\n",
      " 4.34355840e-09 2.69538862e-12 8.89153941e-12 5.23824983e-10\n",
      " 3.72588314e-11 1.25585401e-11 3.04951703e-10 9.95999463e-11\n",
      " 1.79905979e-09 1.44595960e-10 1.77766279e-09 4.57359706e-11\n",
      " 1.02618118e-08 1.20480836e-09 2.95951076e-11 1.41694656e-09\n",
      " 5.51885593e-10 6.43573528e-11 2.37112463e-09 1.12325607e-08\n",
      " 8.55872173e-09 1.04831359e-10 4.01630905e-06 1.24376634e-10\n",
      " 1.33382116e-11]\n",
      "Max prob per class: [0.9996055  0.9999597  0.9998012  0.9999995  0.9967691  0.99995494\n",
      " 0.9967885  0.99604475 0.9712035  0.9907616  0.9969221  0.99988914\n",
      " 0.98578787 0.9912617  0.9996141  0.9999987  0.99074405 0.9974806\n",
      " 0.9758646  0.9998845  0.99973637 0.9996082  0.9995697  0.9830071\n",
      " 0.9962429  0.98954403 0.9996753  0.9929951  0.9999455  0.9997147\n",
      " 0.9985839  0.99999785 0.9984609  0.99997485 0.99988544 0.9999895\n",
      " 0.99932337 0.99999154 0.99978644 0.9986702  0.944397   0.9996244\n",
      " 0.9709707  0.99659747 0.9993438  0.9999548  0.9999989  0.9994708\n",
      " 0.9996978  0.99793917 0.9962035  0.9986248  0.9902726  0.99994946\n",
      " 0.99913615 0.97707987 0.99863523 0.9982135  0.9963728  0.99955255\n",
      " 0.9975923  0.9943506  0.99921465 0.99978274 0.9999218  0.99997306\n",
      " 0.9846304  0.9920209  0.9077022  0.9999912  0.99999166 0.99975187\n",
      " 0.8374308  0.9985688  0.9999981  0.99860114 0.99756575 0.9870073\n",
      " 0.9999956  0.9985531  0.9975073 ]\n",
      "Epoch 15: 100%|██████████| 640/640 [03:38<00:00,  2.93it/s, v_num=0, train_loss_step=0.187, val_loss=0.118, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.906, val_recall_all=0.735, val_f1_all=0.811, avg_threshold=0.362, val_acc=0.996, val_precision=0.906, val_recall=0.735, val_f1=0.811, train_loss_epoch=0.127, train_acc=0.993, train_precision=0.908, train_recall=0.465, train_f1=0.615] Min prob per class: [1.3390544e-11 2.4761776e-10 8.0711621e-10 1.0941089e-09 1.6076203e-10\n",
      " 8.1440982e-12 2.9351192e-11 1.1668914e-10 2.3407340e-10 4.2651296e-10\n",
      " 1.4809864e-10 2.4245028e-10 2.0240254e-10 6.2574412e-10 1.0291090e-09\n",
      " 8.7789533e-12 5.6003441e-11 1.2305733e-10 9.2382244e-09 3.7783892e-08\n",
      " 4.6973616e-11 7.3000890e-11 1.9674587e-11 3.1451296e-11 7.4733231e-10\n",
      " 3.1820366e-09 3.0100711e-10 2.6151900e-10 6.9049350e-10 3.0468383e-09\n",
      " 1.1372435e-10 1.2068858e-08 1.3732181e-09 1.1185792e-10 1.5007064e-10\n",
      " 6.3633725e-09 3.2970391e-09 3.0926504e-11 6.7101533e-11 1.2717742e-09\n",
      " 3.5087190e-11 1.0754568e-09 1.3244142e-10 3.5262854e-10 5.5764678e-11\n",
      " 1.5951553e-10 7.2681763e-11 2.7291165e-09 6.2633672e-09 1.5399081e-11\n",
      " 2.2578828e-09 2.9107370e-09 6.5487589e-09 3.2093188e-09 3.3915612e-08\n",
      " 6.5816130e-10 5.1138640e-09 2.2918016e-11 3.0062066e-12 4.0199072e-10\n",
      " 8.4140221e-12 1.2628403e-09 4.0328327e-10 3.9440581e-10 6.4870349e-09\n",
      " 7.9392853e-11 8.7583283e-09 5.8620253e-12 4.2951349e-08 4.0301257e-10\n",
      " 1.5752155e-10 7.6101470e-10 3.7904378e-11 2.2660865e-11 2.5604594e-09\n",
      " 5.8427347e-09 6.2785213e-09 1.2522268e-09 5.8167620e-07 9.5280372e-10\n",
      " 1.3379471e-10]\n",
      "Max prob per class: [0.9939534  0.9999591  0.9998685  0.9999956  0.9724681  0.99998593\n",
      " 0.9991259  0.9973099  0.9337508  0.99722606 0.9980246  0.9999614\n",
      " 0.9949122  0.99701643 0.99870527 0.9999958  0.99873954 0.9968432\n",
      " 0.9728908  0.99996245 0.99902725 0.99987376 0.99959    0.98265785\n",
      " 0.99893814 0.986402   0.9996729  0.98853904 0.999622   0.99933535\n",
      " 0.9989348  0.9998884  0.9994185  0.99999857 0.9999254  0.9999182\n",
      " 0.9635506  0.999841   0.9988514  0.9994435  0.9860633  0.9994406\n",
      " 0.98416096 0.9993044  0.974498   0.99982953 0.9999918  0.99919397\n",
      " 0.9999788  0.9974605  0.9988611  0.99933654 0.99714607 0.9999838\n",
      " 0.9985964  0.97054243 0.99804544 0.99896455 0.99850464 0.9996146\n",
      " 0.99281454 0.99973994 0.9969523  0.9995555  0.99987566 0.99987113\n",
      " 0.9959751  0.9965938  0.83220094 0.9999933  0.9999677  0.9998375\n",
      " 0.98451716 0.99815303 0.9999484  0.99964345 0.99727315 0.99177355\n",
      " 0.999944   0.99990594 0.99939823]\n",
      "Epoch 16: 100%|██████████| 640/640 [03:32<00:00,  3.01it/s, v_num=0, train_loss_step=0.115, val_loss=0.119, val_ood_acc=0.926, val_acc_all=0.996, val_precision_all=0.918, val_recall_all=0.730, val_f1_all=0.813, avg_threshold=0.355, val_acc=0.996, val_precision=0.918, val_recall=0.730, val_f1=0.813, train_loss_epoch=0.127, train_acc=0.993, train_precision=0.907, train_recall=0.465, train_f1=0.615] Min prob per class: [4.0350270e-09 1.7335239e-10 2.7300887e-10 2.5190858e-09 3.8568407e-10\n",
      " 7.3727754e-13 2.0119387e-11 1.8800964e-11 4.8115478e-10 2.8321947e-11\n",
      " 1.8860449e-10 4.5651197e-10 9.3234977e-12 9.2135619e-11 2.8995314e-10\n",
      " 1.0178050e-12 1.2049295e-10 7.7054030e-11 1.4110564e-09 3.1630663e-09\n",
      " 1.0745767e-10 4.2906418e-11 5.1310033e-12 1.6432118e-11 4.5810054e-09\n",
      " 4.2498405e-09 5.6029004e-10 2.0748159e-10 2.1746246e-10 1.1721446e-10\n",
      " 2.3430521e-10 1.4676982e-08 1.5068299e-10 8.9828776e-11 6.1208968e-11\n",
      " 7.6755455e-09 2.6380413e-09 1.7141319e-10 3.9846605e-11 3.1386649e-10\n",
      " 1.2297825e-10 4.3403819e-11 6.5098621e-10 1.9298592e-09 4.0480428e-11\n",
      " 1.8053599e-10 3.4957817e-10 1.1000936e-09 3.9690389e-09 3.2799749e-11\n",
      " 1.7834515e-11 1.3041945e-10 9.5998960e-09 2.9183569e-10 4.1242831e-09\n",
      " 7.5824484e-09 6.0170796e-10 5.1307097e-12 1.2889534e-10 3.0626965e-10\n",
      " 2.9819831e-11 1.3189816e-10 3.1811065e-10 6.0515626e-10 4.0311194e-09\n",
      " 1.5185547e-11 1.6725831e-09 1.4428959e-10 7.6123579e-09 9.5748527e-11\n",
      " 8.3023387e-11 2.6889331e-09 4.3602095e-09 4.3819982e-11 3.2219287e-09\n",
      " 3.4539165e-09 4.1925325e-09 1.0791375e-08 1.4106286e-06 1.2277311e-11\n",
      " 1.2963000e-11]\n",
      "Max prob per class: [0.9998233  0.99948347 0.9995316  0.9999982  0.98571825 0.99998724\n",
      " 0.99880445 0.9922161  0.97739035 0.99222046 0.9990965  0.9999434\n",
      " 0.99600655 0.9988965  0.99864    0.99999666 0.99764854 0.9987727\n",
      " 0.86248744 0.99998724 0.9996518  0.9964265  0.9997317  0.9843255\n",
      " 0.99837637 0.9897696  0.999663   0.9974656  0.9998281  0.9962147\n",
      " 0.99960905 0.9999902  0.9993734  0.99999833 0.9996486  0.99997115\n",
      " 0.99171865 0.99989057 0.99977595 0.99851686 0.9581184  0.9996474\n",
      " 0.9756487  0.99944395 0.9995511  0.99992025 0.99999774 0.99975127\n",
      " 0.9997447  0.9973627  0.9982192  0.9994136  0.9890816  0.9999491\n",
      " 0.9996613  0.9993705  0.99729687 0.9997588  0.9833413  0.99970067\n",
      " 0.99781287 0.9997912  0.99970216 0.99985826 0.99949574 0.99951375\n",
      " 0.9812408  0.995741   0.9223866  0.9999757  0.9999745  0.99979967\n",
      " 0.9010467  0.9995933  0.99997413 0.9994104  0.99104077 0.9971967\n",
      " 0.99999666 0.9997793  0.9981996 ]\n",
      "Epoch 17: 100%|██████████| 640/640 [03:36<00:00,  2.96it/s, v_num=0, train_loss_step=0.0669, val_loss=0.116, val_ood_acc=0.926, val_acc_all=0.996, val_precision_all=0.914, val_recall_all=0.730, val_f1_all=0.812, avg_threshold=0.359, val_acc=0.996, val_precision=0.914, val_recall=0.730, val_f1=0.812, train_loss_epoch=0.124, train_acc=0.993, train_precision=0.915, train_recall=0.465, train_f1=0.616]Min prob per class: [1.0193739e-10 3.2476247e-10 1.1669120e-09 2.9090663e-09 2.0390403e-10\n",
      " 1.9445509e-12 4.3250525e-11 5.3320521e-12 8.6632235e-10 5.7680759e-11\n",
      " 5.0978586e-11 4.6624982e-10 1.9739925e-10 1.9323712e-09 2.9443217e-10\n",
      " 2.6057732e-11 4.5749148e-11 4.5720101e-11 8.9690737e-08 2.2186471e-08\n",
      " 2.6878960e-10 2.0136844e-12 2.2503482e-11 3.4607467e-10 1.6136668e-09\n",
      " 8.1156406e-09 1.1124266e-09 3.0695974e-10 6.8335687e-10 4.2857445e-10\n",
      " 1.6159152e-09 6.5612866e-09 3.8893747e-10 7.4586420e-10 7.6081842e-11\n",
      " 2.7584124e-09 8.3192569e-10 2.9071734e-10 7.2975834e-11 1.3475916e-11\n",
      " 3.8853837e-11 4.2434314e-10 1.2926563e-10 2.1302231e-09 3.4286754e-11\n",
      " 3.4906765e-11 9.9853875e-11 3.1117223e-10 2.9743688e-09 9.4327032e-11\n",
      " 1.1285641e-09 2.1499087e-10 1.7262790e-09 1.4964254e-09 2.0391591e-08\n",
      " 1.8181882e-09 1.8643476e-08 5.2067399e-11 5.1427309e-12 9.1141661e-10\n",
      " 1.4319861e-11 3.1884065e-11 1.9759062e-10 1.5198971e-10 2.4743469e-09\n",
      " 3.0070224e-11 3.2171270e-09 3.9038876e-11 1.4259402e-08 1.3415253e-09\n",
      " 3.0375993e-10 6.5922418e-10 2.3284763e-09 1.0667634e-10 1.2579433e-09\n",
      " 1.8174608e-08 5.9463354e-09 1.2711897e-09 4.0710464e-07 4.3231967e-11\n",
      " 8.1505046e-11]\n",
      "Max prob per class: [0.99991655 0.99992645 0.99986684 0.9999999  0.9973078  0.9999641\n",
      " 0.98067665 0.99746025 0.98185927 0.9931959  0.9991079  0.9999882\n",
      " 0.98029226 0.99753463 0.99923813 0.99999917 0.98727    0.99937016\n",
      " 0.9436271  0.9999442  0.99983835 0.9999418  0.9999161  0.9990595\n",
      " 0.99747854 0.99800116 0.99631506 0.99333715 0.99985087 0.99952734\n",
      " 0.9988894  0.9999927  0.9998148  0.9999831  0.9991019  0.99993074\n",
      " 0.99799746 0.99997747 0.99979347 0.9967547  0.9333797  0.99979466\n",
      " 0.9925436  0.9992704  0.9843695  0.99994445 0.9999994  0.99952877\n",
      " 0.9998834  0.9986249  0.99847704 0.9996024  0.9898142  0.9999113\n",
      " 0.99897146 0.9973212  0.99408615 0.99972385 0.9979856  0.9998381\n",
      " 0.99285734 0.9981244  0.999461   0.9989077  0.9999521  0.9999821\n",
      " 0.98677725 0.9955902  0.96577936 0.999985   0.9995152  0.99896467\n",
      " 0.85021484 0.9999789  0.99997044 0.99766684 0.99967515 0.99706644\n",
      " 0.99999    0.99993277 0.9994122 ]\n",
      "Epoch 18: 100%|██████████| 640/640 [03:39<00:00,  2.91it/s, v_num=0, train_loss_step=0.0394, val_loss=0.118, val_ood_acc=0.927, val_acc_all=0.996, val_precision_all=0.915, val_recall_all=0.736, val_f1_all=0.816, avg_threshold=0.364, val_acc=0.996, val_precision=0.915, val_recall=0.736, val_f1=0.816, train_loss_epoch=0.123, train_acc=0.993, train_precision=0.917, train_recall=0.466, train_f1=0.618]Min prob per class: [2.81619228e-10 1.89369666e-11 5.86829585e-10 1.26025834e-09\n",
      " 3.76638359e-10 5.21054987e-13 2.12614561e-11 1.04224865e-10\n",
      " 1.68300179e-10 1.66896369e-12 6.32058086e-11 9.89935578e-10\n",
      " 8.96926197e-12 8.98821184e-10 1.15830590e-09 6.43549927e-12\n",
      " 6.25476546e-11 1.00733831e-10 3.90706667e-08 2.38632509e-08\n",
      " 2.10906067e-11 5.92373915e-13 6.58075330e-12 2.28339778e-11\n",
      " 5.11638110e-09 2.93994629e-09 2.48116999e-10 1.38075787e-10\n",
      " 5.59309876e-10 1.59667113e-10 5.34546096e-09 1.24915482e-08\n",
      " 3.30669825e-10 4.15936424e-10 2.53275127e-11 1.61186815e-08\n",
      " 2.09636930e-09 7.91568269e-11 5.03629749e-10 8.16624046e-10\n",
      " 4.53917390e-11 4.95543773e-10 1.12268306e-10 4.15658841e-10\n",
      " 1.98816900e-11 7.91198773e-10 1.13179272e-10 7.31157523e-10\n",
      " 7.26158778e-09 1.13262198e-10 5.97694283e-10 2.24493135e-09\n",
      " 1.13799556e-08 3.91101485e-10 1.73989907e-08 3.85648069e-10\n",
      " 5.52284929e-09 1.57451482e-11 1.14390867e-11 2.43381693e-09\n",
      " 2.15631003e-11 4.66009523e-11 2.35034353e-10 1.57252655e-09\n",
      " 9.16446208e-09 7.73418898e-11 1.25360859e-08 1.22459334e-10\n",
      " 1.90175697e-08 7.11281145e-10 9.12924597e-11 7.45146522e-09\n",
      " 7.54774243e-10 1.31934144e-10 1.75102755e-09 1.27195925e-08\n",
      " 2.02660022e-08 7.45635165e-10 1.12751457e-06 9.60250109e-12\n",
      " 3.56816347e-11]\n",
      "Max prob per class: [0.999435   0.9990004  0.9996567  0.99999857 0.9755427  0.99991643\n",
      " 0.99873453 0.9965778  0.974238   0.99827003 0.99842143 0.99996614\n",
      " 0.9925649  0.99419147 0.9986816  0.99999774 0.99917287 0.9959895\n",
      " 0.88127834 0.99998724 0.99866414 0.99984217 0.9994382  0.98402876\n",
      " 0.9985505  0.99911016 0.99978703 0.9729322  0.9996824  0.9994017\n",
      " 0.99920374 0.99997175 0.9986871  0.9999919  0.99996305 0.9998079\n",
      " 0.97820026 0.9999236  0.9999058  0.9973572  0.92664105 0.99834764\n",
      " 0.97981757 0.9985739  0.9979511  0.99984586 0.99997866 0.9979474\n",
      " 0.9999491  0.99408156 0.9992618  0.99915385 0.91041553 0.99980634\n",
      " 0.9998338  0.9982488  0.9977053  0.99703586 0.99797696 0.9998022\n",
      " 0.99533457 0.9993369  0.9994941  0.99973017 0.999959   0.99976546\n",
      " 0.98283696 0.99595094 0.94154125 0.9999536  0.9999329  0.9993554\n",
      " 0.95405185 0.99961    0.9998803  0.99900705 0.99984574 0.98415077\n",
      " 0.99996555 0.99992967 0.99901235]\n",
      "Epoch 19: 100%|██████████| 640/640 [03:37<00:00,  2.95it/s, v_num=0, train_loss_step=0.063, val_loss=0.117, val_ood_acc=0.925, val_acc_all=0.996, val_precision_all=0.915, val_recall_all=0.732, val_f1_all=0.814, avg_threshold=0.362, val_acc=0.996, val_precision=0.915, val_recall=0.732, val_f1=0.814, train_loss_epoch=0.124, train_acc=0.993, train_precision=0.917, train_recall=0.463, train_f1=0.615] Min prob per class: [1.23364957e-10 5.38204259e-10 1.93719818e-09 2.60586530e-09\n",
      " 1.68733472e-10 8.20486908e-13 6.13092146e-11 1.93770607e-11\n",
      " 1.64390238e-10 4.10350920e-11 7.66650021e-10 2.79633483e-10\n",
      " 3.58348178e-11 1.44489268e-10 1.57486790e-09 1.94277979e-11\n",
      " 1.89875393e-10 4.23489924e-11 3.91630950e-09 1.94458085e-08\n",
      " 1.49115650e-10 4.32039404e-11 4.00310861e-12 1.88297891e-10\n",
      " 2.75300893e-09 3.52826524e-09 1.64055469e-09 8.51351628e-11\n",
      " 1.19579957e-09 1.33163577e-10 1.03267138e-10 1.13285425e-08\n",
      " 1.45158074e-09 7.11652903e-10 2.75664908e-10 4.27832259e-09\n",
      " 2.01193684e-09 4.89323249e-10 3.84555832e-10 2.21612106e-09\n",
      " 5.00713915e-11 2.56785287e-10 1.01243747e-09 9.98055638e-10\n",
      " 1.89145602e-10 1.00728446e-10 1.32229216e-09 8.86597684e-10\n",
      " 3.77395587e-10 6.25340543e-11 3.48126306e-10 2.35453435e-10\n",
      " 4.72744066e-09 2.40731518e-10 5.25988000e-08 3.77441811e-09\n",
      " 3.01892134e-09 4.75656623e-12 9.98227889e-12 4.55745469e-10\n",
      " 1.88125487e-11 1.56861572e-11 1.73073209e-10 1.66254413e-10\n",
      " 1.23808697e-08 2.72628864e-11 1.41373748e-08 8.32627245e-10\n",
      " 2.80277188e-08 4.55421340e-10 3.10404411e-11 2.61453814e-09\n",
      " 5.49095036e-09 7.58155830e-11 8.57314442e-10 1.61800706e-08\n",
      " 1.94742764e-08 2.82110085e-10 8.88910336e-07 5.44066459e-11\n",
      " 2.44771412e-12]\n",
      "Max prob per class: [0.99914587 0.99909794 0.9997422  0.9999989  0.99097157 0.9999584\n",
      " 0.9980781  0.9962674  0.99147016 0.99683553 0.9977368  0.99993384\n",
      " 0.9985672  0.9993887  0.9986518  0.9999957  0.9818189  0.999121\n",
      " 0.88857883 0.9999918  0.99933404 0.9998253  0.9998165  0.9950795\n",
      " 0.99898535 0.99597675 0.9974746  0.99153465 0.999433   0.99932826\n",
      " 0.9988147  0.99995923 0.9994548  0.99998784 0.99994683 0.99998283\n",
      " 0.97262436 0.9999962  0.99911684 0.9978188  0.8289181  0.99904174\n",
      " 0.982726   0.99872106 0.999572   0.9998834  0.9999988  0.9990275\n",
      " 0.9998772  0.9969747  0.99862325 0.99943405 0.9785906  0.99999607\n",
      " 0.9988697  0.99792564 0.9975062  0.999691   0.9918898  0.99956423\n",
      " 0.97234374 0.99968994 0.99973565 0.9992829  0.999584   0.9999846\n",
      " 0.9372068  0.9971928  0.9160965  0.99995613 0.9998338  0.99950886\n",
      " 0.9859137  0.99913824 0.9999989  0.99966574 0.99840397 0.9912683\n",
      " 0.9999634  0.98838145 0.99616027]\n",
      "Epoch 19: 100%|██████████| 640/640 [04:25<00:00,  2.41it/s, v_num=0, train_loss_step=0.063, val_loss=0.123, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.911, val_recall_all=0.734, val_f1_all=0.813, avg_threshold=0.370, val_acc=0.996, val_precision=0.911, val_recall=0.734, val_f1=0.813, train_loss_epoch=0.121, train_acc=0.993, train_precision=0.922, train_recall=0.467, train_f1=0.620]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`Trainer.fit` stopped: `max_epochs=20` reached.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 19: 100%|██████████| 640/640 [04:25<00:00,  2.41it/s, v_num=0, train_loss_step=0.063, val_loss=0.123, val_ood_acc=0.922, val_acc_all=0.996, val_precision_all=0.911, val_recall_all=0.734, val_f1_all=0.813, avg_threshold=0.370, val_acc=0.996, val_precision=0.911, val_recall=0.734, val_f1=0.813, train_loss_epoch=0.121, train_acc=0.993, train_precision=0.922, train_recall=0.467, train_f1=0.620]\n",
      "Best model saved at: with_ood/ood_4/version_0/checkpoints/epoch=17-val_f1=0.8157.ckpt\n"
     ]
    }
   ],
   "source": [
    "# KFold Cross-Validation\n",
    "#lr = 0.008274600824983372\n",
    "#threshold = 0.3527\n",
    "\n",
    "# or 0.3395    es batch 51\n",
    "from iterstrat.ml_stratifiers import MultilabelStratifiedKFold\n",
    "from sklearn.naive_bayes import LabelBinarizer\n",
    "lb = LabelBinarizer()\n",
    "\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=42)  # 5-fold cross-validation\n",
    "val_losses = []\n",
    "\n",
    "# Assuming you have `train_dataset` defined as an ImageFolder dataset\n",
    "for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(train_dataset)) ,lb.fit_transform(np.array([train_dataset[i][1] for i in range(len(train_dataset))])))):\n",
    "    print(f\"Fold {fold + 1}/{kf.n_splits}\")\n",
    "\n",
    "    # Split the dataset into train and validation subsets using indices\n",
    "    train_subset = Subset(train_dataset, train_idx)\n",
    "    val_subset = Subset(train_dataset, val_idx)\n",
    "    # In your training_step:\n",
    "    # print(f\"x dtype: {train_dataset[1]}\")\n",
    "\n",
    "    # Create Data Module for each fold\n",
    "    data_module = MyDataModule(\n",
    "        train_data=train_subset,\n",
    "        val_data=val_subset,\n",
    "        test_dataset=test_dataset,\n",
    "        batch_size=batch_size,\n",
    "        num_workers=1,\n",
    "        persistent_workers=True\n",
    "    )\n",
    "    \n",
    "    data_module.setup()\n",
    "    print(\"Setup ran successfully\")\n",
    "\n",
    "    # Setup the model for multilabel classification\n",
    "    model = ResNetWrapper(lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "    # Logger\n",
    "    logger = TensorBoardLogger(\"with_ood\", name=f\"ood_{fold}\")\n",
    "\n",
    "    # ModelCheckpoint to save only the best models (monitor F1 score for multilabel)\n",
    "    checkpoint_callback = ModelCheckpoint(\n",
    "        monitor=\"val_f1\",  # Monitor validation F1 score\n",
    "        mode=\"max\",        # Maximize the F1 score\n",
    "        save_top_k=1,      # Save only the best model\n",
    "        filename=\"{epoch}-{val_f1:.4f}\"\n",
    "    )\n",
    "\n",
    "    # EarlyStopping based on F1 score\n",
    "    early_stopping = EarlyStopping(\n",
    "        monitor=\"val_f1\",  # Use F1 for stopping\n",
    "        patience=3,        # Stop after 3 non-improving epochs\n",
    "        mode=\"max\"\n",
    "    )\n",
    "\n",
    "    # Setup the Trainer\n",
    "    trainer = Trainer(\n",
    "        logger=logger,\n",
    "        max_epochs=20,\n",
    "        devices='auto',  # Adjust based on your hardware\n",
    "        accelerator=\"mps\",  # Use \"gpu\" or \"tpu\" based on availability\n",
    "        callbacks=[early_stopping, checkpoint_callback], \n",
    "        fast_dev_run=False    \n",
    "    )\n",
    "\n",
    "    # Train the model\n",
    "    trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "    # Log validation loss for this fold\n",
    "    try:\n",
    "        val_losses.append(trainer.callback_metrics[\"val_loss\"].item())\n",
    "    except KeyError:\n",
    "        print(f\"Warning: val_loss not found in callback metrics for fold {fold + 1}\")\n",
    "        print(\"Available metrics:\", trainer.callback_metrics.keys())\n",
    "\n",
    "# After cross-validation, retrieve the best model path\n",
    "best_model_path = checkpoint_callback.best_model_path\n",
    "print(\"Best model saved at:\", best_model_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T16:42:44.260816Z",
     "iopub.status.busy": "2025-01-03T16:42:44.260531Z",
     "iopub.status.idle": "2025-01-03T16:42:44.523653Z",
     "shell.execute_reply": "2025-01-03T16:42:44.522644Z",
     "shell.execute_reply.started": "2025-01-03T16:42:44.260794Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "image, label = train_dataset[3]\n",
    "print(idx_to_label.get(image))\n",
    "\n",
    "plt.imshow(image.permute(1, 2, 0))  # A csatornákat (C, H, W) átrendezzük (H, W, C) formátumba\n",
    "plt.title(f\"Label: {label}\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## test "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.313Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install pretty-confusion-matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "Macro F1: 0.3046\n",
      "Macro Precision: 0.6973\n",
      "Macro Recall: 0.2112\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "\n",
    "best_model_path=\"/kaggle/working/lightning_logs/multilabel_training_fold_0/version_5/checkpoints/epoch=22-val_loss=0.04.ckpt\"\n",
    "best_model_path=\"lightning_logs/multilabel_training_fold_4/version_10/checkpoints/epoch=17-val_f1=0.6584.ckpt\"\n",
    "best_model_path=\"/kaggle/input/may_be_the_best_resnet/pytorch/default/1/version_10/checkpoints/epoch=17-val_f1=0.6584.ckpt\"\n",
    "best_model_path=\"lightning_logs/multilabel_training_fold_4/version_0/checkpoints/epoch=22-val_f1=0.6924.ckxpt\"\n",
    "best_model_path=\"lightning_logs/multilabel_training_fold_4/version_0/checkpoints/epoch=15-val_f1=0.6850.ckpt\" \n",
    "best_model_path=\"with_ood/ood_4/version_0/checkpoints/epoch=17-val_f1=0.8157.ckpt\"\n",
    "best_model_path=\"with_ood_80_classes_onlyID_metrics/ood_fold_0/version_4/checkpoints/epoch=13-val_f1=0.7128.ckpt\"\n",
    "\n",
    "best_model = ResNetWrapper.load_from_checkpoint(best_model_path, lr=lr, dropout_rate=0.1)\n",
    "\n",
    "mxs = []\n",
    "f1s = []\n",
    "from sklearn.preprocessing import MultiLabelBinarizer\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support, multilabel_confusion_matrix\n",
    "from torch.utils.data import DataLoader, Dataset, Subset\n",
    "import lightning.pytorch as pl\n",
    "from lightning.pytorch import Trainer\n",
    "from lightning.pytorch.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from lightning.pytorch.loggers import TensorBoardLogger\n",
    "from torchmetrics.classification import MultilabelF1Score\n",
    "from tqdm import tqdm\n",
    "from sklearn.model_selection import KFold\n",
    "def evaluate_model(model, test_loader, num_classes=80):\n",
    "    \"\"\"Evaluate the model with stored thresholds\"\"\"\n",
    "    model.eval()\n",
    "    if(test_loader==None):\n",
    "        print(\"test_loader is None\")\n",
    "        return None\n",
    "    device = next(model.parameters()).device\n",
    "    all_preds = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            #print(len(batch))\n",
    "            x, y_class= batch\n",
    "            x, y_class = x.to(device), y_class.to(device)\n",
    "            class_logits, ood_logits = best_model(x)\n",
    "        \n",
    "            #ood_probs = torch.sigmoid(ood_logits).cpu().numpy()\n",
    "\n",
    "            # Get predictions using model's get_preds method\n",
    "            preds = best_model._get_preds(class_logits, ood_logits).cpu().numpy()\n",
    "            #preds = (class_probs > model.sigmoid_thresholds.to(device)).cpu().numpy()\n",
    "\n",
    "            # Convert targets to multi-hot encoding if needed\n",
    "            targets = y_class.cpu().numpy()\n",
    "            if targets.ndim == 1:  # If targets are in index format instead of multi-hot\n",
    "                multi_hot_targets = np.zeros((targets.shape[0], num_classes))\n",
    "                for i, labels in enumerate(targets):\n",
    "                    multi_hot_targets[i, labels] = 1  # Convert to one-hot\n",
    "            else:\n",
    "                multi_hot_targets = targets  # Already in multi-hot format\n",
    "\n",
    "            all_preds.append(preds)\n",
    "            all_targets.append(multi_hot_targets)\n",
    "\n",
    "    # Concatenate along the first axis\n",
    "    all_preds = np.concatenate(all_preds, axis=0)  # Shape (N, 80)\n",
    "    all_targets = np.concatenate(all_targets, axis=0)  # Shape (N, 80)\n",
    "\n",
    "    #print(f\"Preds shape: {all_preds.shape}, Targets shape: {all_targets.shape}\")\n",
    "\n",
    "    # Ensure correct shape before computing metrics\n",
    "    assert all_preds.shape == all_targets.shape, f\"Shape mismatch: {all_preds.shape} vs {all_targets.shape}\"\n",
    "\n",
    "    # Calculate metrics\n",
    "    precision, recall, f1, _ = precision_recall_fscore_support(\n",
    "        all_targets, all_preds, average=None, zero_division=0\n",
    "    )\n",
    "\n",
    "    return precision, recall, f1\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    # Load dataset (ensure this is defined properly)\n",
    "    data_module = MyDataModule(train_dataset, train_dataset, test_dataset, batch_size, num_workers=2, idx_to_label=idx_to_label)\n",
    "    #test_dataset = data_module.test_dataloader()\n",
    "\n",
    "    data_module.setup()\n",
    "\n",
    "    # Load dataset and create dataloaders\n",
    "    test_loader = data_module.test_dataloader()\n",
    "    # Split the dataset into train and validation subsets using indices\n",
    "\n",
    "\n",
    "    # Load model\n",
    "    best_model = ResNetWrapper.load_from_checkpoint(\n",
    "        best_model_path,\n",
    "        lr=lr,  # Or the appropriate learning rate\n",
    "        dropout_rate=dropout_rate\n",
    "    )\n",
    "    best_model.eval()\n",
    "\n",
    "    # Run evaluation\n",
    "    precision, recall, f1 = evaluate_model(best_model, test_loader)\n",
    "    \n",
    "    # Output metrics\n",
    "    print(f\"Macro F1: {np.nanmean(f1):.4f}\")\n",
    "    print(f\"Macro Precision: {np.nanmean(precision):.4f}\")\n",
    "    print(f\"Macro Recall: {np.nanmean(recall):.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deployment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.2849, 0.2963, 0.2818, 0.6259, 0.2959, 0.2011, 0.4135, 0.3734, 0.2627,\n",
       "        0.2619, 0.1802, 0.3827, 0.5185, 0.1720, 0.3604, 0.1651, 0.2453, 0.4623,\n",
       "        0.1707, 0.2890, 0.3945, 0.4414, 0.3920, 0.1822, 0.2859, 0.3029, 0.2686,\n",
       "        0.2115, 0.5593, 0.4130, 0.1735, 0.5432, 0.4935, 0.4109, 0.1949, 0.2178,\n",
       "        0.3441, 0.3421, 0.3135, 0.3135, 0.1489, 0.1800, 0.4276, 0.2586, 0.3230,\n",
       "        0.2559, 0.2908, 0.2778, 0.6469, 0.4203, 0.3101, 0.4571, 0.4003, 0.2587,\n",
       "        0.3394, 0.2790, 0.2593, 0.3019, 0.3836, 0.1453, 0.3771, 0.3427, 0.2840,\n",
       "        0.4190, 0.2334, 0.6731, 0.4133, 0.3434, 0.3068, 0.2820, 0.2273, 0.2123,\n",
       "        0.1689, 0.3102, 0.3898, 0.1775, 0.2295, 0.3089, 0.3682, 0.3530],\n",
       "       device='mps:0')"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model.sigmoid_thresholds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "'NoneType' object is not iterable",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[22]\u001b[39m\u001b[32m, line 45\u001b[39m\n\u001b[32m     43\u001b[39m \u001b[38;5;66;03m# Iterate through test DataLoader\u001b[39;00m\n\u001b[32m     44\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m torch.no_grad():\n\u001b[32m---> \u001b[39m\u001b[32m45\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     46\u001b[39m \u001b[43m        \u001b[49m\u001b[43mx\u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch\u001b[49m\n\u001b[32m     48\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Move data to the same device as the model\u001b[39;49;00m\n",
      "\u001b[31mTypeError\u001b[39m: 'NoneType' object is not iterable"
     ]
    }
   ],
   "source": [
    "# kepeket loaderbe\n",
    "from torch.utils.data import Dataset\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "# best_model_path=\"finals/per_class_threshold_4/version_0/checkpoints/epoch=11-val_f1=0.6729.ckpt\"\n",
    "# best_model = ResNetWrapper.load_from_checkpoint(best_model_path, lr=lr, dropout_rate=dropout_rate)\n",
    "best_model_path=\"with_ood_80_classes_onlyID_metrics/ood_fold_0/version_4/checkpoints/epoch=13-val_f1=0.7128.ckpt\"\n",
    "\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image  # Mivel nincsenek címkék, csak a képet adjuk vissza\n",
    "\n",
    "# Példa használat:\n",
    "deploy_set = CustomImageDataset(\"/Users/beszabo/codes/arctic_shift/scripts/downloaded_images\", transform=transform_)\n",
    "\n",
    "# Test the evaluation\n",
    "test_data_module = MyDataModule(None, None, deploy_set, batch_size, num_workers=2)\n",
    "test_loader = test_data_module.test_dataloader()\n",
    " # Ensure model is on the appropriate device\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "best_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "all_preds = []\n",
    "all_targets = []\n",
    "\n",
    "# Iterate through test DataLoader\n",
    "with torch.no_grad():\n",
    "    for batch in test_loader:\n",
    "        x= batch\n",
    "        \n",
    "        # Move data to the same device as the model\n",
    "        x = x.to(device)\n",
    "\n",
    "        logits = best_model(x)\n",
    "        # Sigmoid activation + thresholding\n",
    "        preds = (torch.sigmoid(logits)  > best_model.sigmoid_thresholds.to(device)).cpu().numpy()\n",
    "\n",
    "\n",
    "        # Append batch predictions and targets\n",
    "        all_preds.append(preds)# Preds\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "# Convert predictions to class indices\n",
    "pred_classes = [np.where(pred == 1)[0] for pred in all_preds]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Average OOD score: 0.4969\n",
      "OOD score distribution: min=0.4564, max=0.5563\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "import os\n",
    "\n",
    "import multiprocessing\n",
    "multiprocessing.set_start_method('fork', force=True)\n",
    "\n",
    "\n",
    "best_model_path=\"with_ood_80_classes_onlyID_metrics/ood_fold_0/version_4/checkpoints/epoch=13-val_f1=0.7128.ckpt\"\n",
    "\n",
    "\n",
    "# Custom dataset for deployment images\n",
    "class CustomImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.transform = transform\n",
    "        self.image_paths = [os.path.join(image_dir, f) for f in os.listdir(image_dir) if f.endswith(('png', 'jpg', 'jpeg'))]\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "\n",
    "        return image  # Only return the image (no labels in deployment)\n",
    "    \n",
    "best_model = ResNetWrapper.load_from_checkpoint(best_model_path, lr=lr, dropout_rate=dropout_rate)\n",
    "\n",
    "# Load deployment dataset\n",
    "deploy_set = CustomImageDataset(\"/Users/beszabo/codes/arctic_shift/scripts/downloaded_images\", transform=transform_)\n",
    "\n",
    "# # Create DataLoader\n",
    "# deploy_loader = DataLoader(deploy_set, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# # Ensure model is on the appropriate device\n",
    "# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "# best_model.to(device)\n",
    "# best_model.eval()  # Set model to evaluation mode\n",
    "\n",
    "# all_preds = []\n",
    "\n",
    "# # Iterate through test DataLoader\n",
    "# with torch.no_grad():\n",
    "#     for batch in deploy_loader:\n",
    "#         x = batch.to(device)\n",
    "\n",
    "#         logits = best_model(x)\n",
    "#         probs = torch.sigmoid(logits)  # Convert logits to probabilities\n",
    "\n",
    "#         # Use the model's stored sigmoid thresholds for prediction\n",
    "#         preds = (probs > best_model.sigmoid_thresholds.to(device)).cpu().numpy()\n",
    "        \n",
    "#         all_preds.append(preds)\n",
    "\n",
    "# # Concatenate predictions\n",
    "# all_preds = np.concatenate(all_preds, axis=0)\n",
    "\n",
    "N = 100  # Change this to the number of images you want to process\n",
    "\n",
    "# Limit dataset to first N images\n",
    "limited_dataset = torch.utils.data.Subset(deploy_set, range(min(N, len(deploy_set))))\n",
    "\n",
    "# Create DataLoader with limited samples\n",
    "limited_loader = DataLoader(limited_dataset, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "# Ensure model is on the correct device\n",
    "device = torch.device(\"mps\" if torch.cuda.is_available() else \"cpu\")\n",
    "best_model.to(device)\n",
    "best_model.eval()\n",
    "\n",
    "all_preds = []\n",
    "all_ood_scores = []  # Track OOD scores for analysis\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in limited_loader:\n",
    "        x = batch.to(device)\n",
    "        \n",
    "        # Get both class and OOD logits\n",
    "        class_logits, ood_logits = best_model(x)\n",
    "        \n",
    "        # Get OOD probabilities for analysis\n",
    "        ood_probs = torch.sigmoid(ood_logits).cpu().numpy()\n",
    "        all_ood_scores.extend(ood_probs)\n",
    "        \n",
    "        # Use the model's built-in prediction method\n",
    "        preds = best_model._get_preds(class_logits, ood_logits).cpu().numpy()\n",
    "        all_preds.append(preds)\n",
    "\n",
    "# Concatenate predictions\n",
    "all_preds = np.concatenate(all_preds, axis=0)\n",
    "all_ood_scores = np.array(all_ood_scores)\n",
    "\n",
    "# Analyze OOD scores\n",
    "print(f\"Average OOD score: {np.mean(all_ood_scores):.4f}\")\n",
    "print(f\"OOD score distribution: min={np.min(all_ood_scores):.4f}, max={np.max(all_ood_scores):.4f}\")\n",
    "\n",
    "# Count occurrences per class\n",
    "class_counts = np.sum(all_preds, axis=0)\n",
    "\n",
    "# Print class-wise image counts\n",
    "for class_idx, count in enumerate(class_counts):\n",
    "    if count > 0:\n",
    "        print(f\"Class {idx_to_label.get(class_idx)}: {count} images\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Zara images\n",
      "Heineken images\n",
      "Samsung images\n",
      "Dior images\n",
      "Oracle images\n",
      "LEGO images\n",
      "Airbnb images\n",
      "Nestlé images\n",
      "Facebook images\n",
      "YouTube images\n",
      "KFC images\n",
      "Nescafé images\n",
      "Sony images\n",
      "Huawei images\n",
      "Images saved in the output directory.\n",
      "Class counts: {'Airbnb images': 1, 'Amazon images': 0, 'AmericanExpress images': 0, 'Apple images': 0, 'Audi images': 0, 'BMW images': 0, 'Budweiser images': 0, 'Burberry images': 0, 'Canon images': 0, 'Cartier images': 0, 'Chanel images': 0, 'Cisco images': 0, 'CocaCola images': 0, 'Colgate images': 0, 'Corona images': 0, 'DHL images': 0, 'Danone images': 0, 'Dior images': 1, 'Disney images': 0, 'Facebook images': 1, 'FedEx images': 0, 'Ferrari images': 0, 'Ford images': 0, 'Gillette images': 0, 'Google images': 0, 'Gucci images': 0, 'HM images': 0, 'HP images': 0, 'Heineken images': 1, 'Hennessy images': 0, 'Honda images': 0, 'Huawei images': 1, 'Hyundai images': 0, 'IBM images': 0, 'IKEA images': 0, 'Instagram images': 0, 'Intel images': 0, 'JackDaniels images': 0, 'KFC images': 1, 'Kelloggs images': 0, 'Kia images': 0, 'LEGO images': 1, 'LOréalParis images': 0, 'LinkedIn images': 0, 'LouisVuitton images': 0, 'Mastercard images': 0, 'McDonalds images': 0, 'MercedesBenz images': 0, 'Microsoft images': 0, 'Nescafé images': 1, 'Nespresso images': 0, 'Nestlé images': 1, 'Netflix images': 0, 'Nike images': 0, 'Nintendo images': 0, 'Nissan images': 0, 'Oracle images': 1, 'Pampers images': 0, 'Panasonic images': 0, 'PayPal images': 0, 'Pepsi images': 0, 'Porsche images': 0, 'Prada images': 0, 'RedBull images': 0, 'SAP images': 0, 'Salesforce images': 0, 'Samsung images': 1, 'Sephora images': 0, 'Sony images': 1, 'Spotify images': 0, 'Starbucks images': 0, 'Tesla images': 0, 'Toyota images': 0, 'UPS images': 0, 'Volkswagen images': 0, 'Xiaomi images': 0, 'YouTube images': 1, 'Zara images': 1, 'adidas images': 0, 'eBay images': 0}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "from torchvision.utils import save_image\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "# Define the output directory\n",
    "output_dir = \"/Users/beszabo/Documents/meme/preds\"\n",
    "\n",
    "# Create subdirectories for each label\n",
    "for label in idx_to_label.values():\n",
    "    os.makedirs(os.path.join(output_dir, label), exist_ok=True)\n",
    "\n",
    "# Define the inverse transform (example for normalization)\n",
    "inverse_transform = transforms.Compose([\n",
    "    transforms.Normalize(mean=[-mean/std for mean, std in zip([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])], std=[1/std for std in [0.229, 0.224, 0.225]]),\n",
    "    transforms.ToPILImage()\n",
    "])\n",
    "\n",
    "# Dictionary to count the number of images per class\n",
    "class_counts = {label: 0 for label in idx_to_label.values()}\n",
    "\n",
    "# Iterate through the predictions and save the images in the corresponding directories\n",
    "for i, preds in enumerate(all_preds):\n",
    "    img_path = deploy_set.image_paths[i]\n",
    "    img = Image.open(img_path).convert(\"RGB\")\n",
    "    img_tensor = transform_(img)  # Apply the same transform used in the dataset\n",
    "\n",
    "    for j, pred in enumerate(preds):\n",
    "        if pred == 1:\n",
    "            label = idx_to_label[j]\n",
    "            save_path = os.path.join(output_dir, label, f\"image_{i}.png\")\n",
    "            \n",
    "            # Reverse the transformations\n",
    "            img_reversed = inverse_transform(img_tensor)\n",
    "            img_reversed.save(save_path)\n",
    "            \n",
    "            # Increment the class count\n",
    "            class_counts[label] += 1\n",
    "            print(label)\n",
    "\n",
    "print(\"Images saved in the output directory.\")\n",
    "print(\"Class counts:\", class_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:23:23.610107Z",
     "iopub.status.busy": "2025-01-30T12:23:23.609686Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmatplotlib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpyplot\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mplt\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mseaborn\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01msns\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;66;03m# Assuming you already have the reverse index mapping: idx_to_label\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/seaborn/__init__.py:5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpalettes\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mrelational\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mregression\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcategorical\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m  \u001b[38;5;66;03m# noqa: F401,F403\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/seaborn/relational.py:21\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mutils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m     14\u001b[0m     adjust_legend_subtitles,\n\u001b[1;32m     15\u001b[0m     _default_color,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     18\u001b[0m     _scatter_legend_artist,\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_compat\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m groupby_apply_include_groups\n\u001b[0;32m---> 21\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_statistics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EstimateAggregator, WeightedAggregator\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maxisgrid\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m FacetGrid, _facet_docs\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_docstrings\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m DocstringComponents, _core_docs\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/seaborn/_statistics.py:32\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpandas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[1;32m     31\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mstats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m gaussian_kde\n\u001b[1;32m     33\u001b[0m     _no_scipy \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m     34\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/stats/__init__.py:653\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_distribution_infrastructure\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m    650\u001b[0m     make_distribution, Mixture, order_statistic, truncate, exp, log, \u001b[38;5;28mabs\u001b[39m\n\u001b[1;32m    651\u001b[0m )\n\u001b[1;32m    652\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_new_distributions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Normal, Uniform\n\u001b[0;32m--> 653\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_mgc\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m multiscale_graphcorr\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_correlation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m chatterjeexi\n\u001b[1;32m    657\u001b[0m \u001b[38;5;66;03m# Deprecated namespaces, to be removed in v2.0.0\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/stats/_mgc.py:7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_bunch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _make_tuple_bunch\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mspatial\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistance\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m cdist\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mndimage\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _measurements\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_stats\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _local_correlations  \u001b[38;5;66;03m# type: ignore[import-not-found]\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m distributions\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/ndimage/__init__.py:156\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;124;03m=========================================================\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mMultidimensional image processing (:mod:`scipy.ndimage`)\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    119\u001b[0m \n\u001b[1;32m    120\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;66;03m# Copyright (C) 2003-2005 Peter J. Verveer\u001b[39;00m\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# Redistribution and use in source and binary forms, with or without\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[38;5;66;03m# mypy: ignore-errors\u001b[39;00m\n\u001b[0;32m--> 156\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_support_alternative_backends\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;66;03m# adjust __all__ and do not leak implementation details\u001b[39;00m\n\u001b[1;32m    159\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _support_alternative_backends\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/ndimage/_support_alternative_backends.py:7\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_array_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[1;32m      3\u001b[0m     is_cupy, is_jax, scipy_namespace_for, SCIPY_ARRAY_API\n\u001b[1;32m      4\u001b[0m )\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnumpy\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_ndimage_api\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m   \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ndimage_api\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _delegators\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/ndimage/_ndimage_api.py:9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;124;03m\"\"\"This is the 'bare' ndimage API.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \n\u001b[1;32m      3\u001b[0m \u001b[38;5;124;03mThis --- private! --- module only collects implementations of public ndimage API\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;124;03mre-exports decorated names to __init__.py\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_filters\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m    \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_fourier\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m   \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_interpolation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m   \u001b[38;5;66;03m# noqa: F403\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/scipy/ndimage/_filters.py:39\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mscipy\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_lib\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_util\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m normalize_axis_index\n\u001b[1;32m     38\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ni_support\n\u001b[0;32m---> 39\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _nd_image\n\u001b[1;32m     40\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _ni_docstrings\n\u001b[1;32m     41\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _rank_filter_1d\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "# Assuming you already have the reverse index mapping: idx_to_label\n",
    "mxs_0 = mxs[0]\n",
    "# Calculate number of rows and columns for the subplots (80 matrices)\n",
    "n_rows = 10  # You can adjust this for different grid sizes\n",
    "n_cols = 8   # Adjusted to fit 80 matrices in a grid\n",
    "\n",
    "# Set up the matplotlib figure\n",
    "fig, axes = plt.subplots(n_rows, n_cols, figsize=(24, 16))  # Adjust the size as needed\n",
    "axes = axes.ravel()  # Flatten the axes array to index them easily\n",
    "\n",
    "# Plot each confusion matrix\n",
    "for i, mx in enumerate(mxs_0):\n",
    "    ax = axes[i]\n",
    "    \n",
    "    # Plot the heatmap with masked zeros\n",
    "    sns.heatmap(mx, annot=True, fmt='d', vmin=0.5, vmax=1, cbar=False, ax=ax, square=True)\n",
    "    \n",
    "    # Set title with the index and label name\n",
    "    label_name = idx_to_label.get(i, f\"Label {i}\")  # Get the label name or default to \"Label i\"\n",
    "    ax.set_title(f'{label_name} (Index {i})')\n",
    "    ax.set_xlabel('Predicted', fontsize=14)\n",
    "    ax.set_ylabel('Actual', fontsize=14)\n",
    "\n",
    "for j in range(i + 1, len(axes)):\n",
    "    axes[j].axis('off')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bayesian optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | sigmoi... |\n",
      "-------------------------------------\n",
      "Test dataset is INVALID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: True (mps), used: True\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test dataset is INVALID\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading `train_dataloader` to estimate number of stepping batches.\n",
      "Traceback (most recent call last):\n",
      "  File \u001b[35m\"<string>\"\u001b[0m, line \u001b[35m1\u001b[0m, in \u001b[35m<module>\u001b[0m\n",
      "    from multiprocessing.spawn import spawn_main; \u001b[31mspawn_main\u001b[0m\u001b[1;31m(tracker_fd=83, pipe_handle=97)\u001b[0m\n",
      "                                                  \u001b[31m~~~~~~~~~~\u001b[0m\u001b[1;31m^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\u001b[0m\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m122\u001b[0m, in \u001b[35mspawn_main\u001b[0m\n",
      "    exitcode = _main(fd, parent_sentinel)\n",
      "  File \u001b[35m\"/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/spawn.py\"\u001b[0m, line \u001b[35m132\u001b[0m, in \u001b[35m_main\u001b[0m\n",
      "    self = reduction.pickle.load(from_parent)\n",
      "\u001b[1;35mAttributeError\u001b[0m: \u001b[35mCan't get attribute 'custom_collate_fn' on <module '__main__' (<class '_frozen_importlib.BuiltinImporter'>)>\u001b[0m\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "DataLoader worker (pid 18589) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/bayes_opt/target_space.py:191\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    190\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 191\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cache\u001b[49m\u001b[43m[\u001b[49m\u001b[43m_hashable\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n",
      "\u001b[0;31mKeyError\u001b[0m: (0.449816047538945,)",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 100\u001b[0m\n\u001b[1;32m     89\u001b[0m pbounds \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     90\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid_threshold\u001b[39m\u001b[38;5;124m'\u001b[39m: (\u001b[38;5;241m0.3\u001b[39m, \u001b[38;5;241m0.7\u001b[39m)\n\u001b[1;32m     91\u001b[0m } \n\u001b[1;32m     93\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m BayesianOptimization(\n\u001b[1;32m     94\u001b[0m     f\u001b[38;5;241m=\u001b[39mobjective,\n\u001b[1;32m     95\u001b[0m     pbounds\u001b[38;5;241m=\u001b[39mpbounds,\n\u001b[1;32m     96\u001b[0m     random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m,\n\u001b[1;32m     97\u001b[0m     verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m\n\u001b[1;32m     98\u001b[0m ) \n\u001b[0;32m--> 100\u001b[0m \u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaximize\u001b[49m\u001b[43m(\u001b[49m\u001b[43minit_points\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_iter\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m7\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/bayes_opt/bayesian_optimization.py:305\u001b[0m, in \u001b[0;36mBayesianOptimization.maximize\u001b[0;34m(self, init_points, n_iter, acq, kappa, kappa_decay, kappa_decay_delay, xi, **gp_params)\u001b[0m\n\u001b[1;32m    303\u001b[0m     x_probe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msuggest(util)\n\u001b[1;32m    304\u001b[0m     iteration \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[0;32m--> 305\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_probe\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlazy\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer \u001b[38;5;129;01mand\u001b[39;00m iteration \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    308\u001b[0m     \u001b[38;5;66;03m# The bounds transformer should only modify the bounds after\u001b[39;00m\n\u001b[1;32m    309\u001b[0m     \u001b[38;5;66;03m# the init_points points (only for the true iterations)\u001b[39;00m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mset_bounds(\n\u001b[1;32m    311\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bounds_transformer\u001b[38;5;241m.\u001b[39mtransform(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_space))\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/bayes_opt/bayesian_optimization.py:200\u001b[0m, in \u001b[0;36mBayesianOptimization.probe\u001b[0;34m(self, params, lazy)\u001b[0m\n\u001b[1;32m    198\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_queue\u001b[38;5;241m.\u001b[39madd(params)\n\u001b[1;32m    199\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 200\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_space\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprobe\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    201\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdispatch(Events\u001b[38;5;241m.\u001b[39mOPTIMIZATION_STEP)\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/bayes_opt/target_space.py:194\u001b[0m, in \u001b[0;36mTargetSpace.probe\u001b[0;34m(self, params)\u001b[0m\n\u001b[1;32m    192\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    193\u001b[0m     params \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(\u001b[38;5;28mzip\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_keys, x))\n\u001b[0;32m--> 194\u001b[0m     target \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    195\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mregister(x, target)\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m target\n",
      "Cell \u001b[0;32mIn[7], line 81\u001b[0m, in \u001b[0;36mobjective\u001b[0;34m(sigmoid_threshold)\u001b[0m\n\u001b[1;32m     72\u001b[0m early_stopping \u001b[38;5;241m=\u001b[39m EarlyStopping(monitor\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[1;32m     75\u001b[0m     logger\u001b[38;5;241m=\u001b[39mlogger,\n\u001b[1;32m     76\u001b[0m     max_epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     77\u001b[0m     accelerator\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmps\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[checkpoint_callback, early_stopping]\n\u001b[1;32m     79\u001b[0m )\n\u001b[0;32m---> 81\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdata_module\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     83\u001b[0m val_loss \u001b[38;5;241m=\u001b[39m trainer\u001b[38;5;241m.\u001b[39mcallback_metrics\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_loss\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28mfloat\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minf\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n\u001b[1;32m     84\u001b[0m val_losses\u001b[38;5;241m.\u001b[39mappend(val_loss\u001b[38;5;241m.\u001b[39mitem() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(val_loss, torch\u001b[38;5;241m.\u001b[39mTensor) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mfloat\u001b[39m(val_loss))\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:539\u001b[0m, in \u001b[0;36mTrainer.fit\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    537\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstatus \u001b[38;5;241m=\u001b[39m TrainerStatus\u001b[38;5;241m.\u001b[39mRUNNING\n\u001b[1;32m    538\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m--> 539\u001b[0m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_and_handle_interrupt\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    540\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_impl\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_dataloaders\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdatamodule\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\n\u001b[1;32m    541\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:47\u001b[0m, in \u001b[0;36m_call_and_handle_interrupt\u001b[0;34m(trainer, trainer_fn, *args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     46\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mlauncher\u001b[38;5;241m.\u001b[39mlaunch(trainer_fn, \u001b[38;5;241m*\u001b[39margs, trainer\u001b[38;5;241m=\u001b[39mtrainer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m---> 47\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtrainer_fn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     49\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m _TunerExitException:\n\u001b[1;32m     50\u001b[0m     _call_teardown_hook(trainer)\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:575\u001b[0m, in \u001b[0;36mTrainer._fit_impl\u001b[0;34m(self, model, train_dataloaders, val_dataloaders, datamodule, ckpt_path)\u001b[0m\n\u001b[1;32m    568\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    569\u001b[0m ckpt_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_checkpoint_connector\u001b[38;5;241m.\u001b[39m_select_ckpt_path(\n\u001b[1;32m    570\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn,\n\u001b[1;32m    571\u001b[0m     ckpt_path,\n\u001b[1;32m    572\u001b[0m     model_provided\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    573\u001b[0m     model_connected\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    574\u001b[0m )\n\u001b[0;32m--> 575\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_run\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mckpt_path\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mckpt_path\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    577\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mstopped\n\u001b[1;32m    578\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:958\u001b[0m, in \u001b[0;36mTrainer._run\u001b[0;34m(self, model, ckpt_path)\u001b[0m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_logger_connector\u001b[38;5;241m.\u001b[39mreset_metrics()\n\u001b[1;32m    957\u001b[0m \u001b[38;5;66;03m# strategy will configure model and move it to the device\u001b[39;00m\n\u001b[0;32m--> 958\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstrategy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    960\u001b[0m \u001b[38;5;66;03m# hook\u001b[39;00m\n\u001b[1;32m    961\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:159\u001b[0m, in \u001b[0;36mStrategy.setup\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    156\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_model(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel)\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n\u001b[0;32m--> 159\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_optimizers\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrainer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msetup_precision_plugin()\n\u001b[1;32m    161\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mfn \u001b[38;5;241m==\u001b[39m TrainerFn\u001b[38;5;241m.\u001b[39mFITTING:\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/strategies/strategy.py:139\u001b[0m, in \u001b[0;36mStrategy.setup_optimizers\u001b[0;34m(self, trainer)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Creates optimizers and schedulers.\u001b[39;00m\n\u001b[1;32m    133\u001b[0m \n\u001b[1;32m    134\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[1;32m    135\u001b[0m \u001b[38;5;124;03m    trainer: the Trainer, these optimizers should be connected to\u001b[39;00m\n\u001b[1;32m    136\u001b[0m \n\u001b[1;32m    137\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    138\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlightning_module \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 139\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizers, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler_configs \u001b[38;5;241m=\u001b[39m \u001b[43m_init_optimizers_and_lr_schedulers\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlightning_module\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/core/optimizer.py:180\u001b[0m, in \u001b[0;36m_init_optimizers_and_lr_schedulers\u001b[0;34m(model)\u001b[0m\n\u001b[1;32m    177\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Calls `LightningModule.configure_optimizers` and parses and validates the output.\"\"\"\u001b[39;00m\n\u001b[1;32m    178\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mlightning\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpytorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mtrainer\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m call\n\u001b[0;32m--> 180\u001b[0m optim_conf \u001b[38;5;241m=\u001b[39m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_module_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mconfigure_optimizers\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpl_module\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    182\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m optim_conf \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    183\u001b[0m     rank_zero_warn(\n\u001b[1;32m    184\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`LightningModule.configure_optimizers` returned `None`, this fit will run with no optimizer\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m    185\u001b[0m     )\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:171\u001b[0m, in \u001b[0;36m_call_lightning_module_hook\u001b[0;34m(trainer, hook_name, pl_module, *args, **kwargs)\u001b[0m\n\u001b[1;32m    168\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m hook_name\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpl_module\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 171\u001b[0m     output \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[38;5;66;03m# restore current_fx when nested context\u001b[39;00m\n\u001b[1;32m    174\u001b[0m pl_module\u001b[38;5;241m.\u001b[39m_current_fx_name \u001b[38;5;241m=\u001b[39m prev_fx_name\n",
      "Cell \u001b[0;32mIn[3], line 320\u001b[0m, in \u001b[0;36mResNetWrapper.configure_optimizers\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    317\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39moptim\u001b[38;5;241m.\u001b[39mAdamW(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr)\n\u001b[1;32m    319\u001b[0m \u001b[38;5;66;03m# Ensure num_training_steps is an int\u001b[39;00m\n\u001b[0;32m--> 320\u001b[0m total_training_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimated_stepping_batches\u001b[49m)  \n\u001b[1;32m    321\u001b[0m warmup_steps \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(\u001b[38;5;241m0.1\u001b[39m \u001b[38;5;241m*\u001b[39m total_training_steps)  \u001b[38;5;66;03m# 10% of steps for warmup\u001b[39;00m\n\u001b[1;32m    323\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m get_cosine_schedule_with_warmup(\n\u001b[1;32m    324\u001b[0m     optimizer,\n\u001b[1;32m    325\u001b[0m     num_warmup_steps\u001b[38;5;241m=\u001b[39mwarmup_steps,\n\u001b[1;32m    326\u001b[0m     num_training_steps\u001b[38;5;241m=\u001b[39mtotal_training_steps  \u001b[38;5;66;03m# Now explicitly an int\u001b[39;00m\n\u001b[1;32m    327\u001b[0m )\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/trainer.py:1677\u001b[0m, in \u001b[0;36mTrainer.estimated_stepping_batches\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1675\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_dataloader \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m   1676\u001b[0m     rank_zero_info(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLoading `train_dataloader` to estimate number of stepping batches.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m-> 1677\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_loop\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msetup_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1679\u001b[0m total_batches \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_training_batches\n\u001b[1;32m   1681\u001b[0m \u001b[38;5;66;03m# iterable dataset\u001b[39;00m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/loops/fit_loop.py:237\u001b[0m, in \u001b[0;36m_FitLoop.setup_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    234\u001b[0m log\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m: resetting train dataloader\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    236\u001b[0m source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data_source\n\u001b[0;32m--> 237\u001b[0m train_dataloader \u001b[38;5;241m=\u001b[39m \u001b[43m_request_dataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m trainer\u001b[38;5;241m.\u001b[39mstrategy\u001b[38;5;241m.\u001b[39mbarrier(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtrain_dataloader()\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    240\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(train_dataloader, CombinedLoader):\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:326\u001b[0m, in \u001b[0;36m_request_dataloader\u001b[0;34m(data_source)\u001b[0m\n\u001b[1;32m    315\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Requests a dataloader by calling dataloader hooks corresponding to the given stage.\u001b[39;00m\n\u001b[1;32m    316\u001b[0m \n\u001b[1;32m    317\u001b[0m \u001b[38;5;124;03mReturns:\u001b[39;00m\n\u001b[1;32m    318\u001b[0m \u001b[38;5;124;03m    The requested dataloader\u001b[39;00m\n\u001b[1;32m    319\u001b[0m \n\u001b[1;32m    320\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _replace_dunder_methods(DataLoader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdataset\u001b[39m\u001b[38;5;124m\"\u001b[39m), _replace_dunder_methods(BatchSampler):\n\u001b[1;32m    322\u001b[0m     \u001b[38;5;66;03m# under this context manager, the arguments passed to `DataLoader.__init__` will be captured and saved as\u001b[39;00m\n\u001b[1;32m    323\u001b[0m     \u001b[38;5;66;03m# attributes on the instance in case the dataloader needs to be re-instantiated later by Lightning.\u001b[39;00m\n\u001b[1;32m    324\u001b[0m     \u001b[38;5;66;03m# Also, it records all attribute setting and deletion using patched `__setattr__` and `__delattr__`\u001b[39;00m\n\u001b[1;32m    325\u001b[0m     \u001b[38;5;66;03m# methods so that the re-instantiated object is as close to the original as possible.\u001b[39;00m\n\u001b[0;32m--> 326\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mdata_source\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdataloader\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/connectors/data_connector.py:293\u001b[0m, in \u001b[0;36m_DataLoaderSource.dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    291\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance, pl\u001b[38;5;241m.\u001b[39mLightningDataModule):\n\u001b[1;32m    292\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\u001b[38;5;241m.\u001b[39mtrainer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m--> 293\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mcall\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_lightning_datamodule_hook\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minstance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrainer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    295\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minstance\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/trainer/call.py:193\u001b[0m, in \u001b[0;36m_call_lightning_datamodule_hook\u001b[0;34m(trainer, hook_name, *args, **kwargs)\u001b[0m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(fn):\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m trainer\u001b[38;5;241m.\u001b[39mprofiler\u001b[38;5;241m.\u001b[39mprofile(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m[LightningDataModule]\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtrainer\u001b[38;5;241m.\u001b[39mdatamodule\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mhook_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m--> 193\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    194\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[4], line 171\u001b[0m, in \u001b[0;36mMyDataModule.train_dataloader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    168\u001b[0m     combined \u001b[38;5;241m=\u001b[39m CombinedLoader({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m: id_loader, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mood\u001b[39m\u001b[38;5;124m\"\u001b[39m: ood_loader}, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmin_size\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    170\u001b[0m     \u001b[38;5;66;03m# ✨ Force building the iterator manually\u001b[39;00m\n\u001b[0;32m--> 171\u001b[0m     _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mcombined\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    173\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m combined\n\u001b[1;32m    174\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/combined_loader.py:351\u001b[0m, in \u001b[0;36mCombinedLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mcls\u001b[39m \u001b[38;5;241m=\u001b[39m _SUPPORTED_MODES[\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_mode][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124miterator\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m    350\u001b[0m iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mcls\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mflattened, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_limits)\n\u001b[0;32m--> 351\u001b[0m \u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterator\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m iterator\n\u001b[1;32m    353\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/lightning/pytorch/utilities/combined_loader.py:43\u001b[0m, in \u001b[0;36m_ModeIterator.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[38;5;129m@override\u001b[39m\n\u001b[1;32m     42\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__iter__\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m---> 43\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterators \u001b[38;5;241m=\u001b[39m [\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m iterable \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39miterables]\n\u001b[1;32m     44\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m     45\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:486\u001b[0m, in \u001b[0;36mDataLoader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpersistent_workers \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_workers \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    485\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 486\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_iterator\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    487\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    488\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_iterator\u001b[38;5;241m.\u001b[39m_reset(\u001b[38;5;28mself\u001b[39m)\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:422\u001b[0m, in \u001b[0;36mDataLoader._get_iterator\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    420\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    421\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcheck_worker_number_rationality()\n\u001b[0;32m--> 422\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_MultiProcessingDataLoaderIter\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:1146\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter.__init__\u001b[0;34m(self, loader)\u001b[0m\n\u001b[1;32m   1139\u001b[0m w\u001b[38;5;241m.\u001b[39mdaemon \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;66;03m# NB: Process.start() actually take some time as it needs to\u001b[39;00m\n\u001b[1;32m   1141\u001b[0m \u001b[38;5;66;03m#     start a process and pass the arguments over via a pipe.\u001b[39;00m\n\u001b[1;32m   1142\u001b[0m \u001b[38;5;66;03m#     Therefore, we only add a worker to self._workers list after\u001b[39;00m\n\u001b[1;32m   1143\u001b[0m \u001b[38;5;66;03m#     it started, so that we do not call .join() if program dies\u001b[39;00m\n\u001b[1;32m   1144\u001b[0m \u001b[38;5;66;03m#     before it starts, and __del__ tries to join but will get:\u001b[39;00m\n\u001b[1;32m   1145\u001b[0m \u001b[38;5;66;03m#     AssertionError: can only join a started process.\u001b[39;00m\n\u001b[0;32m-> 1146\u001b[0m \u001b[43mw\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstart\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1147\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_index_queues\u001b[38;5;241m.\u001b[39mappend(index_queue)\n\u001b[1;32m   1148\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_workers\u001b[38;5;241m.\u001b[39mappend(w)\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/process.py:121\u001b[0m, in \u001b[0;36mBaseProcess.start\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m _current_process\u001b[38;5;241m.\u001b[39m_config\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemon\u001b[39m\u001b[38;5;124m'\u001b[39m), \\\n\u001b[1;32m    119\u001b[0m        \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdaemonic processes are not allowed to have children\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m    120\u001b[0m _cleanup()\n\u001b[0;32m--> 121\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sentinel \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_popen\u001b[38;5;241m.\u001b[39msentinel\n\u001b[1;32m    123\u001b[0m \u001b[38;5;66;03m# Avoid a refcycle if the target function holds an indirect\u001b[39;00m\n\u001b[1;32m    124\u001b[0m \u001b[38;5;66;03m# reference to the process object (see bpo-30775)\u001b[39;00m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:224\u001b[0m, in \u001b[0;36mProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    222\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[0;32m--> 224\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_default_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_context\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mProcess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_Popen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/context.py:289\u001b[0m, in \u001b[0;36mSpawnProcess._Popen\u001b[0;34m(process_obj)\u001b[0m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;129m@staticmethod\u001b[39m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_Popen\u001b[39m(process_obj):\n\u001b[1;32m    288\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mpopen_spawn_posix\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Popen\n\u001b[0;32m--> 289\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mPopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:32\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, process_obj):\n\u001b[1;32m     31\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fds \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m---> 32\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;21;43m__init__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_fork.py:20\u001b[0m, in \u001b[0;36mPopen.__init__\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreturncode \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m     19\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfinalizer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_launch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mprocess_obj\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/homebrew/Cellar/python@3.13/3.13.3/Frameworks/Python.framework/Versions/3.13/lib/python3.13/multiprocessing/popen_spawn_posix.py:62\u001b[0m, in \u001b[0;36mPopen._launch\u001b[0;34m(self, process_obj)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msentinel \u001b[38;5;241m=\u001b[39m parent_r\n\u001b[1;32m     61\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(parent_w, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwb\u001b[39m\u001b[38;5;124m'\u001b[39m, closefd\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m f:\n\u001b[0;32m---> 62\u001b[0m         \u001b[43mf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgetbuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     63\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     64\u001b[0m     fds_to_close \u001b[38;5;241m=\u001b[39m []\n",
      "File \u001b[0;32m~/Documents/meme/.venv/lib/python3.13/site-packages/torch/utils/data/_utils/signal_handling.py:73\u001b[0m, in \u001b[0;36m_set_SIGCHLD_handler.<locals>.handler\u001b[0;34m(signum, frame)\u001b[0m\n\u001b[1;32m     70\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mhandler\u001b[39m(signum, frame):\n\u001b[1;32m     71\u001b[0m     \u001b[38;5;66;03m# This following call uses `waitid` with WNOHANG from C side. Therefore,\u001b[39;00m\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;66;03m# Python can still get and update the process status successfully.\u001b[39;00m\n\u001b[0;32m---> 73\u001b[0m     \u001b[43m_error_if_any_worker_fails\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     74\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m previous_handler \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     75\u001b[0m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mcallable\u001b[39m(previous_handler)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: DataLoader worker (pid 18589) exited unexpectedly with exit code 1. Details are lost due to multiprocessing. Rerunning with num_workers=0 may give better error trace."
     ]
    }
   ],
   "source": [
    "from torch.utils.data import ConcatDataset, Subset\n",
    "import numpy as np\n",
    "import torch\n",
    "from bayes_opt import BayesianOptimization\n",
    "from lightning.pytorch.utilities.combined_loader import CombinedLoader\n",
    "import tensorboardX, tensorboard \n",
    "\n",
    "def objective(sigmoid_threshold):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    val_losses = []\n",
    "    \n",
    "\n",
    "    # Alkalmazz transzformációt és kezelj multi-hot címkéket egyetlen lépésben\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(train_dataset)))):\n",
    "        train_idx = train_idx.astype(int)\n",
    "        val_idx = val_idx.astype(int)\n",
    "\n",
    "        # Egyszerűbb adatelőkészítés - csak egyszer alkalmazzuk a transzformációt\n",
    "        id_train = TensorLabelDataset(dataset=Subset(train_dataset, train_idx), transform=transform_, is_ood=False)\n",
    "        id_val = TensorLabelDataset(dataset=Subset(train_dataset, val_idx), is_ood=False)\n",
    "\n",
    "        # OOD adatok\n",
    "        ood_train_indices = np.random.choice(len(ood_dataset), size=len(id_train), replace=False).astype(int)\n",
    "        ood_train = TensorLabelDataset(Subset(ood_dataset, ood_train_indices), is_ood=True)\n",
    "\n",
    "    for fold, (train_idx, val_idx) in enumerate(kf.split(np.arange(len(train_dataset)))):\n",
    "        # print(\"TRAIN\" + str(train_idx))\n",
    "        # print(len(train_idx))\n",
    "        # print(type(train_idx))\n",
    "        train_idx = train_idx.astype(int)\n",
    "        val_idx = val_idx.astype(int)\n",
    "\n",
    "        # Create ID train and validation subsets\n",
    "        id_train = TensorLabelDataset(Subset(train_dataset, train_idx))\n",
    "        id_val = TensorLabelDataset(Subset(train_dataset, val_idx))\n",
    "\n",
    "\n",
    "        # Sample OOD data for training to match length of ID train set\n",
    "        ood_train_indices = np.random.choice(len(ood_dataset), size=len(id_train), replace=False).astype(int)\n",
    "        ood_train = Subset(ood_dataset, ood_train_indices)\n",
    "\n",
    "        # Sample separate OOD data for validation to match length of ID validation set\n",
    "        ood_val_indices = np.random.choice(\n",
    "            [i for i in range(len(ood_dataset)) if i not in ood_train_indices], \n",
    "            size=len(id_val), \n",
    "            replace=False\n",
    "        ).astype(int)\n",
    "        ood_val = Subset(ood_dataset, ood_val_indices)  \n",
    "\n",
    "        # print(\"Sample from id_train:\", id_train[0])\n",
    "        # print(\"Sample from ood_train:\", ood_train[0])\n",
    "\n",
    "        # Combine ID and OOD for training (50-50 mix)\n",
    "        combined_train = ConcatDataset([id_train, ood_train])\n",
    "\n",
    "        # Data module with separate OOD validation data\n",
    "        data_module = MyDataModule(\n",
    "            combined_train, \n",
    "            id_val, \n",
    "            ood_data=ood_train,\n",
    "            val_ood_data=ood_val, \n",
    "            batch_size=batch_size, \n",
    "            num_workers=1, \n",
    "            persistent_workers=True\n",
    "        )\n",
    "        data_module.setup()\n",
    "        # Model\n",
    "        model = ResNetWrapper(dropout_rate=0.3, lr=0.001, ood_threshold=sigmoid_threshold)\n",
    "\n",
    "        logger = TensorBoardLogger(\"lightning_logs\", name=f\"threshold_finetune_fold_{fold}\")\n",
    "        checkpoint_callback = ModelCheckpoint(monitor=\"val_loss\", mode=\"min\", save_top_k=1, filename=\"{epoch}-{val_loss:.2f}\")\n",
    "        early_stopping = EarlyStopping(monitor=\"val_loss\", patience=3, mode=\"min\")\n",
    "\n",
    "        trainer = Trainer(\n",
    "            logger=logger,\n",
    "            max_epochs=10,\n",
    "            accelerator=\"mps\",\n",
    "            callbacks=[checkpoint_callback, early_stopping]\n",
    "        )\n",
    "\n",
    "        trainer.fit(model, datamodule=data_module)\n",
    "\n",
    "        val_loss = trainer.callback_metrics.get(\"val_loss\", float(\"inf\"))\n",
    "        val_losses.append(val_loss.item() if isinstance(val_loss, torch.Tensor) else float(val_loss))\n",
    "\n",
    "    return -np.mean(val_losses)  # BayesianOptimization maximizes, so return negative loss\n",
    "\n",
    "\n",
    "pbounds = {\n",
    "    'sigmoid_threshold': (0.3, 0.7)\n",
    "} \n",
    "\n",
    "optimizer = BayesianOptimization(\n",
    "    f=objective,\n",
    "    pbounds=pbounds,\n",
    "    random_state=42,\n",
    "    verbose=2\n",
    ") \n",
    "\n",
    "optimizer.maximize(init_points=3, n_iter=7)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.status.busy": "2025-01-03T09:22:21.842738Z",
     "iopub.status.idle": "2025-01-03T09:22:21.843102Z",
     "shell.execute_reply": "2025-01-03T09:22:21.842938Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Print the best parameters\n",
    "best_params = optimizer.max['params']\n",
    "best_params['batch_size'] = int(best_params['batch_size'])\n",
    "print(\"Best parameters: \", best_params)\n",
    " # 0.00732   | 0.4592  val_f1= 0.461\n",
    "# | 0.007323  | 0.4796    val_f1=0.528"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.315Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "print(f\"Best threshold: {optimizer.max['params']['threshold']:.4f}\")\n",
    "print(f\"Best validation loss: {-optimizer.max['target']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T14:57:43.807932Z",
     "iopub.status.busy": "2025-01-03T14:57:43.807639Z",
     "iopub.status.idle": "2025-01-03T14:59:18.765581Z",
     "shell.execute_reply": "2025-01-03T14:59:18.764659Z",
     "shell.execute_reply.started": "2025-01-03T14:57:43.807907Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "from IPython.display import FileLink, display\n",
    "\n",
    "def download_file(path, download_file_name):\n",
    "    os.chdir('/kaggle/working/')\n",
    "    zip_name = f\"/kaggle/working/{download_file_name}.zip\"\n",
    "    command = f\"zip {zip_name} {path} -r\"\n",
    "    result = subprocess.run(command, shell=True, capture_output=True, text=True)\n",
    "    if result.returncode != 0:\n",
    "        print(\"Unable to run zip command!\")\n",
    "        print(result.stderr)\n",
    "        return\n",
    "    display(FileLink(f'{download_file_name}.zip'))\n",
    "download_file('/kaggle/working/', 'out')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T14:59:18.766868Z",
     "iopub.status.busy": "2025-01-03T14:59:18.766631Z",
     "iopub.status.idle": "2025-01-03T14:59:18.916940Z",
     "shell.execute_reply": "2025-01-03T14:59:18.916170Z",
     "shell.execute_reply.started": "2025-01-03T14:59:18.766848Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r file.zip /kaggle/working/lightning_logs/training_proper_test\n",
    "from IPython.display import FileLink\n",
    "FileLink(r'file.zip')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Testing on the labeled logo dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!ls lightning_logs/training_proper_test/version_24/checkpoints/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "from torchmetrics.classification import MulticlassConfusionMatrix\n",
    "\n",
    "metric = MulticlassConfusionMatrix(num_classes=5)\n",
    "metric.update(all_labels, all_preds)\n",
    "fig_, ax_ = metric.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import f1_score\n",
    "from glob import glob\n",
    "\n",
    "# Assuming your test dataset is set up already\n",
    "batch_size = 23\n",
    "lr = 0.008274600824983372\n",
    "\n",
    "# Create the test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=0)\n",
    "\n",
    "# Find all checkpoint files recursively across all versions\n",
    "checkpoint_directory = '/kaggle/working/lightning_logs/multilabel_training_fold_2/'\n",
    "checkpoint_files = glob(os.path.join(checkpoint_directory, '**', '*.ckpt'), recursive=True)\n",
    "\n",
    "# Define a dictionary to store the performance of each checkpoint\n",
    "checkpoint_performance = {}\n",
    "\n",
    "# Iterate through each checkpoint file\n",
    "for checkpoint in checkpoint_files:\n",
    "    # Load the model from checkpoint\n",
    "    model = best_model\n",
    "    model.eval()  \n",
    "    model.to('cuda')\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "\n",
    "    # Evaluate model\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in test_loader:\n",
    "            inputs, labels = inputs.to('cuda'), labels.to('cuda')\n",
    "\n",
    "            # Get model predictions\n",
    "            outputs = model(inputs)\n",
    "            probabilities = F.softmax(outputs, dim=1)\n",
    "            _, predicted = torch.max(probabilities, 1)\n",
    "\n",
    "            # Store predictions and labels\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "\n",
    "            # Update accuracy\n",
    "            correct += (predicted == labels).sum().item()\n",
    "            total += labels.size(0)\n",
    "\n",
    "    # Calculate F1 score and accuracy\n",
    "    f1 = f1_score(all_labels, all_preds, average='macro')\n",
    "    accuracy = 100 * correct / total\n",
    "\n",
    "    # Store the performance metrics for the current checkpoint\n",
    "    checkpoint_performance[checkpoint] = {'f1_score': f1, 'accuracy': accuracy}\n",
    "\n",
    "    # Print results\n",
    "    print(f\"Checkpoint: {checkpoint}\")\n",
    "    print(f\"F1 Score: {f1:.4f}\")\n",
    "    print(f\"Accuracy: {accuracy:.2f}%\\n\")\n",
    "\n",
    "# Find the best checkpoint based on F1 score\n",
    "best_checkpoint = max(checkpoint_performance, key=lambda x: checkpoint_performance[x]['f1_score'])\n",
    "best_f1_score = checkpoint_performance[best_checkpoint]['f1_score']\n",
    "best_accuracy = checkpoint_performance[best_checkpoint]['accuracy']\n",
    "\n",
    "print(\"\\nBest Checkpoint:\")\n",
    "print(f\"Path: {best_checkpoint}\")\n",
    "print(f\"F1 Score: {best_f1_score:.4f}\")\n",
    "print(f\"Accuracy: {best_accuracy:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "\n",
    "# Function to unnormalize images for display\n",
    "def unnormalize(img_tensor, mean, std):\n",
    "    img_tensor = img_tensor.clone()  # Clone the tensor to avoid modifying the original\n",
    "    for t, m, s in zip(img_tensor, mean, std):\n",
    "        t.mul_(s).add_(m)  # Unnormalize each channel\n",
    "    return img_tensor\n",
    "\n",
    "# Define mean and std from your transform for unnormalizing\n",
    "mean = [0.485, 0.456, 0.406]\n",
    "std = [0.229, 0.224, 0.225]\n",
    "\n",
    "# Create a function to plot a grid of images\n",
    "def plot_image_grid(dataset, num_images=16):\n",
    "    fig, axes = plt.subplots(4, 4, figsize=(8, 8))  # Grid of 4x4 for 16 images\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(num_images):\n",
    "        img, label = dataset[i]\n",
    "        img = unnormalize(img, mean, std)  # Unnormalize\n",
    "        img = img.permute(1, 2, 0).numpy()  # Convert from CxHxW to HxWxC\n",
    "\n",
    "        axes[i].imshow(img)\n",
    "        axes[i].set_title(f\"Label: {label}\")\n",
    "        axes[i].axis(\"off\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# Visualize some images from the training dataset\n",
    "plot_image_grid(test_set, num_images=16)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "\n",
    "# Define the threshold for classification\n",
    "threshold = 0.8\n",
    "batch_size = 23\n",
    "lr = 0.008274600824983372\n",
    "\n",
    "# Initialize the trainer\n",
    "trainer = Trainer()\n",
    "\n",
    "results = []\n",
    "checkpoint = \"/kaggle/input/checkpoints/lightning_logs/the training/version_4/checkpoints/epoch=8-val_loss=1.39.ckpt\"\n",
    "\n",
    "# Load the test dataset\n",
    "test_dataset = ImageFolder(root='/kaggle/input/brand-logos/test', transform=transform)\n",
    "\n",
    "# Create the test DataLoader\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)\n",
    "\n",
    "# Load the model from the checkpoint\n",
    "model = ResNetWrapper.load_from_checkpoint(checkpoint, lr=lr, num_classes=100, \n",
    "                                            backbone_weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "print(isinstance(model, LightningModule))\n",
    "model.eval()  # Set model to evaluation mode if necessary\n",
    "\n",
    "# Test the model using the Trainer\n",
    "test_results = trainer.test(model, test_loader)\n",
    "\n",
    "# Process the test results to compute thresholds if needed\n",
    "preds = test_results[0]['preds']\n",
    "labels = test_results[0]['labels']\n",
    "\n",
    "# Convert logits to probabilities and apply thresholding\n",
    "probabilities = F.softmax(preds, dim=1)\n",
    "predicted = (probabilities > threshold).long().max(dim=1)[1]\n",
    "\n",
    "# Store results\n",
    "accuracy = (predicted == labels).sum().item() / len(labels)\n",
    "results.append({'checkpoint': checkpoint, 'accuracy': accuracy})\n",
    "\n",
    "# Output all results\n",
    "for result in results:\n",
    "    print(f\"Checkpoint: {result['checkpoint']} - Accuracy: {result['accuracy']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "overall_f1_macro = f1_score(all_labels, all_preds, average='macro')   # Macro-average\n",
    "print(overall_f1_macro)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Saving best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\n",
    "!ls /kaggle/working/checkpoints/lightning_logs/\n",
    "!ls /kaggle/working/checkpoints/lightning_logs/version_6/checkpoints\n",
    "!zip -r nyertes.zip /kaggle/working/checkpoints/lightning_logs/version_13/checkpoints/epoch=14-step=2190.ckpt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!zip -r version_5.zip /kaggle/working/checkpoints/lightning_logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install -U ipywidgets\n",
    "!pip install ipywidgets\n",
    "!jupyter nbextension enable --py widgetsnbextension\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -rf /kaggle/working/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test on unlabeled marketing memes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from PIL import Image\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision.transforms as transforms\n",
    "# Define your brand names in the order they appear in your list\n",
    "brand_names = [\n",
    "    \"3m\", \"axa\", \"accenture\", \"adobe\", \"airbnb\", \"allianz\", \"amazon\", \"americanexpress\", \n",
    "    \"apple\", \"audi\", \"bmw\", \"budweiser\", \"burberry\", \"canon\", \"cartier\", \"caterpillar\", \n",
    "    \"chanel\", \"cisco\", \"citi bank\", \"cocacola\", \"colgate\", \"corona\", \"dhl\", \"danone\", \n",
    "    \"dior\", \"disney\", \"facebook\", \"fedex\", \"ferrari\", \"ford\", \"ge\", \"gillette\", \n",
    "    \"goldmansachs\", \"google\", \"gucci\", \"hm\", \"hp\", \"hsbc\", \"heineken\", \"hennessy\", \n",
    "    \"hermès\", \"hewlettpackardenterprise\", \"honda\", \"huawei\", \"hyundai\", \"ibm\", \"ikea\", \n",
    "    \"instagram\", \"intel\", \"jpmorgan\", \"jackdaniels\", \"johnsonjohnson\", \"kfc\", \"kelloggs\", \n",
    "    \"kia\", \"lego\", \"loréalparis\", \"linkedin\", \"louisvuitton\", \"mastercard\", \"mcdonalds\", \n",
    "    \"mercedesbenz\", \"microsoft\", \"morganstanley\", \"nescafé\", \"nespresso\", \"nestlé\", \n",
    "    \"netflix\", \"nike\", \"nintendo\", \"nissan\", \"oracle\", \"pampers\", \"panasonic\", \"paypal\", \n",
    "    \"pepsi\", \"philips\", \"porsche\", \"prada\", \"redbull\", \"sap\", \"salesforce\", \"samsung\", \n",
    "    \"santander\", \"sephora\", \"siemens\", \"sony\", \"spotify\", \"starbucks\", \"tesla\", \n",
    "    \"tiffanyco\", \"toyota\", \"ups\", \"visa\", \"volkswagen\", \"xiaomi\", \"youtube\", \"zara\", \n",
    "    \"adidas\", \"ebay\"\n",
    "]\n",
    "\n",
    "class UnlabeledImageDataset(Dataset):\n",
    "    def __init__(self, image_dir, transform=None):\n",
    "        self.image_dir = image_dir\n",
    "        self.image_paths = [os.path.join(image_dir, img) for img in os.listdir(image_dir) if img.lower().endswith(('png', 'jpg', 'jpeg'))]\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.image_paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.image_paths[idx]\n",
    "        image = Image.open(img_path).convert(\"RGB\")\n",
    "        if self.transform:\n",
    "            image = self.transform(image)\n",
    "        return image, img_path\n",
    "\n",
    "\n",
    "# Load the model checkpoint\n",
    "num_classes = 100  # Adjust based on your actual number of classes\n",
    "model = ResNetWrapper.load_from_checkpoint(checkpoint,lr=lr, num_classes=num_classes, backbone_weights=models.ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "# Move the model to the GPU\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model = model.to(device)\n",
    "model.eval()\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # ResNet50 typically takes 224x224 input\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Define the prediction function\n",
    "def classify_images(model, image_folder, transform, device):\n",
    "    dataset = UnlabeledImageDataset(image_folder, transform=transform)\n",
    "    dataloader = DataLoader(dataset, batch_size=32, shuffle=False)\n",
    "    results = {}  # Ensure 'results' dictionary is initialized\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, img_paths in dataloader:\n",
    "            images = images.to(device)\n",
    "            logits = model(images)\n",
    "            probs = F.softmax(logits, dim=1)  # Convert logits to probabilities\n",
    "            confidences, preds = torch.max(probs, dim=1)  # Get the highest probability and its index for each prediction\n",
    "\n",
    "            for confidence, pred, img_path in zip(confidences, preds, img_paths):\n",
    "                if confidence.item() >= 0.9:  # Only consider predictions with confidence >= 80%\n",
    "                    brand = brand_names[pred.item()].lower()  # Map index to brand name\n",
    "\n",
    "                    if brand not in results:\n",
    "                        results[brand] = []\n",
    "                    results[brand].append(os.path.basename(img_path))  # Append the filename, not the full path\n",
    "\n",
    "    return results\n",
    "\n",
    "# Define the folder containing images to classify\n",
    "image_folder = \"/kaggle/input/5memes-for-top-100-most-valuable-brand-2023\"\n",
    "\n",
    "# Classify images and store the results in a dictionary\n",
    "dict1 = classify_images(model, image_folder, transform, device)\n",
    "\n",
    "\n",
    "# Classify images and store the results in a dictionary\n",
    "dict1 = classify_images(model, image_folder, transform, device)\n",
    "dict2 = {'apple': ['Xiaomi memes_9daf00f7-8607-4ff6-b8e1-1dd5c755f43a.jpeg', 'Apple memes_iphone-chatgpt.jpg', 'Apple memes_Image_2.jpg', 'Huawei memes_Image_3.jpg'], 'microsoft': ['Microsoft memes_Image_5.jpg', 'Microsoft memes_Image_1.jpeg', 'Microsoft memes_Image_4.jpeg', 'HP memes_190d521ac62eaf40a74e50ab39635231.jpg'], 'amazon': ['Amazon memes_Image_1.jpg', 'Amazon memes_Image_2.jpg', 'Amazon memes_Image_4.jpeg', 'Amazon memes_Image_5.jpg', 'Amazon memes_Image_3.jpg'], 'google': ['Google memes_Image_2.jpeg', 'Google memes_Image_1.jpg', 'Xiaomi memes_Image_2.jpeg', 'Google memes_Image_4.jpg', 'Google memes_Image_3.jpg'], 'samsung': ['Samsung memes_Image_1.jpeg', 'Samsung memes_Image_2.jpg', 'Samsung memes_Image_3.jpg', 'Samsung memes_Image_4.jpg', 'Samsung memes_Image_5.jpeg'], 'toyota': ['Toyota memes_Image_1.jpg', 'Toyota memes_Image_2.jpg', 'Toyota memes_Image_4.jpg'], 'mercedes-benz': ['MercedesBenz memes_Image_5.jpg', 'MercedesBenz memes_Image_3.jpg'], 'coca-cola': ['CocaCola memes_Image_2.jpeg', 'Pepsi memes_Image_3.jpg', 'CocaCola memes_Image_1.jpg'], 'nike': [], 'bmw': ['BMW memes_ayNdM3q_460s.jpg', 'BMW memes_Image_4.jpg', 'Audi memes_Image_5.jpg', 'MercedesBenz memes_Image_1.jpg'], \"mcdonald's\": ['McDonalds memes_Image_2.jpg', 'McDonalds memes_Image_1.jpg', 'McDonalds memes_Image_2 (copy 1).jpg'], 'tesla': ['Tesla memes_Image_2.jpg', 'Tesla memes_teslamemes-191110030713-thumbnail.webp', 'Caterpillar memes_image-1.webp', 'Tesla memes_Image_5.jpg'], 'disney': ['Disney memes_Image_1.png', 'Disney memes_Image_2.jpg', 'Disney memes_relatable-disney-memes.png'], 'louis vuitton': ['LouisVuitton memes_Image_1.jpg', 'LouisVuitton memes_tgeycyn3z4k91.jpg', 'LouisVuitton memes_0a2163e267c3e9c175e7c08bf8253318.jpg', 'LouisVuitton memes_Image_2.jpg'], 'cisco': ['Cisco memes_d77c72918e7440d37fc3eabc307ad8ac3a8025383ea34cdb6e1df27c047628f6_1.jpg', 'Cisco memes_Image_3.jpg', 'Cisco memes_Image_1.jpg', 'Cisco memes_fc0j9uamxd371.png'], 'instagram': ['Instagram memes_Image_4.jpg', 'Instagram memes_ig.jpg', 'Heineken memes_Image_1.jpg', 'Instagram memes_Image_5.jpg', 'Airbnb memes_Image_1.jpg', 'Instagram memes_person-women-on-instagram-be-like-got-new-shoes.jpeg', 'Instagram memes_Image_2.jpg', 'HewlettPackardEnterprise memes_EPo6d7mUYAAHsU0.jpg'], 'adobe': ['Adobe memes_Image_3.jpeg', 'Adobe memes_Image_1.jpeg', 'Adobe memes_Image_2.jpeg'], 'ibm': ['IBM memes_Image_2.jpg', 'IBM memes_Image_1.jpg', 'IBM memes_Image_3.jpg'], 'oracle': ['Oracle memes_EXT38P1WkAAjs_5.jpg', 'Oracle memes_Image_5.jpg', 'Oracle memes_programmerhumor-io-databases-memes-backend-memes-ce04a4d894b6035-608x613.webp'], 'sap': ['SAP memes_Image_1.jpeg', 'SAP memes_Image_3.jpg', 'SAP memes_Image_5.png', 'SAP memes_Image_2.jpeg', 'SAP memes_Image_4.jpg'], 'facebook': ['Instagram memes_Image_4.jpg', 'Heineken memes_Image_1.jpg', 'Caterpillar memes_Bj9v-r7IUAApml9.png', 'Facebook memes_Image_1.jpg', 'Facebook memes_Screenshot2021-11-01at16.29.20.jpg', 'TiffanyCo memes_98facae07556bfaab7cb6672371ff06c.jpg', 'Facebook memes_f.jpg', 'Facebook memes_Image_4.jpg', 'HewlettPackardEnterprise memes_EPo6d7mUYAAHsU0.jpg'], 'chanel': ['Chanel memes_images (1).jpeg', 'Chanel memes_Screen-Shot-2021-04-29-at-8.webp', 'Chanel memes_Image_2.jpg', 'Chanel memes_images.jpeg'], 'hermes': ['Herms memes_434053634_18427064311043375_1898595488137274113_n.jpg', 'Herms memes_Fd_gt0bUUAAeCRh.jpg'], 'intel': ['Intel memes_Image_4 (copy 1).jpeg', 'Intel memes_Image_2.jpg', 'Intel memes_Image_4.jpeg', 'Intel memes_Image_1.jpg'], 'youtube': ['YouTube memes_Image_2.jpg', 'YouTube memes_Image_3.jpg'], 'j.p. morgan': ['JPMorgan memes_xq0z4545oqe41.webp', 'MorganStanley memes_Image_1.jpg'], 'honda': ['Honda memes_Image_4.jpg', 'Honda memes_Image_5.jpg'], 'american express': ['AmericanExpress memes_Image_3.jpg', 'AmericanExpress memes_Image_4.jpg', 'AmericanExpress memes_Image_1.jpg'], 'ikea': ['IKEA memes_Image_3.jpg', 'IKEA memes_Image_5.png', 'IKEA memes_Image_2.jpg', 'IKEA memes_Image_4.jpg'], 'accenture': ['Accenture memes_Image_4.jpg', 'Accenture memes_Image_1.png', 'Accenture memes_Image_2.jpg', 'Accenture memes_Image_5.jpg'], 'allianz': ['Allianz memes_amLOMzj_460s.jpg', 'Allianz memes_Image_2.jpg', 'Allianz memes_Image_1.jpg', 'Allianz memes_images.jpeg'], 'hyundai': ['Hyundai memes_Image_3.jpeg', 'Hyundai memes_Image_4.jpg'], 'ups': ['Cisco memes_d77c72918e7440d37fc3eabc307ad8ac3a8025383ea34cdb6e1df27c047628f6_1.jpg', 'UPS memes_Image_4.jpg', 'Ferrari memes_Image_2.jpg', 'DHL memes_Image_2.jpeg', 'UPS memes_Image_3.jpeg', 'UPS memes_Image_2.jpeg'], 'gucci': ['Gucci memes_Image_3.jpg', 'Gucci memes_Image_1.jpg', 'Gucci memes_Image_2 (copy 1).jpg', 'Gucci memes_Image_2.jpg'], 'pepsi': ['Pepsi memes_Image_2.jpeg', 'Pepsi memes_Image_3.jpg', 'Pepsi memes_Image_5.jpeg'], 'sony': ['Sony memes_Image_2.jpg', 'Sony memes_Image_3.jpeg', 'Sony memes_Image_2 (copy 1).jpg', 'Sony memes_Image_1.jpeg'], 'visa': ['Visa memes_Image_3.jpg', 'AmericanExpress memes_Image_2.jpg', 'Visa memes_Image_2.jpg', 'Visa memes_Image_5.jpg', 'Visa memes_Image_4.jpg'], 'salesforce': ['Salesforce memes_Image_1.jpeg', 'Salesforce memes_Image_3.jpeg', 'Salesforce memes_Image_5.jpeg', 'Salesforce memes_Image_2.jpeg', 'Salesforce memes_Image_4.jpg'], 'netflix': ['Netflix memes_Image_4.jpg', 'Cisco memes_d77c72918e7440d37fc3eabc307ad8ac3a8025383ea34cdb6e1df27c047628f6_1.jpg', 'Netflix memes_Image_5.jpg', 'Netflix memes_Image_1.jpg'], 'paypal': ['PayPal memes_Image_4.jpeg', 'PayPal memes_Image_2.jpg', 'PayPal memes_Image_3.jpeg', 'PayPal memes_Image_1.jpeg'], 'mastercard': ['Mastercard memes_Image_2.jpg', 'Mastercard memes_Image_4.jpg', 'Mastercard memes_Image_1.jpeg', 'AmericanExpress memes_Image_2.jpg', 'Visa memes_Image_5.jpg'], 'adidas': ['adidas memes_Image_1.jpeg'], 'zara': ['Zara memes_Image_1.jpeg', 'Zara memes_Image_3.jpeg', 'Zara memes_Image_5.jpg'], 'axa': ['AXA memes_gettyimages-1226469012-612x612.jpg'], 'audi': ['Audi memes_Image_5.jpg', 'MercedesBenz memes_Image_1.jpg', 'Audi memes_Image_4.jpg'], 'airbnb': ['Airbnb memes_Image_3.png', 'Airbnb memes_Image_1.jpg', 'Airbnb memes_Image_4.jpeg', 'Airbnb memes_Image_2.jpg', 'Airbnb memes_P4BwmWt.jpg'], 'porsche': ['Porsche memes_Image_2.jpeg', 'Porsche memes_Image_1.jpg', 'Porsche memes_Image_4.jpg'], 'starbucks': ['Starbucks memes_Image_4.jpg', 'Starbucks memes_Image_2.png', 'Starbucks memes_4e59a310d37f67cbb82bcd067ee4e0c7.jpg', 'Starbucks memes_Image_3.png'], 'ge': ['GE memes_images (2).jpeg'], 'volkswagen': ['Volkswagen memes_Image_3.jpg', 'Volkswagen memes_Image_1.jpeg'], 'ford': ['Ford memes_Image_5.jpeg'], 'nescafé': [], 'siemens': ['Siemens memes_Image_5.jpg', 'Siemens memes_siemens-heh-heh-heh-v0-yf2fbabx814b1.webp', 'Siemens memes_Image_4.jpg', 'Siemens memes_A6Db4nQCQAEw2Xg.jpg'], 'goldman sachs': ['GoldmanSachs memes_Image_5.jpg', 'GoldmanSachs memes_Image_2.jpg', 'GoldmanSachs memes_Image_3.jpg', 'GoldmanSachs memes_Image_4.jpg'], 'pampers': ['Pampers memes_Image_3.jpg', 'Pampers memes_Image_5 (copy 1).jpg', 'Pampers memes_Image_5.jpg'], 'h&m': ['HM memes_tandem-x-visuals-FZOOxR2auVI-unsplash-1313x900.webp', 'HM memes_Image_4.jpeg'], 'l’oréal paris': ['LOralParis memes_Image_3.jpg', 'LOralParis memes_Image_1.jpg', 'LOralParis memes_Image_2.jpeg'], 'citi': ['Cisco memes_d77c72918e7440d37fc3eabc307ad8ac3a8025383ea34cdb6e1df27c047628f6_1.jpg', 'Citi Bank memes_Image_4.jpg', 'MorganStanley memes_Image_3.jpg', 'Citi Bank memes_citi.jpg', 'Citi Bank memes_Image_2.jpg', 'Citi Bank memes_Image_5.jpg'], 'lego': ['LEGO memes_Image_5.jpg', 'LEGO memes_images.jpeg', 'LEGO memes_Image_4.jpeg'], 'red bull': ['JackDaniels memes_Image_2.jpg', 'RedBull memes_Image_5.jpg', 'RedBull memes_Image_2.jpg', 'RedBull memes_Image_4.jpeg', 'RedBull memes_Image_3.jpg'], 'budweiser': ['Budweiser memes_Image_3.jpg', 'Budweiser memes_aoKL853_460s.jpg', 'Budweiser memes_Image_2.jpg', 'Budweiser memes_Image_4.jpg', 'Budweiser memes_Image_1.jpg'], 'ebay': ['eBay memes_Image_4.jpg', 'eBay memes_Image_2.jpg'], 'nissan': ['Nissan memes_Image_2.jpeg', 'Nissan memes_Image_4.jpg', 'Nissan memes_Image_3.jpeg', 'Nissan memes_Image_3.jpg'], 'hp': ['HewlettPackardEnterprise memes_k8m3z.jpg', 'HP memes_38ef2b77327f4020f04dc14dc41f1538.jpg', 'HP memes_190d521ac62eaf40a74e50ab39635231.jpg'], 'hsbc': ['HSBC memes_Image_1.jpg', 'HSBC memes_Image_3.jpg', 'HSBC memes_images.jpeg'], 'morgan stanley': ['MorganStanley memes_4521ccf691de4bacef41ccbb143387b2.jpg', 'MorganStanley memes_Image_4.jpg', 'MorganStanley memes_images.jpeg', 'MorganStanley memes_Image_1.jpg'], 'nestle': ['Nestl memes_Image_1.jpeg', 'Nestl memes_Image_2.jpeg', 'Nestl memes_Image_3.jpeg', 'Nestl memes_Image_4.jpeg', 'Nestl memes_Image_5.jpg'], 'philips': ['Philips memes_Image_4.jpeg', 'Philips memes_images.jpeg', 'Philips memes_Philips-Innovationandyou-1002x1417.jpg', 'Philips memes_unnamed.jpg'], 'spotify': ['Spotify memes_spotify-now-listen-most-boring-pop-charts-yes-hey-xx-xx-tap-banner-now-last-ad-just-two-songs-ago.png', 'Spotify memes_Image_5.jpeg'], 'ferrari': ['Ferrari memes_Image_5.jpg', 'Ferrari memes_Image_1.jpg', 'Ferrari memes_Image_1.jpeg', 'Ferrari memes_Image_2.jpg', 'Ferrari memes_Image_3.png'], 'nintendo': ['Philips memes_Image_4.jpeg', 'Nintendo memes_Image_5.jpeg', 'Nintendo memes_Image_4.jpeg', 'Panasonic memes_Image_3.jpeg'], 'gillette': ['Gillette memes_Image_1.jpeg', 'Gillette memes_Image_2.jpg', 'Gillette memes_Image_4.jpeg', 'Gillette memes_Image_5.jpg'], 'colgate': ['Colgate memes_Image_4.jpeg', 'Colgate memes_Image_2.jpg', 'LOralParis memes_Image_2.jpeg'], 'cartier': ['Cartier memes_CaGpc-1VIAAOpWc.png', 'Cartier memes_Image_3.jpg', 'Cartier memes_Image_1.jpeg', 'Cartier memes_65c080cea63c4.jpeg'], '3m': ['3M memes_Image_2.jpg', '3M memes_Image_5.jpg', '3M memes_Image_3.jpg', '3M memes_khkn86l1vgs41.webp'], 'dior': ['Dior memes_Image_3.jpg', 'Dior memes_Image_5.jpg', 'Dior memes_Image_2.jpg', 'Dior memes_Image_4.jpeg', 'Dior memes_Image_4.jpg'], 'santander': ['Santander memes_Image_2.jpg', 'Santander memes_Image_3.jpg', 'Santander memes_Image_4.jpg', 'Santander memes_meme1.webp'], 'danone': ['Danone memes_Image_2.jpg', 'Danone memes_Image_3.jpg', 'Danone memes_5lmocm.jpg', 'Danone memes_5b1d46878de83.jpeg', 'Danone memes_page_1.webp'], \"kellogg's\": ['Kelloggs memes_Image_3.jpeg', 'Kelloggs memes_Image_3.jpg'], 'linkedin': ['LinkedIn memes_Image_2.jpg'], 'corona': ['Corona memes_om9ysstu92k41.jpg'], 'fedex': ['FedEx memes_Image_1.jpg', 'FedEx memes_Image_5.jpg', 'FedEx memes_Image_2.jpeg', 'FedEx memes_Image_3.jpg'], 'caterpillar': ['Caterpillar memes_image-1.webp'], 'dhl': ['DHL memes_Image_5.jpg', 'DHL memes_Image_4.png', 'DHL memes_Image_1.jpg', 'DHL memes_Image_2.jpeg'], \"jack daniel's\": ['JackDaniels memes_Image_4.jpg'], 'prada': ['Prada memes_highxtar-prada-ss21-campaign-4.jpg', 'Prada memes_highxtar-prada-ss21-campaign-1.jpg', 'Prada memes_DIETPRADA3.webp', 'Prada memes_highxtar-prada-ss21-campaign-2.jpg'], 'xiaomi': ['Xiaomi memes_9daf00f7-8607-4ff6-b8e1-1dd5c755f43a.jpeg', 'Xiaomi memes_Image_4.jpeg', 'Xiaomi memes_Image_1.jpeg'], 'kia': [], 'tiffany & co.': [], 'panasonic': ['Panasonic memes_Image_5.jpg', 'Panasonic memes_Image_4.jpeg', 'Panasonic memes_Image_1.jpeg'], 'hewlett packard enterprise': ['hp enterprise', 'hewlett packard', 'HewlettPackardEnterprise memes_maxresdefault.jpg', 'HewlettPackardEnterprise memes_EPo6d7mUYAAHsU0.jpg'], 'huawei': ['Huawei memes_Image_5.jpg', 'Huawei memes_u8978vigrj031.webp', 'Huawei memes_Image_3.jpg'], 'hennessy': ['Hennessy memes_Image_4.jpg', 'Hennessy memes_Image_3.jpg', 'Hennessy memes_Image_2.jpg', 'Hennessy memes_Image_1.jpg', 'Hennessy memes_Image_5.jpg'], 'burberry': ['Burberry memes_images.jpeg', 'Burberry memes_Image_3.jpg', 'Burberry memes_4febf2b9add2cf73f8e3bcca6f67c904.jpg', 'Burberry memes_Image_5.jpg'], 'kfc': ['KFC memes_Image_4.jpg', 'KFC memes_Image_1.jpeg', 'KFC memes_Image_5.jpeg', 'KFC memes_Image_2 (copy 1).jpg'], 'johnson & johnson': ['JohnsonJohnson memes_jj-vaccine-covid-19-vaccine-memes-covid-19-memes-funny-memes-memes-twitter-memes-funny-tweets.jpeg', 'JohnsonJohnson memes_memes-for-use-to-spam-j-j-v0-2kmhfpfrrmbb1.webp', 'JohnsonJohnson memes_Image_1.jpg'], 'sephora': ['Sephora memes_Image_5.png', 'Sephora memes_Image_2.jpg', 'Sephora memes_Image_1.jpg', 'Sephora memes_Image_4.jpeg', 'Sephora memes_Image_3.jpg'], 'nespresso': ['Nespresso memes_Image_3.jpg', 'Nespresso memes_Image_1.jpg', 'Nespresso memes_Image_2.jpg', 'Nespresso memes_Image_4.jpg', 'Nespresso memes_Image_5.jpg'], 'heineken': ['Corona memes_images.jpeg', 'Heineken memes_Image_1.jpg', 'Heineken memes_Image_5.jpg'], 'canon': ['Canon memes_55bc7be8031ed.jpeg', 'Canon memes_7ugwbf.jpg', 'Canon memes_Image_1.jpg', 'Canon memes_xzgpcaalz69a1.jpg']}\n",
    "result = {}\n",
    "\n",
    "for key in dict1:\n",
    "    if key in dict2:\n",
    "        if key not in result:\n",
    "            result[key] = dict1[key] + dict2[key]\n",
    "    else:\n",
    "        result[key] = dict1[key]\n",
    "\n",
    "for key in dict2:\n",
    "    if key not in result:\n",
    "        result[key] = dict2[key]\n",
    "print(\"finished\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "for i, (key, value_list) in enumerate(dict1.items()):\n",
    "    print(f\"{i} \\t {key}: {value_list}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dagsub connection, authentication"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-05-22T13:47:12.279017Z",
     "iopub.status.busy": "2024-05-22T13:47:12.278664Z",
     "iopub.status.idle": "2024-05-22T13:47:25.888728Z",
     "shell.execute_reply": "2024-05-22T13:47:25.887436Z",
     "shell.execute_reply.started": "2024-05-22T13:47:12.278987Z"
    }
   },
   "source": [
    "!pip install -U mlflow>=1.0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-03T08:52:19.316018Z",
     "iopub.status.busy": "2025-01-03T08:52:19.315751Z",
     "iopub.status.idle": "2025-01-03T08:52:22.745874Z",
     "shell.execute_reply": "2025-01-03T08:52:22.745053Z",
     "shell.execute_reply.started": "2025-01-03T08:52:19.315983Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -r /kaggle/working/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-01-02T18:40:28.317Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!rm -fr /kaggle/working/DIRPATH/model-epoch=14-val_loss=1.60.ckpt"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [
    {
     "datasetId": 5366753,
     "sourceId": 8922601,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 5717524,
     "sourceId": 10364389,
     "sourceType": "datasetVersion"
    },
    {
     "datasetId": 6031274,
     "sourceId": 10364801,
     "sourceType": "datasetVersion"
    },
    {
     "modelId": 90746,
     "modelInstanceId": 65899,
     "sourceId": 78408,
     "sourceType": "modelInstanceVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 231646,
     "modelInstanceId": 209951,
     "sourceId": 245713,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
